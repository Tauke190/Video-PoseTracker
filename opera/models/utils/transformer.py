# Copyright (c) Hikvision Research Institute. All rights reserved.
import math

import torch
import torch.nn as nn
from torch.nn.init import normal_
from mmcv.cnn import constant_init, xavier_init
from mmcv.cnn.bricks.transformer import (BaseTransformerLayer,
                                         TransformerLayerSequence)
from mmcv.ops.multi_scale_deform_attn import (MultiScaleDeformableAttention,
                                              MultiScaleDeformableAttnFunction,
                                              multi_scale_deformable_attn_pytorch,
                                              multi_scale_deformable_attn_pytorchV1)
from mmcv.runner.base_module import BaseModule
from mmdet.models.utils.transformer import (DeformableDetrTransformer,
                                            Transformer, inverse_sigmoid)

from .builder import (TRANSFORMER, ATTENTION, TRANSFORMER_LAYER_SEQUENCE,
                      build_transformer_layer_sequence)
from easydict import EasyDict


@TRANSFORMER.register_module()
class SOITTransformer(DeformableDetrTransformer):
    """Implements the SOIT transformer.

    Args:
        mask_channels (int): Number of channels of output mask feature.
        seg_encoder (obj:`ConfigDict`): ConfigDict is used for building the
            encoder for mask feature generation.
        num_feature_levels (int): Number of feature maps from FPN:
            Default: 4.
        two_stage_num_proposals (int): Number of proposals when set
            `as_two_stage` as True. Default: 300.
    """

    def __init__(self,
                 mask_channels=8,
                 seg_encoder=dict(
                     type='DetrTransformerEncoder',
                     num_layers=1,
                     transformerlayers=dict(
                         type='BaseTransformerLayer',
                         attn_cfgs=dict(
                             type='MultiScaleDeformableAttention',
                             embed_dims=256,
                             num_heads=1,
                             num_levels=1),
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'ffn', 'norm'))),
                 as_two_stage=False,
                 num_feature_levels=4,
                 two_stage_num_proposals=300,
                 **kwargs):
        super(SOITTransformer, self).__init__(
            as_two_stage=as_two_stage, 
            num_feature_levels=num_feature_levels,
            two_stage_num_proposals=two_stage_num_proposals,
            **kwargs)
        self.seg_encoder = build_transformer_layer_sequence(seg_encoder)
        self.mask_channels = mask_channels
        self.mask_trans = nn.Linear(self.embed_dims, self.mask_channels)
        self.mask_trans_norm = nn.LayerNorm(self.mask_channels)
    
    def forward(self,
                mlvl_feats,
                mlvl_masks,
                query_embed,
                mlvl_pos_embeds,
                reg_branches=None,
                cls_branches=None,
                **kwargs):
        """Forward function for `Transformer`.

        Args:
            mlvl_feats (list(Tensor)): Input queries from
                different level. Each element has shape
                [bs, embed_dims, h, w].
            mlvl_masks (list(Tensor)): The key_padding_mask from
                different level used for encoder and decoder,
                each element has shape [bs, h, w].
            query_embed (Tensor): The query embedding for decoder,
                with shape [num_query, c].
            mlvl_pos_embeds (list(Tensor)): The positional encoding
                of feats from different level, has the shape
                [bs, embed_dims, h, w].
            reg_branches (obj:`nn.ModuleList`): Regression heads for
                feature maps from each decoder layer. Only would be
                passed when `with_box_refine` is Ture. Default to None.
            cls_branches (obj:`nn.ModuleList`): Classification heads
                for feature maps from each decoder layer. Only would
                be passed when `as_two_stage` is Ture. Default to None.

        Returns:
            tuple[Tensor]: results of decoder containing the following tensor.

                - inter_states: Outputs from decoder. If
                    return_intermediate_dec is True output has shape \
                      (num_dec_layers, bs, num_query, embed_dims), else has \
                      shape (1, bs, num_query, embed_dims).
                - init_reference_out: The initial value of reference \
                    points, has shape (bs, num_queries, 4).
                - inter_references_out: The internal value of reference \
                    points in decoder, has shape \
                    (num_dec_layers, bs,num_query, embed_dims)
                - enc_outputs_class: The classification score of \
                    proposals generated from \
                    encoder's feature maps, has shape \
                    (batch, h*w, num_classes). \
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
                - enc_outputs_coord_unact: The regression results \
                    generated from encoder's feature maps., has shape \
                    (batch, h*w, 4). Only would \
                    be returned when `as_two_stage` is True, \
                    otherwise None.
                - mask_proto: Feature, positional encoding and other \
                    information for mask feature.
        """
        assert self.as_two_stage or query_embed is not None

        feat_flatten = []
        mask_flatten = []
        lvl_pos_embed_flatten = []
        spatial_shapes = []
        for lvl, (feat, mask, pos_embed) in enumerate(
                zip(mlvl_feats, mlvl_masks, mlvl_pos_embeds)):
            bs, c, h, w = feat.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            feat = feat.flatten(2).transpose(1, 2)
            mask = mask.flatten(1)
            pos_embed = pos_embed.flatten(2).transpose(1, 2)
            lvl_pos_embed = pos_embed + self.level_embeds[lvl].view(1, 1, -1)
            lvl_pos_embed_flatten.append(lvl_pos_embed)
            feat_flatten.append(feat)
            mask_flatten.append(mask)
        feat_flatten = torch.cat(feat_flatten, 1)
        mask_flatten = torch.cat(mask_flatten, 1)
        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=feat_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        reference_points = \
            self.get_reference_points(spatial_shapes,
                                      valid_ratios,
                                      device=feat.device)

        feat_flatten = feat_flatten.permute(1, 0, 2)  # (H*W, bs, embed_dims)
        lvl_pos_embed_flatten = lvl_pos_embed_flatten.permute(
            1, 0, 2)  # (H*W, bs, embed_dims)
        memory = self.encoder(
            query=feat_flatten,
            key=None,
            value=None,
            query_pos=lvl_pos_embed_flatten,
            query_key_padding_mask=mask_flatten,
            spatial_shapes=spatial_shapes,
            reference_points=reference_points,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            **kwargs)

        memory = memory.permute(1, 0, 2)
        bs, _, c = memory.shape
        seg_memory = memory[:, level_start_index[0]:level_start_index[1], :]
        seg_pos_embed = lvl_pos_embed_flatten[
            level_start_index[0]:level_start_index[1], :, :]
        seg_mask = mask_flatten[:, level_start_index[0]:level_start_index[1]]
        seg_reference_points = reference_points[
            :, level_start_index[0]:level_start_index[1], [0], :]
        seg_memory = seg_memory.permute(1, 0, 2)

        seg_memory = self.seg_encoder(
            query=seg_memory,
            key=None,
            value=None,
            query_pos=seg_pos_embed,
            query_key_padding_mask=seg_mask,
            spatial_shapes=spatial_shapes[[0]],
            reference_points=seg_reference_points,
            level_start_index=level_start_index[0],
            valid_ratios=valid_ratios[:, [0], :],
            **kwargs)
        
        seg_memory = self.mask_trans_norm(self.mask_trans(seg_memory))
        mask_proto = (seg_memory, seg_pos_embed, seg_mask,
                      spatial_shapes[[0]], seg_reference_points,
                      level_start_index[0], valid_ratios[:, [0], :])
        
        if self.as_two_stage:
            output_memory, output_proposals = \
                self.gen_encoder_output_proposals(
                    memory, mask_flatten, spatial_shapes)
            enc_outputs_class = cls_branches[self.decoder.num_layers](
                output_memory)
            enc_outputs_coord_unact = \
                reg_branches[
                    self.decoder.num_layers](output_memory) + output_proposals

            topk = self.two_stage_num_proposals
            topk_proposals = torch.topk(
                enc_outputs_class[..., 0], topk, dim=1)[1]
            topk_coords_unact = torch.gather(
                enc_outputs_coord_unact, 1,
                topk_proposals.unsqueeze(-1).repeat(1, 1, 4))
            topk_coords_unact = topk_coords_unact.detach()
            reference_points = topk_coords_unact.sigmoid()
            init_reference_out = reference_points
            pos_trans_out = self.pos_trans_norm(
                self.pos_trans(self.get_proposal_pos_embed(topk_coords_unact)))
            query_pos, query = torch.split(pos_trans_out, c, dim=2)
        else:
            query_pos, query = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query = query.unsqueeze(0).expand(bs, -1, -1)
            reference_points = self.reference_points(query_pos).sigmoid()
            init_reference_out = reference_points

        # decoder
        query = query.permute(1, 0, 2)
        memory = memory.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        inter_states, inter_references = self.decoder(
            query=query,
            key=None,
            value=memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            reg_branches=reg_branches,
            **kwargs)

        inter_references_out = inter_references
        if self.as_two_stage:
            return inter_states, init_reference_out,\
                inter_references_out, enc_outputs_class,\
                enc_outputs_coord_unact, mask_proto
        return inter_states, init_reference_out, \
            inter_references_out, None, None, mask_proto


@ATTENTION.register_module()
class MultiScaleDeformablePoseAttention(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        xavier_init(self.output_proj, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """

        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)

        bs, num_query, _ = query.shape
        bs, num_key, _ = value.shape
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        value = self.value_proj(value)
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
        value = value.view(bs, num_key, self.num_heads, -1)
        sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        attention_weights = attention_weights.softmax(-1)

        attention_weights = attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        if reference_points.shape[-1] == self.num_points * 2:
            reference_points_reshape = reference_points.reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            sampling_locations = reference_points_reshape \
                                 + sampling_offsets * wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
        if torch.cuda.is_available():
            output = MultiScaleDeformableAttnFunction.apply(
                value, spatial_shapes, level_start_index, sampling_locations,
                attention_weights, self.im2col_step)
        else:
            output = multi_scale_deformable_attn_pytorch(
                value, spatial_shapes, level_start_index, sampling_locations,
                attention_weights, self.im2col_step)
        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual


# 时序多尺度可变形attention v1
@ATTENTION.register_module()
class MulFramesMultiScaleDeformablePoseAttentionV1(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        # 修改
        # 前一帧
        self.pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 当前帧
        self.now_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.now_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后一帧
        self.next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        
        # TODO 合并多帧query
        self.merge = nn.Linear(embed_dims*3, embed_dims)
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.pre_sampling_offsets, 0.)
        constant_init(self.now_sampling_offsets, 0.)
        constant_init(self.next_sampling_offsets, 0.)
        
        constant_init(self.pre_attention_weights, val=0., bias=0.)
        constant_init(self.now_attention_weights, val=0., bias=0.)
        constant_init(self.next_attention_weights, val=0., bias=0.)
        
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        xavier_init(self.output_proj, distribution='uniform', bias=0.)
        xavier_init(self.merge, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)
            

        bs, num_query, _ = query.shape
        bs, num_key, _ = value.shape
        
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        value = self.value_proj(value)
        
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
        value = value.view(bs, num_key, self.num_heads, -1)
        
        # TODO 切分为三份
        bs = bs // 3
        # 提取不同帧的特征
        pre_value = value[0::3].contiguous()
        now_value = value[1::3].contiguous()
        next_value = value[2::3].contiguous()
        
        pre_sampling_offsets = self.pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        now_sampling_offsets = self.now_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_sampling_offsets = self.next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
        pre_attention_weights = self.pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        now_attention_weights = self.now_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_attention_weights = self.next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        
        
        pre_attention_weights = pre_attention_weights.softmax(-1)
        now_attention_weights = now_attention_weights.softmax(-1)
        next_attention_weights = next_attention_weights.softmax(-1)

        pre_attention_weights = pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        now_attention_weights = now_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        next_attention_weights = next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        if reference_points.shape[-1] == self.num_points * 2:
            reference_points_reshape = reference_points.reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            pre_sampling_locations = reference_points_reshape \
                                 + pre_sampling_offsets * wh * 0.5

            now_sampling_locations = reference_points_reshape \
                                 + now_sampling_offsets * wh * 0.5

            next_sampling_locations = reference_points_reshape \
                                 + next_sampling_offsets * wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
        if torch.cuda.is_available():
            pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = MultiScaleDeformableAttnFunction.apply(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = MultiScaleDeformableAttnFunction.apply(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
            
        else:
            pre_output = multi_scale_deformable_attn_pytorch(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = multi_scale_deformable_attn_pytorch(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = multi_scale_deformable_attn_pytorch(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
        
        # TODO 合并三帧的query
        output = self.merge(torch.concat([pre_output, now_output, next_output], dim=-1))
        
        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual

# 时序多尺度可变形attention v2 不同帧value也使用不同的线性层
@ATTENTION.register_module()
class MulFramesMultiScaleDeformablePoseAttentionV2(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        # 修改
        # 前一帧
        self.pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 当前帧
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后一帧
        self.next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        
        self.pre_value_proj = nn.Linear(embed_dims, embed_dims)
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        self.next_value_proj = nn.Linear(embed_dims, embed_dims)
        
        
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        
        # TODO 合并多帧query
        self.merge = nn.Linear(embed_dims*3, embed_dims)
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.pre_sampling_offsets, 0.)
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.next_sampling_offsets, 0.)
        
        constant_init(self.pre_attention_weights, val=0., bias=0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        constant_init(self.next_attention_weights, val=0., bias=0.)
        
        xavier_init(self.pre_value_proj, distribution='uniform', bias=0.)
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        xavier_init(self.next_value_proj, distribution='uniform', bias=0.)
        
        xavier_init(self.output_proj, distribution='uniform', bias=0.)
        xavier_init(self.merge, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)
            

        bs, num_query, _ = query.shape
        bs, num_key, _ = value.shape
        
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
        # TODO 切分为三份
        bs = bs // 3
        # 提取不同帧的特征
        pre_value = value[0::3]
        now_value = value[1::3]
        next_value = value[2::3]
        pre_value = self.pre_value_proj(pre_value)
        now_value = self.value_proj(now_value)
        next_value = self.next_value_proj(next_value)
        
            
        pre_value = pre_value.view(bs, num_key, self.num_heads, -1)
        now_value = now_value.view(bs, num_key, self.num_heads, -1)
        next_value = next_value.view(bs, num_key, self.num_heads, -1)
        
        pre_value = pre_value.contiguous()
        now_value = now_value.contiguous()
        next_value = next_value.contiguous()
        
        
        pre_sampling_offsets = self.pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        now_sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_sampling_offsets = self.next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
        pre_attention_weights = self.pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        now_attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_attention_weights = self.next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        
        
        pre_attention_weights = pre_attention_weights.softmax(-1)
        now_attention_weights = now_attention_weights.softmax(-1)
        next_attention_weights = next_attention_weights.softmax(-1)

        pre_attention_weights = pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        now_attention_weights = now_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        next_attention_weights = next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        if reference_points.shape[-1] == self.num_points * 2:
            reference_points_reshape = reference_points.reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            pre_sampling_locations = reference_points_reshape \
                                 + pre_sampling_offsets * wh * 0.5

            now_sampling_locations = reference_points_reshape \
                                 + now_sampling_offsets * wh * 0.5

            next_sampling_locations = reference_points_reshape \
                                 + next_sampling_offsets * wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
        if torch.cuda.is_available():
            pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = MultiScaleDeformableAttnFunction.apply(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = MultiScaleDeformableAttnFunction.apply(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
            
        else:
            pre_output = multi_scale_deformable_attn_pytorch(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = multi_scale_deformable_attn_pytorch(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = multi_scale_deformable_attn_pytorch(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
        
        # TODO 合并三帧的query
        output = self.merge(torch.concat([pre_output, now_output, next_output], dim=-1))
        
        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual

# 时序多尺度可变形attention v3 不同帧value也使用不同的线性层, 提取辅助帧信息使用attention
@ATTENTION.register_module()
class MulFramesMultiScaleDeformablePoseAttentionV3(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        # 修改
        # 前一帧
        self.pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 当前帧
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后一帧
        self.next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        
        self.pre_value_proj = nn.Linear(embed_dims, embed_dims)
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        self.next_value_proj = nn.Linear(embed_dims, embed_dims)
        
        
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        
        # TODO 合并多帧query
        self.merge = nn.MultiheadAttention(embed_dim=embed_dims, num_heads=num_heads, dropout=0.1)
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.pre_sampling_offsets, 0.)
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.next_sampling_offsets, 0.)
        
        constant_init(self.pre_attention_weights, val=0., bias=0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        constant_init(self.next_attention_weights, val=0., bias=0.)
        
        xavier_init(self.pre_value_proj, distribution='uniform', bias=0.)
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        xavier_init(self.next_value_proj, distribution='uniform', bias=0.)
        
        xavier_init(self.output_proj, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)
            

        bs, num_query, _ = query.shape
        bs, num_key, _ = value.shape
        
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
        # TODO 切分为三份
        bs = bs // 3
        # 提取不同帧的特征
        pre_value = value[0::3]
        now_value = value[1::3]
        next_value = value[2::3]
        pre_value = self.pre_value_proj(pre_value)
        now_value = self.value_proj(now_value)
        next_value = self.next_value_proj(next_value)
        
            
        pre_value = pre_value.view(bs, num_key, self.num_heads, -1)
        now_value = now_value.view(bs, num_key, self.num_heads, -1)
        next_value = next_value.view(bs, num_key, self.num_heads, -1)
        
        pre_value = pre_value.contiguous()
        now_value = now_value.contiguous()
        next_value = next_value.contiguous()
        
        
        pre_sampling_offsets = self.pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        now_sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_sampling_offsets = self.next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
        pre_attention_weights = self.pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        now_attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_attention_weights = self.next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        
        
        pre_attention_weights = pre_attention_weights.softmax(-1)
        now_attention_weights = now_attention_weights.softmax(-1)
        next_attention_weights = next_attention_weights.softmax(-1)

        pre_attention_weights = pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        now_attention_weights = now_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        next_attention_weights = next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        if reference_points.shape[-1] == self.num_points * 2:
            reference_points_reshape = reference_points.reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            pre_sampling_locations = reference_points_reshape \
                                 + pre_sampling_offsets * wh * 0.5

            now_sampling_locations = reference_points_reshape \
                                 + now_sampling_offsets * wh * 0.5

            next_sampling_locations = reference_points_reshape \
                                 + next_sampling_offsets * wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
        if torch.cuda.is_available():
            pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = MultiScaleDeformableAttnFunction.apply(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = MultiScaleDeformableAttnFunction.apply(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
            
        else:
            pre_output = multi_scale_deformable_attn_pytorch(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = multi_scale_deformable_attn_pytorch(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = multi_scale_deformable_attn_pytorch(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
        
        # TODO 合并三帧的query
        output = torch.stack([pre_output, now_output, next_output], dim=1).transpose(0, 1).flatten(1, 2)
        q = k = v = output
        output = self.merge(
                    query=q,
                    key=k,
                    value=v)[0].reshape(-1, bs, num_query, self.embed_dims)[1]
        
        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual


# 添加时间2024-10-26 适用于pose-decoder时空解码方法 ------------- 时序多尺度可变形attention v4 不同帧value也使用不同的线性层, 提取辅助帧信息使用三帧信息加权
@ATTENTION.register_module()
class MulFramesMultiScaleDeformablePoseAttentionV4(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        # 修改
        # 前一帧
        self.pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 当前帧
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后一帧
        self.next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        
        self.pre_value_proj = nn.Linear(embed_dims, embed_dims)
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        self.next_value_proj = nn.Linear(embed_dims, embed_dims)
        
        
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.pre_sampling_offsets, 0.)
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.next_sampling_offsets, 0.)
        
        constant_init(self.pre_attention_weights, val=0., bias=0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        constant_init(self.next_attention_weights, val=0., bias=0.)
        
        xavier_init(self.pre_value_proj, distribution='uniform', bias=0.)
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        xavier_init(self.next_value_proj, distribution='uniform', bias=0.)
        
        xavier_init(self.output_proj, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                query_time_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)
            

        bs, num_query, _ = query.shape
        bs, num_key, _ = value.shape
        
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
        # TODO 切分为三份
        bs = bs // 3
        # 提取不同帧的特征
        pre_value = value[0::3]
        now_value = value[1::3]
        next_value = value[2::3]
        pre_value = self.pre_value_proj(pre_value)
        now_value = self.value_proj(now_value)
        next_value = self.next_value_proj(next_value)
        
            
        pre_value = pre_value.view(bs, num_key, self.num_heads, -1)
        now_value = now_value.view(bs, num_key, self.num_heads, -1)
        next_value = next_value.view(bs, num_key, self.num_heads, -1)
        
        pre_value = pre_value.contiguous()
        now_value = now_value.contiguous()
        next_value = next_value.contiguous()
        
        
        pre_sampling_offsets = self.pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        now_sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_sampling_offsets = self.next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
        pre_attention_weights = self.pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        now_attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_attention_weights = self.next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        
        # BUG ------可能会出现错误
        pre_attention_weights_sum = torch.exp(pre_attention_weights).sum(-1, keepdim=True)
        now_attention_weights_sum = torch.exp(now_attention_weights).sum(-1, keepdim=True)
        next_attention_weights_sum = torch.exp(next_attention_weights).sum(-1, keepdim=True)
        sum_all = pre_attention_weights_sum + now_attention_weights_sum + next_attention_weights_sum
        # BUG
        
        pre_attention_weights = pre_attention_weights.softmax(-1)
        now_attention_weights = now_attention_weights.softmax(-1)
        next_attention_weights = next_attention_weights.softmax(-1)
        

        pre_attention_weights = pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        now_attention_weights = now_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        next_attention_weights = next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        if reference_points.shape[-1] == self.num_points * 2:
            reference_points_reshape = reference_points.reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            pre_sampling_locations = reference_points_reshape \
                                 + pre_sampling_offsets * wh * 0.5

            now_sampling_locations = reference_points_reshape \
                                 + now_sampling_offsets * wh * 0.5

            next_sampling_locations = reference_points_reshape \
                                 + next_sampling_offsets * wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
        if torch.cuda.is_available():
            pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = MultiScaleDeformableAttnFunction.apply(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = MultiScaleDeformableAttnFunction.apply(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
            
        else:
            pre_output = multi_scale_deformable_attn_pytorch(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = multi_scale_deformable_attn_pytorch(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = multi_scale_deformable_attn_pytorch(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
        
        # TODO start: 融合多帧不同参考点对应的feature_token
        pre_output = pre_output.reshape(bs, num_query, self.num_heads, -1)
        now_output = now_output.reshape(bs, num_query, self.num_heads, -1)
        next_output = next_output.reshape(bs, num_query, self.num_heads, -1)
        output = pre_output * (pre_attention_weights_sum / sum_all) + now_output * (now_attention_weights_sum / sum_all) + next_output * (next_attention_weights_sum / sum_all)
        output = output.flatten(-2, -1)
        # TODO end

        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual


# 添加时间2024-10-26 适用于pose-decoder时空解码方法 ------------- 时序多尺度可变形attention v4 不同帧value也使用不同的线性层, 提取辅助帧信息使用三帧信息加权
@ATTENTION.register_module()
class MulFramesMultiScaleDeformablePoseAttentionNumFrames3(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 num_frames=3,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first
        # 记录当前处理到第几个样本
        self.tag = 1

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        self.num_frames = num_frames
        # 修改
        # 前一帧
        self.pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 当前帧
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后一帧
        self.next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.pre_sampling_offsets, 0.)
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.next_sampling_offsets, 0.)
        
        constant_init(self.pre_attention_weights, val=0., bias=0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        constant_init(self.next_attention_weights, val=0., bias=0.)

        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        xavier_init(self.output_proj, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                query_time_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)
            

        bs, num_query, _ = query.shape # shape: 300，256
        bs, num_key, _ = value.shape # shape: 3, 10000, 256
        
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
        # TODO 切分为3份
        bs = bs // self.num_frames
        value = self.value_proj(value)
        # 提取不同帧的特征
        pre_value = value[0::self.num_frames]
        now_value = value[1::self.num_frames]
        next_value = value[2::self.num_frames]
        
        # batch_size批数据中不同帧的feature-tokens
        pre_value = pre_value.view(bs, num_key, self.num_heads, -1).contiguous()
        now_value = now_value.view(bs, num_key, self.num_heads, -1).contiguous()
        next_value = next_value.view(bs, num_key, self.num_heads, -1).contiguous()
        
        pre_sampling_offsets = self.pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        now_sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_sampling_offsets = self.next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
        pre_attention_weights = self.pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        now_attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_attention_weights = self.next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        
        # BUG ------可能会出现错误
        pre_attention_weights_sum = torch.exp(pre_attention_weights).sum(-1, keepdim=True)
        now_attention_weights_sum = torch.exp(now_attention_weights).sum(-1, keepdim=True)
        next_attention_weights_sum = torch.exp(next_attention_weights).sum(-1, keepdim=True)
        sum_all = pre_attention_weights_sum + now_attention_weights_sum + next_attention_weights_sum
        # BUG
        
        pre_attention_weights = pre_attention_weights.softmax(-1)
        now_attention_weights = now_attention_weights.softmax(-1)
        next_attention_weights = next_attention_weights.softmax(-1)
        

        pre_attention_weights = pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        now_attention_weights = now_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        next_attention_weights = next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        if reference_points.shape[-1] == self.num_points * 2:
            # 划分不同帧的初始参考点
            pre_reference_points_reshape = reference_points[:, 0:num_query].reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :num_query, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :num_query, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :num_query, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :num_query, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            pre_wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]
            
            now_reference_points_reshape = reference_points[:, num_query:num_query*2].reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, num_query:num_query*2, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, num_query:num_query*2, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, num_query:num_query*2, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, num_query:num_query*2, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]
            
            next_reference_points_reshape = reference_points[:, num_query*2:].reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            
            x1 = reference_points[:, num_query*2:, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, num_query*2:, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, num_query*2:, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, num_query*2:, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            next_wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]


            # # 可视化采样位置
            # import cv2 as cv
            # # 读取图片
            # img_path =  '/datasets/17/rename/images_renamed/bonn_5sec/008760_mpii/00000001.jpg'
            # img = cv.imread(img_path)
            # print(f'原始图像大小为：h:{img.shape[0]},w:{img.shape[1]}')
            # 三帧采样点，这是300个pose的，先可视化前10个看看效果，shape： 
            pre_sampling_locations = pre_reference_points_reshape \
                                 + pre_sampling_offsets * pre_wh * 0.5

            now_sampling_locations = now_reference_points_reshape \
                                 + now_sampling_offsets * wh * 0.5

            next_sampling_locations = next_reference_points_reshape \
                                 + next_sampling_offsets * next_wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')

        # 可视化
        if self.tag % 3 == 0: #
            pass
        # self.vis_attention(pre_attention_weights, pre_sampling_locations, "pre")
        # self.vis_attention(now_attention_weights, now_sampling_locations, "now")
        # self.vis_attention(next_attention_weights, next_sampling_locations, "next")
        # self.visualize_all_attention_points_per_head(
        #     pos=now_sampling_locations,
        #     weight=now_attention_weights,
        #     output_dir="pure_heatmaps/",
        #     image_size=(1080, 1920),
        #     query_id=0
        # )
        if torch.cuda.is_available():
            pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = MultiScaleDeformableAttnFunction.apply(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = MultiScaleDeformableAttnFunction.apply(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
            
        else:
            pre_output = multi_scale_deformable_attn_pytorch(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = multi_scale_deformable_attn_pytorch(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = multi_scale_deformable_attn_pytorch(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
        
        # TODO start: 融合多帧不同参考点对应的feature_token
        pre_output = pre_output.reshape(bs, num_query, self.num_heads, -1)
        now_output = now_output.reshape(bs, num_query, self.num_heads, -1)
        next_output = next_output.reshape(bs, num_query, self.num_heads, -1)
        output = pre_output * (pre_attention_weights_sum / sum_all) + now_output * (now_attention_weights_sum / sum_all) + next_output * (next_attention_weights_sum / sum_all)
        output = output.flatten(-2, -1)
        # TODO end

        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual


    def visualize_all_attention_points_per_head(self, pos, weight, output_dir, image_size=(1080, 1920),
                                                query_id=0):
        """
        每个注意力头可视化所有点（4组 × 15个）为热力图
        参数:
            pos: Tensor (1, 300, 8, 4, 15, 2) - 相对位置 (x, y ∈ [0,1])
            weight: Tensor 同上 - 对应的权重
            output_dir: 保存文件夹
            image_size: 原图尺寸 (H, W)
            query_id: 第几个 query (0~299)
        """
        import numpy as np
        import torch
        import cv2
        import os
        os.makedirs(output_dir, exist_ok=True)
        h, w = image_size
        max_radius=5

        pos_sel = pos[0, query_id]       # shape: (8, 4, 15, 2)
        weight_sel = weight[0, query_id] # shape: (8, 4, 15)
        canvas_raw = np.zeros((h, w), dtype=np.float32)

        for head in range(8):
            for p in range(4):
                for i in range(15):
                    x_rel, y_rel = pos_sel[head, p, i]
                    x = int(x_rel.item() * w)
                    y = int(y_rel.item() * h)

                    if 0 <= x < w and 0 <= y < h:
                        w_val = weight_sel[head, p, i].item()
                        canvas_raw[y, x] += w_val  # 累加权重

        # 3. 归一化累加权重（用于统一亮度和半径）
        # min_val, max_val = canvas_raw.min(), canvas_raw.max()
        # canvas_norm = (canvas_raw - min_val) / (max_val - min_val + 1e-6)  # -> 0~1
        # 2. 计算 top 20% 的阈值
        flat = canvas_raw.flatten()
        threshold = np.percentile(flat[flat > 0], 80)  # top 20%
        # 4. 初始化 RGB 图像
        canvas_rgb = np.zeros((h, w, 3), dtype=np.uint8)

        # 5. 根据归一化权重绘制红色圆点
        ys, xs = np.where(canvas_raw > threshold)
        for y, x in zip(ys, xs):
            radius = 2
            brightness = 255

            cv2.circle(canvas_rgb, (x, y), radius, (brightness, 0, 0), -1)

        # 6. 保存图像
        canvas_bgr = cv2.cvtColor(canvas_rgb, cv2.COLOR_RGB2BGR)
        save_path = os.path.join(output_dir, f"query{query_id}_accumulated_unified.png")
        cv2.imwrite(save_path, canvas_bgr)
        print(f"Saved: {save_path}")


    
    def vis_attention(self, weights, locations, frame):
        # import numpy as np
        # import cv2

        # # ========== 你的输入 ==========
        # # 原图路径（尺寸为 1080 x 1920）
        # img_path =  '/datasets/17/rename/images_renamed/bonn_5sec/008760_mpii/00000001.jpg'
        # original_image = cv2.imread(img_path)  # 替换路径
        # # original_image = cv2.resize(original_image, (1920, 1080))  # 保证一致

        # # 网络输入尺寸
        # input_h, input_w = 750, 1333

        # # Attention输入（替换为你实际数据）
        # sampling_locations = locations
        # sampling_weights = weights

        # # Feature map 尺寸
        # feature_map_shapes = [(94, 167), (47, 84), (24, 42), (12, 21)]
        # downsample_scales = [(input_h / fh, input_w / fw) for fh, fw in feature_map_shapes]

        # # ====== 初始化热图（网络输入尺寸）======
        # heatmap = np.zeros((input_h, input_w), dtype=np.float32)
        
        # query_ids = [0]
        # # ====== 遍历所有 query，构建 heatmap ======
        # for query_idx in range(len(query_ids)):
        #     for head in range(8):
        #         for level in range(4):
        #             scale_y, scale_x = downsample_scales[level]
        #             for pt in range(15):
        #                 xy = sampling_locations[0, query_idx, head, level, pt]  # (2,)
        #                 w = sampling_weights[0, query_idx, head, level, pt]     # scalar

        #                 # x = int(xy[0] * scale_x * input_w)
        #                 # y = int(xy[1] * scale_y * input_h)
        #                 x = int(xy[0]*input_w)  # 直接使用，无需缩放
        #                 y = int(xy[1]*input_h)


        #                 if 0 <= x < input_w and 0 <= y < input_h:
        #                     heatmap[y, x] += w

        # # ====== 高斯模糊 & 归一化 ======
        # # heatmap_blur = cv2.GaussianBlur(heatmap, (15, 15), 0)
        # heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

        # # 伪彩色
        # heatmap_color = cv2.applyColorMap(heatmap_norm, cv2.COLORMAP_JET)

        # # Resize 到原图尺寸
        # heatmap_resized = cv2.resize(heatmap_color, (1920, 1080), interpolation=cv2.INTER_LINEAR)

        # # ====== 融合到原图上 ======
        # alpha = 0.2  # 原图透明度
        # beta = 0.8   # 热图透明度
        # overlay = cv2.addWeighted(original_image, alpha, heatmap_resized, beta, 0)

        # # 保存结果
        # save_path = "attention_overlay_one_queries_82.png"
        # cv2.imwrite(save_path, overlay)
        # print(f"融合热图（所有query）保存为：{save_path}")


        # import numpy as np
        # import cv2

        # img_path = '/datasets/17/rename/images_renamed/bonn_5sec/008760_mpii/00000001.jpg'
        # original_image = cv2.imread(img_path)

        # input_h, input_w = 750, 1333
        # sampling_locations = locations
        # sampling_weights = weights

        # heatmap = np.zeros((input_h, input_w), dtype=np.float32)

        # query_ids = [0]
        # threshold_ratio = 0.3  # 只显示 top 30% 权重

        # # 预提取所有权重，找出最大值
        # all_weights = sampling_weights[0, query_ids].reshape(-1).cpu().numpy()
        # threshold = np.percentile(all_weights, 100 * (1 - threshold_ratio))  # top 30% 的下限

        # for query_idx in query_ids:
        #     for head in range(8):
        #         for level in range(4):
        #             for pt in range(15):
        #                 xy = sampling_locations[0, query_idx, head, level, pt]
        #                 w = sampling_weights[0, query_idx, head, level, pt].item()
        #                 if w < threshold: continue  # 过滤低权重

        #                 x = int(xy[0] * input_w)
        #                 y = int(xy[1] * input_h)

        #                 if 0 <= x < input_w and 0 <= y < input_h:
        #                     heatmap[y, x] = max(heatmap[y, x], w)  # 取最大值代替累加，更利于动态范围

        # # 动态范围放大：提升亮度感
        # heatmap *= 255.0
        # heatmap = np.clip(heatmap, 0, 255).astype(np.uint8)

        # # 伪彩色映射
        # heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

        # # Resize 回原图尺寸
        # heatmap_resized = cv2.resize(heatmap_color, (1920, 1080), interpolation=cv2.INTER_LINEAR)

        # # 融合
        # alpha, beta = 0.4, 0.6
        # overlay = cv2.addWeighted(original_image, alpha, heatmap_resized, beta, 0)

        # save_path = "attention_overlay_dynamic_brightness.png"
        # cv2.imwrite(save_path, overlay)
        # print(f"亮度动态调整热图保存成功：{save_path}")

        # import numpy as np
        # import cv2

        # # ========== 输入 ==========
        # img_path = '/datasets/17/rename/images_renamed/bonn_5sec/008760_mpii/00000001.jpg'
        # original_image = cv2.imread(img_path)
        # input_h, input_w = 750, 1333

        # sampling_locations = locations      # shape: (1, 300, 8, 4, 15, 2)
        # sampling_weights = weights          # shape: (1, 300, 8, 4, 15)

        # query_ids = [0]  # 要显示的 query

        # # ========== 设置参数 ==========
        # min_radius = 4
        # max_radius = 15
        # display_ratio = 0.3  # 仅显示 top 30% 权重
        # base_color = np.array([0, 0, 255], dtype=np.uint8)  # 红色 (BGR)

        # # ========== 提取并计算阈值 ==========
        # all_weights = sampling_weights[0, query_ids].reshape(-1).cpu().numpy()
        # threshold = np.percentile(all_weights, 100 * (1 - display_ratio))

        # # ========== 创建叠加图（透明背景） ==========
        # canvas = original_image.copy()

        # # ========== 遍历注意力点 ==========
        # for query_idx in query_ids:
        #     for head in range(8):
        #         for level in range(4):
        #             for pt in range(15):
        #                 xy = sampling_locations[0, query_idx, head, level, pt]
        #                 w = sampling_weights[0, query_idx, head, level, pt].item()

        #                 if w < threshold:
        #                     continue

        #                 x = int(xy[0] * input_w)
        #                 y = int(xy[1] * input_h)
        #                 if not (0 <= x < input_w and 0 <= y < input_h):
        #                     continue

        #                 # 半径 & 亮度增强
        #                 radius = int(min_radius + (max_radius - min_radius) * w)
        #                 intensity = int(180 + 75 * w)  # 提高亮度，最大255

        #                 # BGR亮色（纯色 + 高亮度）
        #                 color = np.array([0, 0, intensity], dtype=np.uint8)  # 红色亮度增强
        #                 color_tuple = tuple(int(c) for c in color)

        #                 # 放大到原图尺寸坐标
        #                 x_ori = int(x * 1920 / input_w)
        #                 y_ori = int(y * 1080 / input_h)

        #                 # 绘制半透明圆
        #                 overlay = canvas.copy()
        #                 cv2.circle(overlay, (x_ori, y_ori), radius, color_tuple, -1)
        #                 alpha = 0.6
        #                 canvas = cv2.addWeighted(overlay, alpha, canvas, 1 - alpha, 0)

        # # ========== 保存 ==========
        # save_path = "attention_highlight_points_yuv_bright.png"
        # cv2.imwrite(save_path, canvas)
        # print(f"注意力亮点图已保存：{save_path}")

        import numpy as np
        import cv2


        # # 原图
        # if frame == 'pre':
        #     img_path = '/datasets/17/rename/images_renamed/bonn_mpii_test_5sec/16236_mpii/00000020.jpg'
        # if frame == 'now':
        #     img_path = '/datasets/17/rename/images_renamed/bonn_mpii_test_5sec/16236_mpii/00000021.jpg'
        # if frame == 'next':
        #     img_path = '/datasets/17/rename/images_renamed/bonn_mpii_test_5sec/16236_mpii/00000022.jpg'
        # original_image = cv2.imread(img_path)

        # Skip visualization if image doesn't exist (e.g., during demo inference)
        # if original_image is None:
        #     return

        input_h, input_w = 750, 1333

        sampling_locations = locations
        sampling_weights = weights

        # query_ids = [i for i in range(10, 20)]
        # for id in r):
        query_ids = [2] # 18

        # 初始化热图
        heatmap = np.zeros((input_h, input_w), dtype=np.float32)

        # 收集点 & 累加热度
        for query_idx in query_ids:
            for head in range(8):
                for level in range(4):
                    for pt in range(15):
                        xy = sampling_locations[0, query_idx, head, level, pt]
                        w = sampling_weights[0, query_idx, head, level, pt].item()
                        x = int(xy[0] * input_w)
                        y = int(xy[1] * input_h)
                        if 0 <= x < input_w and 0 <= y < input_h:
                            heatmap[y, x] += w

        # 归一化热图
        heatmap = np.power(heatmap, 1.5)
        heatmap = cv2.GaussianBlur(heatmap, (25, 25), 0)
        heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)
        heatmap_norm = heatmap_norm.astype(np.uint8)

        # 设置阈值（低于30%置零）
        threshold_val = np.percentile(heatmap_norm, 60)
        # heatmap_norm[heatmap_norm < threshold_val] = 0

        # 应用伪彩色
        color_map = cv2.applyColorMap(heatmap_norm, cv2.COLORMAP_INFERNO)

        # Resize 到原图
        color_map_resized = cv2.resize(color_map, (original_image.shape[1], original_image.shape[0]))

        # 叠加
        alpha = 0.3
        beta = 0.7
        overlay = cv2.addWeighted(original_image, alpha, color_map_resized, beta, 0)

        # 保存
        cv2.imwrite(f"attention_vis/test_attention_colormap_style_overlay_{frame}.png", overlay)
        print("已保存：attention_colormap_style_overlay.png")







        # import numpy as np
        # import cv2

        # # ========== 1. 原图读取 ==========
        # img_path = '/datasets/17/rename/images_renamed/bonn_5sec/008760_mpii/00000001.jpg'
        # original_image = cv2.imread(img_path)  # 原始 BGR 图像
        # h_orig, w_orig = original_image.shape[:2]

        # # ========== 2. 初始化注意力灰度图 ==========
        # input_h, input_w = 750, 1333
        # canvas = np.zeros((input_h, input_w), dtype=np.float32)
        # query_ids = [0]
        # threshold = 0
        # sampling_locations = locations
        # sampling_weights = weights

        # # ========== 3. 构建注意力图 ==========
        # for query_idx in query_ids:
        #     for head in range(8):
        #         for level in range(4):
        #             for pt in range(15):
        #                 xy = sampling_locations[0, query_idx, head, level, pt]
        #                 w = sampling_weights[0, query_idx, head, level, pt].item()

        #                 if w < threshold:
        #                     continue

        #                 x = int(xy[0] * input_w)
        #                 y = int(xy[1] * input_h)

        #                 if 0 <= x < input_w and 0 <= y < input_h:
        #                     radius = int(2 + 8 * w)
        #                     value = w * 255
        #                     cv2.circle(canvas, (x, y), radius, value, -1)

        # # ========== 4. Resize 到原图大小 ==========
        # canvas = cv2.normalize(canvas, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)
        # attention_resized = cv2.resize(canvas, (w_orig, h_orig))

        # # ========== 5. 转换原图到 YUV ==========
        # yuv_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2YUV)
        # Y, U, V = cv2.split(yuv_image)

        # # ========== 6. 增强 V 通道：红色增强 ==========
        # V = cv2.addWeighted(V, 1.0, attention_resized, 0.8, 0)  # 0.8 控制红色增强强度

        # # ========== 7. 合并并转回 BGR ==========
        # yuv_modified = cv2.merge((Y, U, V))
        # bgr_result = cv2.cvtColor(yuv_modified, cv2.COLOR_YUV2BGR)

        # # ========== 8. 保存 ==========
        # cv2.imwrite("attention_red_overlay_yuv.png", bgr_result)
        # print("带红色注意力热图保存成功：attention_red_overlay_yuv.png")







        # import numpy as np
        # import cv2

        # # ========== 你的输入 ==========
        # img_path = '/datasets/17/rename/images_renamed/bonn_5sec/008760_mpii/00000001.jpg'
        # original_image = cv2.imread(img_path)

        # # 网络输入尺寸
        # input_h, input_w = 750, 1333

        # # Attention输入
        # sampling_locations = locations       # shape: (1, 300, 8, 4, 15, 2)
        # sampling_weights = weights           # shape: (1, 300, 8, 4, 15)

        # # ====== 初始化热图（网络输入尺寸）======
        # heatmap = np.zeros((input_h, input_w), dtype=np.float32)

        # query_ids = [0]  # 可改成多个 query，比如 [0, 1, 2]

        # # ====== 遍历指定 query 构建热图 ======
        # for query_idx in query_ids:
        #     for head in range(8):
        #         for level in range(4):
        #             for pt in range(15):
        #                 xy = sampling_locations[0, query_idx, head, level, pt]
        #                 w = sampling_weights[0, query_idx, head, level, pt]

        #                 x = int(xy[0] * input_w)
        #                 y = int(xy[1] * input_h)

        #                 if 0 <= x < input_w and 0 <= y < input_h:
        #                     heatmap[y, x] += w

        # # ====== 高斯模糊 & 归一化 ======
        # # heatmap_blur = cv2.GaussianBlur(heatmap, (15, 15), 0)
        # heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)

        # # ====== YUV 固定红色调叠加 ======
        # # Step 1: 构建三通道灰度图
        # heatmap_gray3 = cv2.merge([heatmap_norm] * 3)

        # # Step 2: 转为 YUV 颜色空间
        # heatmap_yuv = cv2.cvtColor(heatmap_gray3, cv2.COLOR_BGR2YUV)

        # # Step 3: 设置固定色调为红色（U ≈ 85, V ≈ 255）
        # heatmap_yuv[:, :, 1] = 85    # U
        # heatmap_yuv[:, :, 2] = 255   # V

        # # Step 4: 转回 BGR
        # heatmap_color = cv2.cvtColor(heatmap_yuv, cv2.COLOR_YUV2BGR)

        # # Step 5: resize 到原图大小
        # heatmap_resized = cv2.resize(heatmap_color, (original_image.shape[1], original_image.shape[0]))

        # # ====== 融合到原图上 ======
        # alpha = 0.6  # 原图透明度
        # beta = 0.4   # 热图透明度
        # overlay = cv2.addWeighted(original_image, alpha, heatmap_resized, beta, 0)

        # # 保存结果
        # save_path = "attention_overlay_yuv_red.png"
        # cv2.imwrite(save_path, overlay)
        # print(f"融合热图（YUV红色调）保存为：{save_path}")


        # import numpy as np
        # import cv2

        # # ========== 输入 ========== 
        # img_path = '/datasets/17/rename/images_renamed/bonn_5sec/008760_mpii/00000001.jpg'
        # original_image = cv2.imread(img_path)  # 原图尺寸：1080x1920

        # # 网络输入尺寸
        # input_h, input_w = 750, 1333

        # # Attention 输入
        # sampling_locations = locations       # shape: (1, 300, 8, 4, 15, 2)
        # sampling_weights = weights           # shape: (1, 300, 8, 4, 15)

        # query_ids = [0]  # 可视化的 query id 列表

        # # ========== 初始化白色画布 ========== 
        # canvas = np.ones((input_h, input_w, 3), dtype=np.uint8) * 255

        # # ========== 参数 ========== 
        # min_radius = 2
        # max_radius = 10

        # # 基础颜色（RGB）
        # base_color_rgb = np.array([255, 0, 0], dtype=np.uint8)  # 红色 (BGR)

        # # ========== 计算权重阈值，筛除底部70% ==========
        # all_weights = []

        # for query_idx in query_ids:
        #     for head in range(8):
        #         for level in range(4):
        #             for pt in range(15):
        #                 w = sampling_weights[0, query_idx, head, level, pt].item()
        #                 if w > 0:
        #                     all_weights.append(w)

        # if len(all_weights) == 0:
        #     threshold = 1.0  # 如果全是0，设置为1避免错误
        # else:
        #     threshold = np.percentile(all_weights, 85)  # 保留 top 10%

        # # 转换为 YUV
        # base_color_yuv = cv2.cvtColor(base_color_rgb.reshape(1, 1, 3), cv2.COLOR_RGB2YUV).reshape(3)

        # # 遍历注意力点
        # for query_idx in query_ids:
        #     for head in range(8):
        #         for level in range(4):
        #             for pt in range(15):
        #                 xy = sampling_locations[0, query_idx, head, level, pt]
        #                 w = sampling_weights[0, query_idx, head, level, pt].item()

        #                 if w <= threshold:
        #                     continue  # 忽略低于阈值的点


        #                 x = int(xy[0] * input_w)
        #                 y = int(xy[1] * input_h)

        #                 if 0 <= x < input_w and 0 <= y < input_h and w > 0:
        #                     # 计算半径和 Y 值（亮度）
        #                     radius = int(min_radius + (max_radius - min_radius) * w)
        #                     y_value = base_color_yuv[0] + w * 100  # 增加亮度（Y通道）
        #                     y_value = min(max(y_value, 0), 255)   # 确保 Y 值在合理范围内

        #                     # 设定 U 和 V 通道保持不变
        #                     u_value = base_color_yuv[1]
        #                     v_value = base_color_yuv[2]

        #                     # 构建 YUV 颜色，并转换回 RGB
        #                     color_yuv = np.array([y_value, u_value, v_value], dtype=np.uint8)
        #                     color_rgb = cv2.cvtColor(color_yuv.reshape(1, 1, 3), cv2.COLOR_YUV2RGB).reshape(3)

        #                     color_tuple = tuple(int(c) for c in color_rgb)

        #                     # 绘制圆形
        #                     cv2.circle(canvas, (x, y), radius, color_tuple, -1)

        # # Resize 到原图大小
        # canvas_resized = cv2.resize(canvas, (original_image.shape[1], original_image.shape[0]), interpolation=cv2.INTER_LINEAR)

        # # 融合叠加
        # alpha = 0.6
        # beta = 0.4
        # overlay = cv2.addWeighted(original_image, alpha, canvas_resized, beta, 0)

        # # 保存
        # save_path = "attention_point_yuv.png"
        # cv2.imwrite(save_path, overlay)
        # print(f"融合图保存成功：{save_path}")







# 添加时间2024-10-26 适用于pose-decoder时空解码方法 ------------- 时序多尺度可变形attention v4 不同帧value也使用不同的线性层, 提取辅助帧信息使用5帧信息加权
@ATTENTION.register_module()
class MulFramesMultiScaleDeformablePoseAttentionV5(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        self.num_frames = 5
        # 修改
        # 前前帧
        self.pre_pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 前一帧
        self.pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 当前帧
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后一帧
        self.next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后后帧
        self.next_next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        
        # self.pre_value_proj = nn.Linear(embed_dims, embed_dims)
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        # self.next_value_proj = nn.Linear(embed_dims, embed_dims)
        
        
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.pre_pre_sampling_offsets, 0.)
        constant_init(self.pre_sampling_offsets, 0.)
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.next_sampling_offsets, 0.)
        constant_init(self.next_next_sampling_offsets, 0.)
        
        constant_init(self.pre_pre_attention_weights, val=0., bias=0.)
        constant_init(self.pre_attention_weights, val=0., bias=0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        constant_init(self.next_attention_weights, val=0., bias=0.)
        constant_init(self.next_next_attention_weights, val=0., bias=0.)
        
        # xavier_init(self.pre_value_proj, distribution='uniform', bias=0.)
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        # xavier_init(self.next_value_proj, distribution='uniform', bias=0.)
        
        xavier_init(self.output_proj, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                query_time_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)
            

        bs, num_query, _ = query.shape
        bs, num_key, _ = value.shape
        
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
        # TODO 切分为5份
        bs = bs // self.num_frames
        value = self.value_proj(value)
        # 提取不同帧的特征
        pre_pre_value = value[0::self.num_frames]
        pre_value = value[1::self.num_frames]
        now_value = value[2::self.num_frames]
        next_value = value[3::self.num_frames]
        next_next_value = value[4::self.num_frames]
        
        # batch_size批数据中不同帧的feature-tokens
        pre_pre_value = pre_pre_value.view(bs, num_key, self.num_heads, -1).contiguous()
        pre_value = pre_value.view(bs, num_key, self.num_heads, -1).contiguous()
        now_value = now_value.view(bs, num_key, self.num_heads, -1).contiguous()
        next_value = next_value.view(bs, num_key, self.num_heads, -1).contiguous()
        next_next_value = next_next_value.view(bs, num_key, self.num_heads, -1).contiguous()
        
        pre_pre_sampling_offsets = self.pre_pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        pre_sampling_offsets = self.pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        now_sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_sampling_offsets = self.next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_next_sampling_offsets = self.next_next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
        pre_pre_attention_weights = self.pre_pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        pre_attention_weights = self.pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        now_attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_attention_weights = self.next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_next_attention_weights = self.next_next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
              
        # BUG ------可能会出现错误
        pre_pre_attention_weights_sum = torch.exp(pre_pre_attention_weights).sum(-1, keepdim=True)
        pre_attention_weights_sum = torch.exp(pre_attention_weights).sum(-1, keepdim=True)
        now_attention_weights_sum = torch.exp(now_attention_weights).sum(-1, keepdim=True)
        next_attention_weights_sum = torch.exp(next_attention_weights).sum(-1, keepdim=True)
        next_next_attention_weights_sum = torch.exp(next_next_attention_weights).sum(-1, keepdim=True)
        
        sum_all = pre_pre_attention_weights_sum + pre_attention_weights_sum + now_attention_weights_sum + next_attention_weights_sum + next_next_attention_weights_sum
        # BUG

        pre_pre_attention_weights = pre_pre_attention_weights.softmax(-1)
        pre_attention_weights = pre_attention_weights.softmax(-1)
        now_attention_weights = now_attention_weights.softmax(-1)
        next_attention_weights = next_attention_weights.softmax(-1)
        next_next_attention_weights = next_next_attention_weights.softmax(-1)


        pre_pre_attention_weights = pre_pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        pre_attention_weights = pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        now_attention_weights = now_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        next_attention_weights = next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        next_next_attention_weights = next_next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        if reference_points.shape[-1] == self.num_points * 2:
            reference_points_reshape = reference_points.reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            pre_pre_sampling_locations = reference_points_reshape \
                                 + pre_pre_sampling_offsets * wh * 0.5
            pre_sampling_locations = reference_points_reshape \
                                 + pre_sampling_offsets * wh * 0.5
            now_sampling_locations = reference_points_reshape \
                                 + now_sampling_offsets * wh * 0.5
            next_sampling_locations = reference_points_reshape \
                                 + next_sampling_offsets * wh * 0.5
            next_next_sampling_locations = reference_points_reshape \
                                 + next_next_sampling_offsets * wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
        if torch.cuda.is_available():
            pre_pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_pre_value, spatial_shapes, level_start_index, pre_pre_sampling_locations,
                pre_pre_attention_weights, self.im2col_step)
            
            pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            
            now_output = MultiScaleDeformableAttnFunction.apply(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            
            next_output = MultiScaleDeformableAttnFunction.apply(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
            
            next_next_output = MultiScaleDeformableAttnFunction.apply(
                next_next_value, spatial_shapes, level_start_index, next_next_sampling_locations,
                next_next_attention_weights, self.im2col_step)
            
        else:
            pre_output = multi_scale_deformable_attn_pytorch(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = multi_scale_deformable_attn_pytorch(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = multi_scale_deformable_attn_pytorch(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
        
        # TODO start: 融合多帧不同参考点对应的feature_token
        pre_pre_output = pre_pre_output.reshape(bs, num_query, self.num_heads, -1)
        pre_output = pre_output.reshape(bs, num_query, self.num_heads, -1)
        now_output = now_output.reshape(bs, num_query, self.num_heads, -1)
        next_output = next_output.reshape(bs, num_query, self.num_heads, -1)
        next_next_output = next_next_output.reshape(bs, num_query, self.num_heads, -1)
        
        output = pre_pre_output * (pre_pre_attention_weights_sum / sum_all) + \
                 pre_output * (pre_attention_weights_sum / sum_all) + \
                 now_output * (now_attention_weights_sum / sum_all) + \
                 next_output * (next_attention_weights_sum / sum_all) + \
                 next_next_output * (next_next_attention_weights_sum / sum_all)
        output = output.flatten(-2, -1)
        # TODO end

        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual


# 添加时间2024-10-26 适用于新pose-decoder时空解码方法 ------------- 时序多尺度可变形attention v4 不同帧value也使用不同的线性层, 提取辅助帧信息使用5帧信息加权
@ATTENTION.register_module()
class MulFramesMultiScaleDeformablePoseAttentionNumFrames5(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        self.num_frames = 5
        # 修改
        # 前前帧
        self.pre_pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 前一帧
        self.pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 当前帧
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后一帧
        self.next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后后帧
        self.next_next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.pre_pre_sampling_offsets, 0.)
        constant_init(self.pre_sampling_offsets, 0.)
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.next_sampling_offsets, 0.)
        constant_init(self.next_next_sampling_offsets, 0.)
        
        constant_init(self.pre_pre_attention_weights, val=0., bias=0.)
        constant_init(self.pre_attention_weights, val=0., bias=0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        constant_init(self.next_attention_weights, val=0., bias=0.)
        constant_init(self.next_next_attention_weights, val=0., bias=0.)
        
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        
        xavier_init(self.output_proj, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                query_time_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)
            

        bs, num_query, _ = query.shape
        bs, num_key, _ = value.shape
        
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
        # TODO 切分为三份
        bs = bs // self.num_frames
        value = self.value_proj(value)
        # print(value.shape)
        # 提取不同帧的特征
        pre_pre_value = value[0::self.num_frames]
        # print(pre_pre_value.shape)
        pre_value = value[1::self.num_frames]
        now_value = value[2::self.num_frames]
        next_value = value[3::self.num_frames]
        next_next_value = value[4::self.num_frames]
        
        # batch_size批数据中不同帧的feature-tokens
        pre_pre_value = pre_pre_value.view(bs, num_key, self.num_heads, -1).contiguous()
        pre_value = pre_value.view(bs, num_key, self.num_heads, -1).contiguous()
        now_value = now_value.view(bs, num_key, self.num_heads, -1).contiguous()
        next_value = next_value.view(bs, num_key, self.num_heads, -1).contiguous()
        next_next_value = next_next_value.view(bs, num_key, self.num_heads, -1).contiguous()
        
        pre_pre_sampling_offsets = self.pre_pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        pre_sampling_offsets = self.pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        now_sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_sampling_offsets = self.next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_next_sampling_offsets = self.next_next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
        pre_pre_attention_weights = self.pre_pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        pre_attention_weights = self.pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        now_attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_attention_weights = self.next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_next_attention_weights = self.next_next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
              
        # BUG ------可能会出现错误
        pre_pre_attention_weights_sum = torch.exp(pre_pre_attention_weights).sum(-1, keepdim=True)
        pre_attention_weights_sum = torch.exp(pre_attention_weights).sum(-1, keepdim=True)
        now_attention_weights_sum = torch.exp(now_attention_weights).sum(-1, keepdim=True)
        next_attention_weights_sum = torch.exp(next_attention_weights).sum(-1, keepdim=True)
        next_next_attention_weights_sum = torch.exp(next_next_attention_weights).sum(-1, keepdim=True)
        
        sum_all = pre_pre_attention_weights_sum + pre_attention_weights_sum + now_attention_weights_sum + next_attention_weights_sum + next_next_attention_weights_sum
        # BUG

        pre_pre_attention_weights = pre_pre_attention_weights.softmax(-1)
        pre_attention_weights = pre_attention_weights.softmax(-1)
        now_attention_weights = now_attention_weights.softmax(-1)
        next_attention_weights = next_attention_weights.softmax(-1)
        next_next_attention_weights = next_next_attention_weights.softmax(-1)


        pre_pre_attention_weights = pre_pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        pre_attention_weights = pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        now_attention_weights = now_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        next_attention_weights = next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        next_next_attention_weights = next_next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        if reference_points.shape[-1] == self.num_points * 2:
            # 获取不同帧的参考点
            pre_pre_reference_points_reshape = reference_points[:, 0:num_query].reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, 0:num_query, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, 0:num_query, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, 0:num_query, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, 0:num_query, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            pre_pre_wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]
            
            pre_reference_points_reshape = reference_points[:, num_query:num_query*2].reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, num_query:num_query*2, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, num_query:num_query*2, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, num_query:num_query*2, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, num_query:num_query*2, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            pre_wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]
            
            now_reference_points_reshape = reference_points[:, num_query*2:num_query*3].reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, num_query*2:num_query*3, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, num_query*2:num_query*3, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, num_query*2:num_query*3, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, num_query*2:num_query*3, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]
            
            next_reference_points_reshape = reference_points[:, num_query*3:num_query*4].reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, num_query*3:num_query*4, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, num_query*3:num_query*4, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, num_query*3:num_query*4, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, num_query*3:num_query*4, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            next_wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]
            
            next_next_reference_points_reshape = reference_points[:, num_query*4:].reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, num_query*4:, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, num_query*4:, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, num_query*4:, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, num_query*4:, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            next_next_wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            pre_pre_sampling_locations = pre_pre_reference_points_reshape \
                                 + pre_pre_sampling_offsets * pre_pre_wh * 0.5
            pre_sampling_locations = pre_reference_points_reshape \
                                 + pre_sampling_offsets * pre_wh * 0.5
            now_sampling_locations = now_reference_points_reshape \
                                 + now_sampling_offsets * wh * 0.5
            next_sampling_locations = next_reference_points_reshape \
                                 + next_sampling_offsets * next_wh * 0.5
            next_next_sampling_locations = next_next_reference_points_reshape \
                                 + next_next_sampling_offsets * next_next_wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
        if torch.cuda.is_available():
            pre_pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_pre_value, spatial_shapes, level_start_index, pre_pre_sampling_locations,
                pre_pre_attention_weights, self.im2col_step)
            
            pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            
            now_output = MultiScaleDeformableAttnFunction.apply(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            
            next_output = MultiScaleDeformableAttnFunction.apply(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
            
            next_next_output = MultiScaleDeformableAttnFunction.apply(
                next_next_value, spatial_shapes, level_start_index, next_next_sampling_locations,
                next_next_attention_weights, self.im2col_step)
            
        else:
            pre_output = multi_scale_deformable_attn_pytorch(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = multi_scale_deformable_attn_pytorch(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = multi_scale_deformable_attn_pytorch(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
        
        # TODO start: 融合多帧不同参考点对应的feature_token
        pre_pre_output = pre_pre_output.reshape(bs, num_query, self.num_heads, -1)
        pre_output = pre_output.reshape(bs, num_query, self.num_heads, -1)
        now_output = now_output.reshape(bs, num_query, self.num_heads, -1)
        next_output = next_output.reshape(bs, num_query, self.num_heads, -1)
        next_next_output = next_next_output.reshape(bs, num_query, self.num_heads, -1)
        
        output = pre_pre_output * (pre_pre_attention_weights_sum / sum_all) + \
                 pre_output * (pre_attention_weights_sum / sum_all) + \
                 now_output * (now_attention_weights_sum / sum_all) + \
                 next_output * (next_attention_weights_sum / sum_all) + \
                 next_next_output * (next_next_attention_weights_sum / sum_all)
        output = output.flatten(-2, -1)
        # TODO end

        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual



# 添加时间2024-10-24 适用于pose-时空解码器
@ATTENTION.register_module()
class MulFramesMultiScaleDeformablePoseAttentionV17(BaseModule):
    
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        # 修改
        # 前一帧
        self.pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 当前帧
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后一帧
        self.next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        
        self.pre_value_proj = nn.Linear(embed_dims, embed_dims)
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        self.next_value_proj = nn.Linear(embed_dims, embed_dims)
        
        
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.pre_sampling_offsets, 0.)
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.next_sampling_offsets, 0.)
        
        constant_init(self.pre_attention_weights, val=0., bias=0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        constant_init(self.next_attention_weights, val=0., bias=0.)
        
        xavier_init(self.pre_value_proj, distribution='uniform', bias=0.)
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        xavier_init(self.next_value_proj, distribution='uniform', bias=0.)
        
        xavier_init(self.output_proj, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                query_time_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
            pre_query = query + query_time_pos[0]
            now_query = query + query_time_pos[1]
            next_query = query + query_time_pos[2]
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            # query = query.permute(1, 0, 2)
            pre_query = pre_query.permute(1, 0, 2)
            now_query = now_query.permute(1, 0, 2)
            next_query = next_query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)
            

        bs, num_query, _ = now_query.shape
        bs, num_key, _ = value.shape
        
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
        # TODO 切分为三份
        bs = bs // 3
        # 提取不同帧的特征
        pre_value = value[0::3]
        now_value = value[1::3]
        next_value = value[2::3]
        pre_value = self.pre_value_proj(pre_value)
        now_value = self.value_proj(now_value)
        next_value = self.next_value_proj(next_value)
        
            
        pre_value = pre_value.view(bs, num_key, self.num_heads, -1)
        now_value = now_value.view(bs, num_key, self.num_heads, -1)
        next_value = next_value.view(bs, num_key, self.num_heads, -1)
        
        pre_value = pre_value.contiguous()
        now_value = now_value.contiguous()
        next_value = next_value.contiguous()
        
        
        pre_sampling_offsets = self.pre_sampling_offsets(pre_query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        now_sampling_offsets = self.sampling_offsets(now_query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_sampling_offsets = self.next_sampling_offsets(next_query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
        pre_attention_weights = self.pre_attention_weights(pre_query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        now_attention_weights = self.attention_weights(now_query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_attention_weights = self.next_attention_weights(next_query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        
        # BUG ------可能会出现错误
        pre_attention_weights_sum = torch.exp(pre_attention_weights).sum(-1, keepdim=True)
        now_attention_weights_sum = torch.exp(now_attention_weights).sum(-1, keepdim=True)
        next_attention_weights_sum = torch.exp(next_attention_weights).sum(-1, keepdim=True)
        sum_all = pre_attention_weights_sum + now_attention_weights_sum + next_attention_weights_sum
        # BUG
        
        pre_attention_weights = pre_attention_weights.softmax(-1)
        now_attention_weights = now_attention_weights.softmax(-1)
        next_attention_weights = next_attention_weights.softmax(-1)
        

        pre_attention_weights = pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        now_attention_weights = now_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)

        next_attention_weights = next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        
        if reference_points.shape[-1] == self.num_points * 2:
            reference_points_reshape = reference_points.reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            pre_sampling_locations = reference_points_reshape \
                                 + pre_sampling_offsets * wh * 0.5

            now_sampling_locations = reference_points_reshape \
                                 + now_sampling_offsets * wh * 0.5

            next_sampling_locations = reference_points_reshape \
                                 + next_sampling_offsets * wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
        if torch.cuda.is_available():
            pre_output = MultiScaleDeformableAttnFunction.apply(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = MultiScaleDeformableAttnFunction.apply(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = MultiScaleDeformableAttnFunction.apply(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
            
        else:
            pre_output = multi_scale_deformable_attn_pytorch(
                pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
                pre_attention_weights, self.im2col_step)
            now_output = multi_scale_deformable_attn_pytorch(
                now_value, spatial_shapes, level_start_index, now_sampling_locations,
                now_attention_weights, self.im2col_step)
            next_output = multi_scale_deformable_attn_pytorch(
                next_value, spatial_shapes, level_start_index, next_sampling_locations,
                next_attention_weights, self.im2col_step)
        
        # TODO start: 融合多帧不同参考点对应的feature_token
        pre_output = pre_output.reshape(bs, num_query, self.num_heads, -1)
        now_output = now_output.reshape(bs, num_query, self.num_heads, -1)
        next_output = next_output.reshape(bs, num_query, self.num_heads, -1)
        output = pre_output * (pre_attention_weights_sum / sum_all) + now_output * (now_attention_weights_sum / sum_all) + next_output * (next_attention_weights_sum / sum_all)
        output = output.flatten(-2, -1)
        # TODO end

        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual
    


# # 时序多尺度可变形attention v5 query使用相同的线性层，不同帧value也使用不同的线性层, 提取辅助帧信息使用三帧信息加权
# @ATTENTION.register_module()
# class MulFramesMultiScaleDeformablePoseAttentionV5(BaseModule):
#     """An attention module used in PETR. `End-to-End Multi-Person
#     Pose Estimation with Transformers`.

#     Args:
#         embed_dims (int): The embedding dimension of Attention.
#             Default: 256.
#         num_heads (int): Parallel attention heads. Default: 8.
#         num_levels (int): The number of feature map used in
#             Attention. Default: 4.
#         num_points (int): The number of sampling points for
#             each query in each head. Default: 17.
#         im2col_step (int): The step used in image_to_column.
#             Default: 64.
#         dropout (float): A Dropout layer on `inp_residual`.
#             Default: 0.1.
#         init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
#             Default: None.
#     """

#     def __init__(self,
#                  embed_dims=256,
#                  num_heads=8,
#                  num_levels=4,
#                  num_points=17,
#                  im2col_step=64,
#                  dropout=0.1,
#                  norm_cfg=None,
#                  init_cfg=None,
#                  batch_first=False):
#         super().__init__(init_cfg)
#         if embed_dims % num_heads != 0:
#             raise ValueError(f'embed_dims must be divisible by num_heads, '
#                              f'but got {embed_dims} and {num_heads}')
#         dim_per_head = embed_dims // num_heads
#         self.norm_cfg = norm_cfg
#         self.init_cfg = init_cfg
#         self.dropout = nn.Dropout(dropout)
#         self.batch_first = batch_first

#         # you'd better set dim_per_head to a power of 2
#         # which is more efficient in the CUDA implementation
#         def _is_power_of_2(n):
#             if (not isinstance(n, int)) or (n < 0):
#                 raise ValueError(
#                     'invalid input for _is_power_of_2: {} (type: {})'.format(
#                         n, type(n)))
#             return (n & (n - 1) == 0) and n != 0

#         if not _is_power_of_2(dim_per_head):
#             warnings.warn(
#                 "You'd better set embed_dims in "
#                 'MultiScaleDeformAttention to make '
#                 'the dimension of each attention head a power of 2 '
#                 'which is more efficient in our CUDA implementation.')

#         self.im2col_step = im2col_step
#         self.embed_dims = embed_dims
#         self.num_levels = num_levels
#         self.num_heads = num_heads
#         self.num_points = num_points
#         # 所有帧共享
#         self.sampling_offsets = nn.Linear(
#             embed_dims, num_heads * num_levels * num_points * 2)
#         self.attention_weights = nn.Linear(embed_dims,
#                                            num_heads * num_levels * num_points)
        
#         self.pre_value_proj = nn.Linear(embed_dims, embed_dims)
#         self.value_proj = nn.Linear(embed_dims, embed_dims)
#         self.next_value_proj = nn.Linear(embed_dims, embed_dims)
        
        
#         self.output_proj = nn.Linear(embed_dims, embed_dims)
        
#         self.init_weights()

#     def init_weights(self):
#         """Default initialization for Parameters of Module."""
#         constant_init(self.sampling_offsets, 0.)
        
#         constant_init(self.attention_weights, val=0., bias=0.)
        
#         xavier_init(self.pre_value_proj, distribution='uniform', bias=0.)
#         xavier_init(self.value_proj, distribution='uniform', bias=0.)
#         xavier_init(self.next_value_proj, distribution='uniform', bias=0.)
        
#         xavier_init(self.output_proj, distribution='uniform', bias=0.)

#     def forward(self,
#                 query,
#                 key,
#                 value,
#                 residual=None,
#                 query_pos=None,
#                 key_padding_mask=None,
#                 reference_points=None,
#                 spatial_shapes=None,
#                 level_start_index=None,
#                 **kwargs):
#         """Forward Function of MultiScaleDeformAttention.

#         Args:
#             query (Tensor): Query of Transformer with shape
#                 (num_query, bs, embed_dims).
#             key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
#             value (Tensor): The value tensor with shape
#                 (num_key, bs, embed_dims).
#             residual (Tensor): The tensor used for addition, with the
#                 same shape as `x`. Default None. If None, `x` will be used.
#             query_pos (Tensor): The positional encoding for `query`.
#                 Default: None.
#             reference_points (Tensor):  The normalized reference points with
#                 shape (bs, num_query, num_levels, K*2), all elements is range
#                 in [0, 1], top-left (0,0), bottom-right (1, 1), including
#                 padding area.
#             key_padding_mask (Tensor): ByteTensor for `query`, with
#                 shape [bs, num_key].
#             spatial_shapes (Tensor): Spatial shape of features in
#                 different level. With shape  (num_levels, 2),
#                 last dimension represent (h, w).
#             level_start_index (Tensor): The start index of each level.
#                 A tensor has shape (num_levels) and can be represented
#                 as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

#         Returns:
#             Tensor: forwarded results with shape [num_query, bs, embed_dims].
#         """
        
#         if key is None:
#             key = query
#         if value is None:
#             value = key

#         if residual is None:
#             inp_residual = query
#         if query_pos is not None:
#             query = query + query_pos
#         if not self.batch_first:
#             # change to (bs, num_query ,embed_dims)
#             query = query.permute(1, 0, 2)
#             value = value.permute(1, 0, 2)
            

#         bs, num_query, _ = query.shape
#         bs, num_key, _ = value.shape
        
#         assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        
#         if key_padding_mask is not None:
#             value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
#         # TODO 切分为三份
#         bs = bs // 3
#         # 提取不同帧的特征
#         pre_value = value[0::3]
#         now_value = value[1::3]
#         next_value = value[2::3]
#         pre_value = self.pre_value_proj(pre_value)
#         now_value = self.value_proj(now_value)
#         next_value = self.next_value_proj(next_value)
        
            
#         pre_value = pre_value.view(bs, num_key, self.num_heads, -1)
#         now_value = now_value.view(bs, num_key, self.num_heads, -1)
#         next_value = next_value.view(bs, num_key, self.num_heads, -1)
        
#         pre_value = pre_value.contiguous()
#         now_value = now_value.contiguous()
#         next_value = next_value.contiguous()
        
        
#         pre_sampling_offsets = self.sampling_offsets(query).view(
#             bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
#         now_sampling_offsets = self.sampling_offsets(query).view(
#             bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
#         next_sampling_offsets = self.sampling_offsets(query).view(
#             bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
#         pre_attention_weights = self.attention_weights(query).view(
#             bs, num_query, self.num_heads, self.num_levels * self.num_points)
#         now_attention_weights = self.attention_weights(query).view(
#             bs, num_query, self.num_heads, self.num_levels * self.num_points)
#         next_attention_weights = self.attention_weights(query).view(
#             bs, num_query, self.num_heads, self.num_levels * self.num_points)
        
#         # BUG ------可能会出现错误
#         pre_attention_weights_sum = torch.exp(pre_attention_weights).sum(-1, keepdim=True)
#         now_attention_weights_sum = torch.exp(now_attention_weights).sum(-1, keepdim=True)
#         next_attention_weights_sum = torch.exp(next_attention_weights).sum(-1, keepdim=True)
#         sum_all = pre_attention_weights_sum + now_attention_weights_sum + next_attention_weights_sum
#         # BUG
        
#         pre_attention_weights = pre_attention_weights.softmax(-1)
#         now_attention_weights = now_attention_weights.softmax(-1)
#         next_attention_weights = next_attention_weights.softmax(-1)
        

#         pre_attention_weights = pre_attention_weights.view(bs, num_query,
#                                                    self.num_heads,
#                                                    self.num_levels,
#                                                    self.num_points)

#         now_attention_weights = now_attention_weights.view(bs, num_query,
#                                                    self.num_heads,
#                                                    self.num_levels,
#                                                    self.num_points)

#         next_attention_weights = next_attention_weights.view(bs, num_query,
#                                                    self.num_heads,
#                                                    self.num_levels,
#                                                    self.num_points)
        
#         if reference_points.shape[-1] == self.num_points * 2:
#             reference_points_reshape = reference_points.reshape(
#                 bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
#             x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
#             y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
#             x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
#             y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
#             w = torch.clamp(x2 - x1, min=1e-4)
#             h = torch.clamp(y2 - y1, min=1e-4)
#             wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

#             pre_sampling_locations = reference_points_reshape \
#                                  + pre_sampling_offsets * wh * 0.5

#             now_sampling_locations = reference_points_reshape \
#                                  + now_sampling_offsets * wh * 0.5

#             next_sampling_locations = reference_points_reshape \
#                                  + next_sampling_offsets * wh * 0.5
#         else:
#             raise ValueError(
#                 f'Last dim of reference_points must be'
#                 f' 2K, but get {reference_points.shape[-1]} instead.')
#         if torch.cuda.is_available():
#             pre_output = MultiScaleDeformableAttnFunction.apply(
#                 pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
#                 pre_attention_weights, self.im2col_step)
#             now_output = MultiScaleDeformableAttnFunction.apply(
#                 now_value, spatial_shapes, level_start_index, now_sampling_locations,
#                 now_attention_weights, self.im2col_step)
#             next_output = MultiScaleDeformableAttnFunction.apply(
#                 next_value, spatial_shapes, level_start_index, next_sampling_locations,
#                 next_attention_weights, self.im2col_step)
            
#         else:
#             pre_output = multi_scale_deformable_attn_pytorch(
#                 pre_value, spatial_shapes, level_start_index, pre_sampling_locations,
#                 pre_attention_weights, self.im2col_step)
#             now_output = multi_scale_deformable_attn_pytorch(
#                 now_value, spatial_shapes, level_start_index, now_sampling_locations,
#                 now_attention_weights, self.im2col_step)
#             next_output = multi_scale_deformable_attn_pytorch(
#                 next_value, spatial_shapes, level_start_index, next_sampling_locations,
#                 next_attention_weights, self.im2col_step)
        
#         # TODO 融合多帧不同参考点对应的feature_token
#         pre_output = pre_output.reshape(bs, num_query, self.num_heads, -1)
#         now_output = now_output.reshape(bs, num_query, self.num_heads, -1)
#         next_output = next_output.reshape(bs, num_query, self.num_heads, -1)
#         output = pre_output * (pre_attention_weights_sum / sum_all) + \
#                  now_output * (now_attention_weights_sum / sum_all) + \
#                  next_output * (next_attention_weights_sum / sum_all)
#         output = output.flatten(-2, -1)
        
#         output = self.output_proj(output).permute(1, 0, 2)
#         # (num_query, bs ,embed_dims)
#         return self.dropout(output) + inp_residual

# 用于提取多帧关节点融合特征， 时序多尺度可变形attention v16 不同帧value也使用不同的线性层, 提取辅助帧信息使用三帧信息加权
@ATTENTION.register_module()
class MulFramesMultiScaleDeformablePoseAttentionV16(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        # 修改
        # 前一帧
        self.pre_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.pre_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 当前帧
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        # 后一帧
        self.next_sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.next_attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        
        self.pre_value_proj = nn.Linear(embed_dims, embed_dims)
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        self.next_value_proj = nn.Linear(embed_dims, embed_dims)
        
        
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.pre_sampling_offsets, 0.)
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.next_sampling_offsets, 0.)
        
        constant_init(self.pre_attention_weights, val=0., bias=0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        constant_init(self.next_attention_weights, val=0., bias=0.)
        
        xavier_init(self.pre_value_proj, distribution='uniform', bias=0.)
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        xavier_init(self.next_value_proj, distribution='uniform', bias=0.)
        
        xavier_init(self.output_proj, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)
            

        bs, num_query, _ = query.shape
        bs, num_key, _ = value.shape
        
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
            
        # TODO 切分为三份
        bs = bs // 3
        # 提取不同帧的特征
        pre_value = value[0::3]
        now_value = value[1::3]
        next_value = value[2::3]
        pre_value = self.pre_value_proj(pre_value)
        now_value = self.value_proj(now_value)
        next_value = self.next_value_proj(next_value)
        
            
        pre_value = pre_value.view(bs, num_key, self.num_heads, -1)
        now_value = now_value.view(bs, num_key, self.num_heads, -1)
        next_value = next_value.view(bs, num_key, self.num_heads, -1)
        
        pre_value = pre_value.contiguous()
        now_value = now_value.contiguous()
        next_value = next_value.contiguous()
        
        
        pre_sampling_offsets = self.pre_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        now_sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        next_sampling_offsets = self.next_sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        
        pre_attention_weights = self.pre_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        now_attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        next_attention_weights = self.next_attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        

        pre_attention_weights = pre_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        # 将不同level的相同关节点进行概率归一化
        # bs, num_query, num_heads, num_levels, num_points
        pre_attention_weights = pre_attention_weights.softmax(-2).unsqueeze(-1)

        now_attention_weights = now_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        # 将不同level的相同关节点进行概率归一化
        # bs, num_query, num_heads, num_levels, num_points
        now_attention_weights = now_attention_weights.softmax(-2).unsqueeze(-1)

        next_attention_weights = next_attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        # 将不同level的相同关节点进行概率归一化
        # bs, num_query, num_heads, num_levels, num_points
        next_attention_weights = next_attention_weights.softmax(-2).unsqueeze(-1)
        # BUG: maybe exist bug
        # 需仔细检查
        
        # # shape: bs, num_query, num_heads, num_levels, num_frames, num_points
        # all_attention_weights = torch.stack([pre_attention_weights, now_attention_weights, next_attention_weights], dim=-2)
        # #  bs, num_query, num_heads, num_levels*num_frames, num_points
        # all_attention_weights = all_attention_weights.flatten(3, 4)
        # #  bs, num_query, num_heads, num_levels*num_frames, num_points
        # all_attention_weights = all_attention_weights.softmax(dim=-2).transpose(-2, -1).unsqueeze(-1)
        
        if reference_points.shape[-1] == self.num_points * 2:
            reference_points_reshape = reference_points.reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            pre_sampling_locations = reference_points_reshape \
                                 + pre_sampling_offsets * wh * 0.5

            now_sampling_locations = reference_points_reshape \
                                 + now_sampling_offsets * wh * 0.5

            next_sampling_locations = reference_points_reshape \
                                 + next_sampling_offsets * wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
            
       # shape: bs, num_querys, num_heads, num_levels, num_points, embed_dims
        pre_samples_output = multi_scale_deformable_attn_pytorchV1(
            pre_value, spatial_shapes, pre_sampling_locations,
            pre_attention_weights)
        now_samples_output = multi_scale_deformable_attn_pytorchV1(
            now_value, spatial_shapes, now_sampling_locations,
            now_attention_weights)
        next_samples_output = multi_scale_deformable_attn_pytorchV1(
            next_value, spatial_shapes, next_sampling_locations,
            next_attention_weights)
        
        # bs, num_querys, num_heads, num_points, embed_dims -> bs, num_querys, num_points, embed_dims
        pre_output = (pre_samples_output * pre_attention_weights).sum(-3).transpose(2, 3).flatten(-2)
        now_output = (now_samples_output * now_attention_weights).sum(-3).transpose(2, 3).flatten(-2)
        next_output = (next_samples_output * next_attention_weights).sum(-3).transpose(2, 3).flatten(-2)
        
        output = torch.stack([pre_output, now_output, next_output], dim=1)

        return output


@ATTENTION.register_module()
class AuxFrameMultiScaleDeformablePoseAttention(BaseModule):
    """An attention module used in PETR. `End-to-End Multi-Person
    Pose Estimation with Transformers`.

    Args:
        embed_dims (int): The embedding dimension of Attention.
            Default: 256.
        num_heads (int): Parallel attention heads. Default: 8.
        num_levels (int): The number of feature map used in
            Attention. Default: 4.
        num_points (int): The number of sampling points for
            each query in each head. Default: 17.
        im2col_step (int): The step used in image_to_column.
            Default: 64.
        dropout (float): A Dropout layer on `inp_residual`.
            Default: 0.1.
        init_cfg (obj:`mmcv.ConfigDict`): The Config for initialization.
            Default: None.
    """

    def __init__(self,
                 embed_dims=256,
                 num_heads=8,
                 num_levels=4,
                 num_points=17,
                 im2col_step=64,
                 dropout=0.1,
                 norm_cfg=None,
                 init_cfg=None,
                 batch_first=False):
        super().__init__(init_cfg)
        if embed_dims % num_heads != 0:
            raise ValueError(f'embed_dims must be divisible by num_heads, '
                             f'but got {embed_dims} and {num_heads}')
        dim_per_head = embed_dims // num_heads
        self.norm_cfg = norm_cfg
        self.init_cfg = init_cfg
        self.dropout = nn.Dropout(dropout)
        self.batch_first = batch_first

        # you'd better set dim_per_head to a power of 2
        # which is more efficient in the CUDA implementation
        def _is_power_of_2(n):
            if (not isinstance(n, int)) or (n < 0):
                raise ValueError(
                    'invalid input for _is_power_of_2: {} (type: {})'.format(
                        n, type(n)))
            return (n & (n - 1) == 0) and n != 0

        if not _is_power_of_2(dim_per_head):
            warnings.warn(
                "You'd better set embed_dims in "
                'MultiScaleDeformAttention to make '
                'the dimension of each attention head a power of 2 '
                'which is more efficient in our CUDA implementation.')

        self.im2col_step = im2col_step
        self.embed_dims = embed_dims
        self.num_levels = num_levels
        self.num_heads = num_heads
        self.num_points = num_points
        self.sampling_offsets = nn.Linear(
            embed_dims, num_heads * num_levels * num_points * 2)
        self.attention_weights = nn.Linear(embed_dims,
                                           num_heads * num_levels * num_points)
        self.value_proj = nn.Linear(embed_dims, embed_dims)
        self.output_proj = nn.Linear(embed_dims, embed_dims)
        self.init_weights()

    def init_weights(self):
        """Default initialization for Parameters of Module."""
        constant_init(self.sampling_offsets, 0.)
        constant_init(self.attention_weights, val=0., bias=0.)
        xavier_init(self.value_proj, distribution='uniform', bias=0.)
        xavier_init(self.output_proj, distribution='uniform', bias=0.)

    def forward(self,
                query,
                key,
                value,
                residual=None,
                query_pos=None,
                key_padding_mask=None,
                reference_points=None,
                spatial_shapes=None,
                level_start_index=None,
                **kwargs):
        """Forward Function of MultiScaleDeformAttention.

        Args:
            query (Tensor): Query of Transformer with shape
                (num_query, bs, embed_dims).
            key (Tensor): The key tensor with shape (num_key, bs, embed_dims).
            value (Tensor): The value tensor with shape
                (num_key, bs, embed_dims).
            residual (Tensor): The tensor used for addition, with the
                same shape as `x`. Default None. If None, `x` will be used.
            query_pos (Tensor): The positional encoding for `query`.
                Default: None.
            reference_points (Tensor):  The normalized reference points with
                shape (bs, num_query, num_levels, K*2), all elements is range
                in [0, 1], top-left (0,0), bottom-right (1, 1), including
                padding area.
            key_padding_mask (Tensor): ByteTensor for `query`, with
                shape [bs, num_key].
            spatial_shapes (Tensor): Spatial shape of features in
                different level. With shape  (num_levels, 2),
                last dimension represent (h, w).
            level_start_index (Tensor): The start index of each level.
                A tensor has shape (num_levels) and can be represented
                as [0, h_0*w_0, h_0*w_0+h_1*w_1, ...].

        Returns:
            Tensor: forwarded results with shape [num_query, bs, embed_dims].
        """
        query_pos = torch.stack([query_pos, query_pos], dim=1).flatten(0, 1)[None]
        if key is None:
            key = query
        if value is None:
            value = key

        if residual is None:
            inp_residual = query
        if query_pos is not None:
            query = query + query_pos
        if not self.batch_first:
            # change to (bs, num_query ,embed_dims)
            query = query.permute(1, 0, 2)
            value = value.permute(1, 0, 2)

        bs, num_query, _ = query.shape
        bs, num_key, _ = value.shape
        assert (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() == num_key

        value = self.value_proj(value)
        if key_padding_mask is not None:
            value = value.masked_fill(key_padding_mask[..., None], 0.0)
        value = value.view(bs, num_key, self.num_heads, -1)
        sampling_offsets = self.sampling_offsets(query).view(
            bs, num_query, self.num_heads, self.num_levels, self.num_points, 2)
        attention_weights = self.attention_weights(query).view(
            bs, num_query, self.num_heads, self.num_levels * self.num_points)
        attention_weights = attention_weights.softmax(-1)

        attention_weights = attention_weights.view(bs, num_query,
                                                   self.num_heads,
                                                   self.num_levels,
                                                   self.num_points)
        if reference_points.shape[-1] == self.num_points * 2:
            reference_points_reshape = reference_points.reshape(
                bs, num_query, self.num_levels, -1, 2).unsqueeze(2)
            x1 = reference_points[:, :, :, 0::2].min(dim=-1, keepdim=True)[0]
            y1 = reference_points[:, :, :, 1::2].min(dim=-1, keepdim=True)[0]
            x2 = reference_points[:, :, :, 0::2].max(dim=-1, keepdim=True)[0]
            y2 = reference_points[:, :, :, 1::2].max(dim=-1, keepdim=True)[0]
            w = torch.clamp(x2 - x1, min=1e-4)
            h = torch.clamp(y2 - y1, min=1e-4)
            wh = torch.cat([w, h], dim=-1)[:, :, None, :, None, :]

            sampling_locations = reference_points_reshape \
                                 + sampling_offsets * wh * 0.5
        else:
            raise ValueError(
                f'Last dim of reference_points must be'
                f' 2K, but get {reference_points.shape[-1]} instead.')
        if torch.cuda.is_available():
            output = MultiScaleDeformableAttnFunction.apply(
                value, spatial_shapes, level_start_index, sampling_locations,
                attention_weights, self.im2col_step)
        else:
            output = multi_scale_deformable_attn_pytorch(
                value, spatial_shapes, level_start_index, sampling_locations,
                attention_weights, self.im2col_step)
        output = self.output_proj(output).permute(1, 0, 2)
        # (num_query, bs ,embed_dims)
        return self.dropout(output) + inp_residual

@TRANSFORMER_LAYER_SEQUENCE.register_module()
class PetrTransformerDecoder(TransformerLayerSequence):
    """Implements the decoder in PETR transformer.

    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        coder_norm_cfg (dict): Config of last normalization layer. Default：
            `LN`.
    """

    def __init__(self,
                 *args,
                 return_intermediate=False,
                 num_keypoints=17,
                 **kwargs):

        super(PetrTransformerDecoder, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        self.num_keypoints = num_keypoints

    def forward(self,
                query,
                *args,
                reference_points=None,
                valid_ratios=None,
                kpt_branches=None,
                **kwargs):
        """Forward function for `TransformerDecoder`.

        Args:
            query (Tensor): Input query with shape (num_query, bs, embed_dims).
            reference_points (Tensor): The reference points of offset,
                has shape (bs, num_query, K*2).
            valid_ratios (Tensor): The radios of valid points on the feature
                map, has shape (bs, num_levels, 2).
            kpt_branches: (obj:`nn.ModuleList`): Used for refining the
                regression results. Only would be passed when `with_box_refine`
                is True, otherwise would be passed a `None`.

        Returns:
            tuple (Tensor): Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims] and
                [num_layers, bs, num_query, K*2].
        """
        output = query
        intermediate = []
        intermediate_reference_points = []
        for lid, layer in enumerate(self.layers):
            if reference_points.shape[-1] == self.num_keypoints * 2:
                reference_points_input = \
                    reference_points[:, :, None] * \
                    valid_ratios.repeat(1, 1, self.num_keypoints)[:, None]
            else:
                assert reference_points.shape[-1] == 2
                reference_points_input = reference_points[:, :, None] * \
                                         valid_ratios[:, None]
            output = layer(
                output,
                *args,
                reference_points=reference_points_input,
                **kwargs)
            output = output.permute(1, 0, 2)

            if kpt_branches is not None:
                tmp = kpt_branches[lid](output)
                if reference_points.shape[-1] == self.num_keypoints * 2:
                    new_reference_points = tmp + inverse_sigmoid(
                        reference_points)
                    new_reference_points = new_reference_points.sigmoid()
                else:
                    raise NotImplementedError
                reference_points = new_reference_points.detach()

            output = output.permute(1, 0, 2)
            if self.return_intermediate:
                intermediate.append(output)
                intermediate_reference_points.append(reference_points)

        if self.return_intermediate:
            return torch.stack(intermediate), torch.stack(
                intermediate_reference_points)

        return output, reference_points


@TRANSFORMER.register_module()
class PETRTransformer(Transformer):
    """Implements the PETR transformer.

    Args:
        as_two_stage (bool): Generate query from encoder features.
            Default: False.
        num_feature_levels (int): Number of feature maps from FPN:
            Default: 4.
        two_stage_num_proposals (int): Number of proposals when set
            `as_two_stage` as True. Default: 300.
    """

    def __init__(self,
                 hm_encoder=dict(
                     type='DetrTransformerEncoder',
                     num_layers=1,
                     transformerlayers=dict(
                         type='BaseTransformerLayer',
                         attn_cfgs=dict(
                             type='MultiScaleDeformableAttention',
                             embed_dims=256,
                             num_levels=1),
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'ffn',
                                          'norm'))),
                 refine_decoder=dict(
                     type='DeformableDetrTransformerDecoder',
                     num_layers=1,
                     return_intermediate=True,
                     transformerlayers=dict(
                         type='DetrTransformerDecoderLayer',
                         attn_cfgs=[
                             dict(
                                 type='MultiheadAttention',
                                 embed_dims=256,
                                 num_heads=8,
                                 dropout=0.1),
                             dict(
                                 type='MultiScaleDeformableAttention',
                                 embed_dims=256)
                         ],
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'cross_attn',
                                          'norm', 'ffn', 'norm'))),
                 as_two_stage=True,
                 num_feature_levels=4,
                 two_stage_num_proposals=300,
                 num_keypoints=17,
                 **kwargs):
        super(PETRTransformer, self).__init__(**kwargs)
        self.as_two_stage = as_two_stage
        self.num_feature_levels = num_feature_levels
        self.two_stage_num_proposals = two_stage_num_proposals
        self.embed_dims = self.encoder.embed_dims
        self.num_keypoints = num_keypoints
        self.init_layers()
        self.hm_encoder = build_transformer_layer_sequence(hm_encoder)
        self.refine_decoder = build_transformer_layer_sequence(refine_decoder)

    def init_layers(self):
        """Initialize layers of the DeformableDetrTransformer."""
        self.level_embeds = nn.Parameter(
            torch.Tensor(self.num_feature_levels, self.embed_dims))

        if self.as_two_stage:
            self.enc_output = nn.Linear(self.embed_dims, self.embed_dims)
            self.enc_output_norm = nn.LayerNorm(self.embed_dims)
            self.refine_query_embedding = nn.Embedding(self.num_keypoints,
                                                       self.embed_dims * 2)
        else:
            self.reference_points = nn.Linear(self.embed_dims,
                                              2 * self.num_keypoints)

    def init_weights(self):
        """Initialize the transformer weights."""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        for m in self.modules():
            if isinstance(m, MultiScaleDeformableAttention):
                m.init_weights()
        for m in self.modules():
            if isinstance(m, MultiScaleDeformablePoseAttention):
                m.init_weights()
        if not self.as_two_stage:
            xavier_init(self.reference_points, distribution='uniform', bias=0.)
        normal_(self.level_embeds)
        normal_(self.refine_query_embedding.weight)

    def gen_encoder_output_proposals(self, memory, memory_padding_mask,
                                     spatial_shapes):
        """Generate proposals from encoded memory.

        Args:
            memory (Tensor): The output of encoder, has shape
                (bs, num_key, embed_dim). num_key is equal the number of points
                on feature map from all level.
            memory_padding_mask (Tensor): Padding mask for memory.
                has shape (bs, num_key).
            spatial_shapes (Tensor): The shape of all feature maps.
                has shape (num_level, 2).

        Returns:
            tuple: A tuple of feature map and bbox prediction.

                - output_memory (Tensor): The input of decoder, has shape
                    (bs, num_key, embed_dim). num_key is equal the number of
                    points on feature map from all levels.
                - output_proposals (Tensor): The normalized proposal
                    after a inverse sigmoid, has shape (bs, num_keys, 4).
        """

        N, S, C = memory.shape
        proposals = []
        _cur = 0
        for lvl, (H, W) in enumerate(spatial_shapes):
            mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H * W)].view(
                N, H, W, 1)
            valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)
            valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)

            grid_y, grid_x = torch.meshgrid(
                torch.linspace(
                    0, H - 1, H, dtype=torch.float32, device=memory.device),
                torch.linspace(
                    0, W - 1, W, dtype=torch.float32, device=memory.device))
            grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)

            scale = torch.cat([valid_W.unsqueeze(-1),
                               valid_H.unsqueeze(-1)], 1).view(N, 1, 1, 2)
            grid = (grid.unsqueeze(0).expand(N, -1, -1, -1) + 0.5) / scale
            proposal = grid.view(N, -1, 2)
            proposals.append(proposal)
            _cur += (H * W)
        output_proposals = torch.cat(proposals, 1)
        output_proposals_valid = ((output_proposals > 0.01) &
                                  (output_proposals < 0.99)).all(
                                      -1, keepdim=True)
        output_proposals = torch.log(output_proposals / (1 - output_proposals))
        output_proposals = output_proposals.masked_fill(
            memory_padding_mask.unsqueeze(-1), float('inf'))
        output_proposals = output_proposals.masked_fill(
            ~output_proposals_valid, float('inf'))

        output_memory = memory
        output_memory = output_memory.masked_fill(
            memory_padding_mask.unsqueeze(-1), float(0))
        output_memory = output_memory.masked_fill(~output_proposals_valid,
                                                  float(0))
        output_memory = self.enc_output_norm(self.enc_output(output_memory))
        return output_memory, output_proposals

    @staticmethod
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """Get the reference points used in decoder.

        Args:
            spatial_shapes (Tensor): The shape of all feature maps,
                has shape (num_level, 2).
            valid_ratios (Tensor): The radios of valid points on the
                feature map, has shape (bs, num_levels, 2).
            device (obj:`device`): The device where reference_points should be.

        Returns:
            Tensor: reference points used in decoder, has \
                shape (bs, num_keys, num_levels, 2).
        """
        reference_points_list = []
        for lvl, (H, W) in enumerate(spatial_shapes):
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(
                    0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(
                    0.5, W - 0.5, W, dtype=torch.float32, device=device))
            ref_y = ref_y.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 1] * H)
            ref_x = ref_x.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 0] * W)
            ref = torch.stack((ref_x, ref_y), -1)
            reference_points_list.append(ref)
        reference_points = torch.cat(reference_points_list, 1)
        reference_points = reference_points[:, :, None] * valid_ratios[:, None]
        return reference_points

    def get_valid_ratio(self, mask):
        """Get the valid radios of feature maps of all level."""
        _, H, W = mask.shape
        valid_H = torch.sum(~mask[:, :, 0], 1)
        valid_W = torch.sum(~mask[:, 0, :], 1)
        valid_ratio_h = valid_H.float() / H
        valid_ratio_w = valid_W.float() / W
        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)
        return valid_ratio

    def get_proposal_pos_embed(self,
                               proposals,
                               num_pos_feats=128,
                               temperature=10000):
        """Get the position embedding of proposal."""
        scale = 2 * math.pi
        dim_t = torch.arange(
            num_pos_feats, dtype=torch.float32, device=proposals.device)
        dim_t = temperature**(2 * (dim_t // 2) / num_pos_feats)
        # N, L, 4
        proposals = proposals.sigmoid() * scale
        # N, L, 4, 128
        pos = proposals[:, :, :, None] / dim_t
        # N, L, 4, 64, 2
        pos = torch.stack((pos[:, :, :, 0::2].sin(), pos[:, :, :, 1::2].cos()),
                          dim=4).flatten(2)
        return pos

    def forward(self,
                mlvl_feats,
                mlvl_masks,
                query_embed,
                mlvl_pos_embeds,
                kpt_branches=None,
                cls_branches=None,
                **kwargs):
        """Forward function for `Transformer`.

        Args:
            mlvl_feats (list(Tensor)): Input queries from different level.
                Each element has shape [bs, embed_dims, h, w].
            mlvl_masks (list(Tensor)): The key_padding_mask from different
                level used for encoder and decoder, each element has shape
                [bs, h, w].
            query_embed (Tensor): The query embedding for decoder,
                with shape [num_query, c].
            mlvl_pos_embeds (list(Tensor)): The positional encoding
                of feats from different level, has the shape
                 [bs, embed_dims, h, w].
            kpt_branches (obj:`nn.ModuleList`): Keypoint Regression heads for
                feature maps from each decoder layer. Only would be passed when
                `with_box_refine` is Ture. Default to None.
            cls_branches (obj:`nn.ModuleList`): Classification heads for
                feature maps from each decoder layer. Only would be passed when
                `as_two_stage` is Ture. Default to None.

        Returns:
            tuple[Tensor]: results of decoder containing the following tensor.

                - inter_states: Outputs from decoder. If
                    `return_intermediate_dec` is True output has shape \
                    (num_dec_layers, bs, num_query, embed_dims), else has \
                    shape (1, bs, num_query, embed_dims).
                - init_reference_out: The initial value of reference \
                    points, has shape (bs, num_queries, 4).
                - inter_references_out: The internal value of reference \
                    points in decoder, has shape \
                    (num_dec_layers, bs,num_query, embed_dims)
                - enc_outputs_class: The classification score of proposals \
                    generated from encoder's feature maps, has shape \
                    (batch, h*w, num_classes). \
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
                - enc_outputs_kpt_unact: The regression results generated from \
                    encoder's feature maps., has shape (batch, h*w, K*2).
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
        """
        assert self.as_two_stage or query_embed is not None

        feat_flatten = []
        mask_flatten = []
        lvl_pos_embed_flatten = []
        spatial_shapes = []
        for lvl, (feat, mask, pos_embed) in enumerate(
                zip(mlvl_feats, mlvl_masks, mlvl_pos_embeds)):
            bs, c, h, w = feat.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            feat = feat.flatten(2).transpose(1, 2)
            mask = mask.flatten(1)
            pos_embed = pos_embed.flatten(2).transpose(1, 2)
            lvl_pos_embed = pos_embed + self.level_embeds[lvl].view(1, 1, -1)
            lvl_pos_embed_flatten.append(lvl_pos_embed)
            feat_flatten.append(feat)
            mask_flatten.append(mask)
        feat_flatten = torch.cat(feat_flatten, 1)
        mask_flatten = torch.cat(mask_flatten, 1)
        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=feat_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        reference_points = \
            self.get_reference_points(spatial_shapes,
                                      valid_ratios,
                                      device=feat.device)

        feat_flatten = feat_flatten.permute(1, 0, 2)  # (H*W, bs, embed_dims)
        lvl_pos_embed_flatten = lvl_pos_embed_flatten.permute(
            1, 0, 2)  # (H*W, bs, embed_dims)
        memory = self.encoder(
            query=feat_flatten,
            key=None,
            value=None,
            query_pos=lvl_pos_embed_flatten,
            query_key_padding_mask=mask_flatten,
            spatial_shapes=spatial_shapes,
            reference_points=reference_points,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            **kwargs)

        memory = memory.permute(1, 0, 2)
        bs, _, c = memory.shape

        hm_proto = None
        if self.training:
            hm_memory = memory[
                :, level_start_index[0]:level_start_index[1], :]
            hm_pos_embed = lvl_pos_embed_flatten[
                level_start_index[0]:level_start_index[1], :, :]
            hm_mask = mask_flatten[
                :, level_start_index[0]:level_start_index[1]]
            hm_reference_points = reference_points[
                :, level_start_index[0]:level_start_index[1], [0], :]
            hm_memory = hm_memory.permute(1, 0, 2)
            hm_memory = self.hm_encoder(
                query=hm_memory,
                key=None,
                value=None,
                query_pose=hm_pos_embed,
                query_key_padding_mask=hm_mask,
                spatial_shapes=spatial_shapes[[0]],
                reference_points=hm_reference_points,
                level_start_index=level_start_index[0],
                valid_ratios=valid_ratios[:, [0], :],
                **kwargs)
            hm_memory = hm_memory.permute(1, 0, 2).reshape(bs,
                spatial_shapes[0, 0], spatial_shapes[0, 1], -1)
            hm_proto = (hm_memory, mlvl_masks[0])

        if self.as_two_stage:
            output_memory, output_proposals = \
                self.gen_encoder_output_proposals(
                    memory, mask_flatten, spatial_shapes)
            enc_outputs_class = cls_branches[self.decoder.num_layers](
                output_memory)
            enc_outputs_kpt_unact = \
                kpt_branches[self.decoder.num_layers](output_memory)
            enc_outputs_kpt_unact[..., 0::2] += output_proposals[..., 0:1]
            enc_outputs_kpt_unact[..., 1::2] += output_proposals[..., 1:2]

            topk = self.two_stage_num_proposals
            topk_proposals = torch.topk(
                enc_outputs_class[..., 0], topk, dim=1)[1]
            # topk_coords_unact = torch.gather(
            #     enc_outputs_coord_unact, 1,
            #     topk_proposals.unsqueeze(-1).repeat(1, 1, 4))
            # topk_coords_unact = topk_coords_unact.detach()
            topk_kpts_unact = torch.gather(
                enc_outputs_kpt_unact, 1,
                topk_proposals.unsqueeze(-1).repeat(
                    1, 1, enc_outputs_kpt_unact.size(-1)))
            topk_kpts_unact = topk_kpts_unact.detach()

            reference_points = topk_kpts_unact.sigmoid()
            init_reference_out = reference_points
            # learnable query and query_pos
            query_pos, query = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query = query.unsqueeze(0).expand(bs, -1, -1)
        else:
            query_pos, query = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query = query.unsqueeze(0).expand(bs, -1, -1)
            reference_points = self.reference_points(query_pos).sigmoid()
            init_reference_out = reference_points

        # decoder
        query = query.permute(1, 0, 2)
        memory = memory.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        inter_states, inter_references = self.decoder(
            query=query,
            key=None,
            value=memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            kpt_branches=kpt_branches,
            **kwargs)

        inter_references_out = inter_references
        if self.as_two_stage:
            return inter_states, init_reference_out, \
                   inter_references_out, enc_outputs_class, \
                   enc_outputs_kpt_unact, hm_proto, memory
        return inter_states, init_reference_out, \
               inter_references_out, None, None, None, None, None, hm_proto

    def forward_refine(self,
                       mlvl_masks,
                       memory,
                       reference_points_pose,
                       img_inds,
                       kpt_branches=None,
                       **kwargs):
        mask_flatten = []
        spatial_shapes = []
        for lvl, mask in enumerate(mlvl_masks):
            bs, h, w = mask.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            mask = mask.flatten(1)
            mask_flatten.append(mask)
        mask_flatten = torch.cat(mask_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=mask_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        # pose refinement (17 queries corresponding to 17 keypoints)
        # learnable query and query_pos
        refine_query_embedding = self.refine_query_embedding.weight
        query_pos, query = torch.split(
            refine_query_embedding, refine_query_embedding.size(1) // 2, dim=1)
        pos_num = reference_points_pose.size(0)
        query_pos = query_pos.unsqueeze(0).expand(pos_num, -1, -1)
        query = query.unsqueeze(0).expand(pos_num, -1, -1)
        reference_points = reference_points_pose.reshape(
            pos_num,
            reference_points_pose.size(1) // 2, 2)
        query = query.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        pos_memory = memory[:, img_inds, :]
        mask_flatten = mask_flatten[img_inds, :]
        valid_ratios = valid_ratios[img_inds, ...]
        inter_states, inter_references = self.refine_decoder(
            query=query,
            key=None,
            value=pos_memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            reg_branches=kpt_branches,
            **kwargs)
        # [num_decoder, num_query, bs, embed_dim]

        init_reference_out = reference_points
        return inter_states, init_reference_out, inter_references

# encoder 添加时序 添加时间：2024-11-10
@TRANSFORMER.register_module()
class PETRTransformerV11_10_1(Transformer):
    """Implements the PETR transformer.

    Args:
        as_two_stage (bool): Generate query from encoder features.
            Default: False.
        num_feature_levels (int): Number of feature maps from FPN:
            Default: 4.
        two_stage_num_proposals (int): Number of proposals when set
            `as_two_stage` as True. Default: 300.
    """

    def __init__(self,
                 hm_encoder=dict(
                     type='DetrTransformerEncoder',
                     num_layers=1,
                     transformerlayers=dict(
                         type='BaseTransformerLayer',
                         attn_cfgs=dict(
                             type='MultiScaleDeformableAttention',
                             embed_dims=256,
                             num_levels=1),
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'ffn',
                                          'norm'))),
                 refine_decoder=dict(
                     type='DeformableDetrTransformerDecoder',
                     num_layers=1,
                     return_intermediate=True,
                     transformerlayers=dict(
                         type='DetrTransformerDecoderLayer',
                         attn_cfgs=[
                             dict(
                                 type='MultiheadAttention',
                                 embed_dims=256,
                                 num_heads=8,
                                 dropout=0.1),
                             dict(
                                 type='MultiScaleDeformableAttention',
                                 embed_dims=256)
                         ],
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'cross_attn',
                                          'norm', 'ffn', 'norm'))),
                 as_two_stage=True,
                 num_feature_levels=4,
                 two_stage_num_proposals=300,
                 num_keypoints=17,
                 num_frames=3,
                 **kwargs):
        super(PETRTransformerV11_10_1, self).__init__(**kwargs)
        self.as_two_stage = as_two_stage
        self.num_feature_levels = num_feature_levels
        self.two_stage_num_proposals = two_stage_num_proposals
        self.embed_dims = self.encoder.embed_dims
        self.num_keypoints = num_keypoints
        self.num_frames = num_frames
        self.init_layers()
        self.hm_encoder = build_transformer_layer_sequence(hm_encoder)
        self.refine_decoder = build_transformer_layer_sequence(refine_decoder)

    def init_layers(self):
        """Initialize layers of the DeformableDetrTransformer."""
        self.level_embeds = nn.Parameter(
            torch.Tensor(self.num_feature_levels, self.embed_dims))
        
        # 时间位置编码 num_frames, embedding_dims
        self.time_pos = nn.Embedding(self.num_frames, self.embed_dims)

        if self.as_two_stage:
            self.enc_output = nn.Linear(self.embed_dims, self.embed_dims)
            self.enc_output_norm = nn.LayerNorm(self.embed_dims)
            self.refine_query_embedding = nn.Embedding(self.num_keypoints,
                                                       self.embed_dims * 2)
        else:
            self.reference_points = nn.Linear(self.embed_dims,
                                              2 * self.num_keypoints)

    def init_weights(self):
        """Initialize the transformer weights."""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        for m in self.modules():
            if isinstance(m, MultiScaleDeformableAttention):
                m.init_weights()
        for m in self.modules():
            if isinstance(m, MultiScaleDeformablePoseAttention):
                m.init_weights()
        if not self.as_two_stage:
            xavier_init(self.reference_points, distribution='uniform', bias=0.)
        normal_(self.level_embeds)
        normal_(self.refine_query_embedding.weight)

    def gen_encoder_output_proposals(self, memory, memory_padding_mask,
                                     spatial_shapes):
        """Generate proposals from encoded memory.

        Args:
            memory (Tensor): The output of encoder, has shape
                (bs, num_key, embed_dim). num_key is equal the number of points
                on feature map from all level.
            memory_padding_mask (Tensor): Padding mask for memory.
                has shape (bs, num_key).
            spatial_shapes (Tensor): The shape of all feature maps.
                has shape (num_level, 2).

        Returns:
            tuple: A tuple of feature map and bbox prediction.

                - output_memory (Tensor): The input of decoder, has shape
                    (bs, num_key, embed_dim). num_key is equal the number of
                    points on feature map from all levels.
                - output_proposals (Tensor): The normalized proposal
                    after a inverse sigmoid, has shape (bs, num_keys, 4).
        """

        N, S, C = memory.shape
        proposals = []
        _cur = 0
        for lvl, (H, W) in enumerate(spatial_shapes):
            mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H * W)].view(
                N, H, W, 1)
            valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)
            valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)

            grid_y, grid_x = torch.meshgrid(
                torch.linspace(
                    0, H - 1, H, dtype=torch.float32, device=memory.device),
                torch.linspace(
                    0, W - 1, W, dtype=torch.float32, device=memory.device))
            grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)

            scale = torch.cat([valid_W.unsqueeze(-1),
                               valid_H.unsqueeze(-1)], 1).view(N, 1, 1, 2)
            grid = (grid.unsqueeze(0).expand(N, -1, -1, -1) + 0.5) / scale
            proposal = grid.view(N, -1, 2)
            proposals.append(proposal)
            _cur += (H * W)
        output_proposals = torch.cat(proposals, 1)
        output_proposals_valid = ((output_proposals > 0.01) &
                                  (output_proposals < 0.99)).all(
                                      -1, keepdim=True)
        output_proposals = torch.log(output_proposals / (1 - output_proposals))
        output_proposals = output_proposals.masked_fill(
            memory_padding_mask.unsqueeze(-1), float('inf'))
        output_proposals = output_proposals.masked_fill(
            ~output_proposals_valid, float('inf'))

        output_memory = memory
        output_memory = output_memory.masked_fill(
            memory_padding_mask.unsqueeze(-1), float(0))
        output_memory = output_memory.masked_fill(~output_proposals_valid,
                                                  float(0))
        output_memory = self.enc_output_norm(self.enc_output(output_memory))
        return output_memory, output_proposals

    @staticmethod
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """Get the reference points used in decoder.

        Args:
            spatial_shapes (Tensor): The shape of all feature maps,
                has shape (num_level, 2).
            valid_ratios (Tensor): The radios of valid points on the
                feature map, has shape (bs, num_levels, 2).
            device (obj:`device`): The device where reference_points should be.

        Returns:
            Tensor: reference points used in decoder, has \
                shape (bs, num_keys, num_levels, 2).
        """
        reference_points_list = []
        for lvl, (H, W) in enumerate(spatial_shapes):
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(
                    0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(
                    0.5, W - 0.5, W, dtype=torch.float32, device=device))
            ref_y = ref_y.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 1] * H)
            ref_x = ref_x.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 0] * W)
            ref = torch.stack((ref_x, ref_y), -1)
            reference_points_list.append(ref)
        reference_points = torch.cat(reference_points_list, 1)
        reference_points = reference_points[:, :, None] * valid_ratios[:, None]
        return reference_points

    def get_valid_ratio(self, mask):
        """Get the valid radios of feature maps of all level."""
        _, H, W = mask.shape
        valid_H = torch.sum(~mask[:, :, 0], 1)
        valid_W = torch.sum(~mask[:, 0, :], 1)
        valid_ratio_h = valid_H.float() / H
        valid_ratio_w = valid_W.float() / W
        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)
        return valid_ratio

    def get_proposal_pos_embed(self,
                               proposals,
                               num_pos_feats=128,
                               temperature=10000):
        """Get the position embedding of proposal."""
        scale = 2 * math.pi
        dim_t = torch.arange(
            num_pos_feats, dtype=torch.float32, device=proposals.device)
        dim_t = temperature**(2 * (dim_t // 2) / num_pos_feats)
        # N, L, 4
        proposals = proposals.sigmoid() * scale
        # N, L, 4, 128
        pos = proposals[:, :, :, None] / dim_t
        # N, L, 4, 64, 2
        pos = torch.stack((pos[:, :, :, 0::2].sin(), pos[:, :, :, 1::2].cos()),
                          dim=4).flatten(2)
        return pos

    def forward(self,
                mlvl_feats,
                mlvl_masks,
                query_embed,
                mlvl_pos_embeds,
                kpt_branches=None,
                cls_branches=None,
                **kwargs):
        """Forward function for `Transformer`.

        Args:
            mlvl_feats (list(Tensor)): Input queries from different level.
                Each element has shape [bs, embed_dims, h, w].
            mlvl_masks (list(Tensor)): The key_padding_mask from different
                level used for encoder and decoder, each element has shape
                [bs, h, w].
            query_embed (Tensor): The query embedding for decoder,
                with shape [num_query, c].
            mlvl_pos_embeds (list(Tensor)): The positional encoding
                of feats from different level, has the shape
                 [bs, embed_dims, h, w].
            kpt_branches (obj:`nn.ModuleList`): Keypoint Regression heads for
                feature maps from each decoder layer. Only would be passed when
                `with_box_refine` is Ture. Default to None.
            cls_branches (obj:`nn.ModuleList`): Classification heads for
                feature maps from each decoder layer. Only would be passed when
                `as_two_stage` is Ture. Default to None.

        Returns:
            tuple[Tensor]: results of decoder containing the following tensor.

                - inter_states: Outputs from decoder. If
                    `return_intermediate_dec` is True output has shape \
                    (num_dec_layers, bs, num_query, embed_dims), else has \
                    shape (1, bs, num_query, embed_dims).
                - init_reference_out: The initial value of reference \
                    points, has shape (bs, num_queries, 4).
                - inter_references_out: The internal value of reference \
                    points in decoder, has shape \
                    (num_dec_layers, bs,num_query, embed_dims)
                - enc_outputs_class: The classification score of proposals \
                    generated from encoder's feature maps, has shape \
                    (batch, h*w, num_classes). \
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
                - enc_outputs_kpt_unact: The regression results generated from \
                    encoder's feature maps., has shape (batch, h*w, K*2).
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
        """
        assert self.as_two_stage or query_embed is not None

        feat_flatten = []
        mask_flatten = []
        lvl_pos_embed_flatten = []
        spatial_shapes = []
        for lvl, (feat, mask, pos_embed) in enumerate(
                zip(mlvl_feats, mlvl_masks, mlvl_pos_embeds)):
            bs, c, h, w = feat.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            feat = feat.flatten(2).transpose(1, 2)
            mask = mask.flatten(1)
            pos_embed = pos_embed.flatten(2).transpose(1, 2)
            lvl_pos_embed = pos_embed + self.level_embeds[lvl].view(1, 1, -1)
            lvl_pos_embed_flatten.append(lvl_pos_embed)
            feat_flatten.append(feat)
            mask_flatten.append(mask)
        feat_flatten = torch.cat(feat_flatten, 1)
        mask_flatten = torch.cat(mask_flatten, 1)
        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=feat_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        reference_points = \
            self.get_reference_points(spatial_shapes,
                                      valid_ratios,
                                      device=feat.device)

        feat_flatten = feat_flatten.permute(1, 0, 2)  # (H*W, bs, embed_dims)
        lvl_pos_embed_flatten = lvl_pos_embed_flatten.permute(
            1, 0, 2)  # (H*W, bs, embed_dims)
        memory = self.encoder(
            query=feat_flatten,
            key=None,
            value=None,
            query_pos=lvl_pos_embed_flatten,
            query_time_pos=self.time_pos.weight,
            query_key_padding_mask=mask_flatten,
            spatial_shapes=spatial_shapes,
            reference_points=reference_points,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            **kwargs)

        # 只使用当前帧的feature_tokens
        memory = memory.permute(1, 0, 2)
        bs, _, c = memory.shape
        bs = bs // self.num_frames
        memory = memory[self.num_frames//2::self.num_frames]
        lvl_pos_embed_flatten = lvl_pos_embed_flatten[:, self.num_frames//2::self.num_frames] # token_num, bs//3, dim
        mask_flatten = mask_flatten[self.num_frames//2::self.num_frames] # shape: bs//3, token_num, dim
        reference_points = reference_points[self.num_frames//2::self.num_frames] # shape: bs//3, token_num, dim
        valid_ratios = valid_ratios[self.num_frames//2::self.num_frames] # shape: bs//3, token_num, dim
        mlvl_masks =[mlvl_mask[self.num_frames//2::self.num_frames] for mlvl_mask in mlvl_masks]

        hm_proto = None
        if self.training:
            hm_memory = memory[
                :, level_start_index[0]:level_start_index[1], :]
            hm_pos_embed = lvl_pos_embed_flatten[
                level_start_index[0]:level_start_index[1], :, :]
            hm_mask = mask_flatten[
                :, level_start_index[0]:level_start_index[1]]
            hm_reference_points = reference_points[
                :, level_start_index[0]:level_start_index[1], [0], :]
            hm_memory = hm_memory.permute(1, 0, 2)
            hm_memory = self.hm_encoder(
                query=hm_memory,
                key=None,
                value=None,
                query_pose=hm_pos_embed,
                query_key_padding_mask=hm_mask,
                spatial_shapes=spatial_shapes[[0]],
                reference_points=hm_reference_points,
                level_start_index=level_start_index[0],
                valid_ratios=valid_ratios[:, [0], :],
                **kwargs)
            hm_memory = hm_memory.permute(1, 0, 2).reshape(bs,
                spatial_shapes[0, 0], spatial_shapes[0, 1], -1)
            hm_proto = (hm_memory, mlvl_masks[0])

        if self.as_two_stage:
            output_memory, output_proposals = \
                self.gen_encoder_output_proposals(
                    memory, mask_flatten, spatial_shapes)
            enc_outputs_class = cls_branches[self.decoder.num_layers](
                output_memory)
            enc_outputs_kpt_unact = \
                kpt_branches[self.decoder.num_layers](output_memory)
            enc_outputs_kpt_unact[..., 0::2] += output_proposals[..., 0:1]
            enc_outputs_kpt_unact[..., 1::2] += output_proposals[..., 1:2]

            topk = self.two_stage_num_proposals
            topk_proposals = torch.topk(
                enc_outputs_class[..., 0], topk, dim=1)[1]
            # topk_coords_unact = torch.gather(
            #     enc_outputs_coord_unact, 1,
            #     topk_proposals.unsqueeze(-1).repeat(1, 1, 4))
            # topk_coords_unact = topk_coords_unact.detach()
            topk_kpts_unact = torch.gather(
                enc_outputs_kpt_unact, 1,
                topk_proposals.unsqueeze(-1).repeat(
                    1, 1, enc_outputs_kpt_unact.size(-1)))
            topk_kpts_unact = topk_kpts_unact.detach()

            reference_points = topk_kpts_unact.sigmoid()
            init_reference_out = reference_points
            # learnable query and query_pos
            query_pos, query = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query = query.unsqueeze(0).expand(bs, -1, -1)
        else:
            query_pos, query = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query = query.unsqueeze(0).expand(bs, -1, -1)
            reference_points = self.reference_points(query_pos).sigmoid()
            init_reference_out = reference_points

        # decoder
        query = query.permute(1, 0, 2)
        memory = memory.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        inter_states, inter_references = self.decoder(
            query=query,
            key=None,
            value=memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            kpt_branches=kpt_branches,
            **kwargs)

        inter_references_out = inter_references
        if self.as_two_stage:
            return inter_states, init_reference_out, \
                   inter_references_out, enc_outputs_class, \
                   enc_outputs_kpt_unact, hm_proto, memory
        return inter_states, init_reference_out, \
               inter_references_out, None, None, None, None, None, hm_proto

    def forward_refine(self,
                       mlvl_masks,
                       memory,
                       reference_points_pose,
                       img_inds,
                       kpt_branches=None,
                       **kwargs):
        mask_flatten = []
        spatial_shapes = []
        for lvl, mask in enumerate(mlvl_masks):
            bs, h, w = mask.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            mask = mask.flatten(1)
            mask_flatten.append(mask)
        mask_flatten = torch.cat(mask_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=mask_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        # pose refinement (17 queries corresponding to 17 keypoints)
        # learnable query and query_pos
        refine_query_embedding = self.refine_query_embedding.weight
        query_pos, query = torch.split(
            refine_query_embedding, refine_query_embedding.size(1) // 2, dim=1)
        pos_num = reference_points_pose.size(0)
        query_pos = query_pos.unsqueeze(0).expand(pos_num, -1, -1)
        query = query.unsqueeze(0).expand(pos_num, -1, -1)
        reference_points = reference_points_pose.reshape(
            pos_num,
            reference_points_pose.size(1) // 2, 2)
        query = query.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        pos_memory = memory[:, img_inds, :]
        mask_flatten = mask_flatten[img_inds, :]
        valid_ratios = valid_ratios[img_inds, ...]
        inter_states, inter_references = self.refine_decoder(
            query=query,
            key=None,
            value=pos_memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            reg_branches=kpt_branches,
            **kwargs)
        # [num_decoder, num_query, bs, embed_dim]

        init_reference_out = reference_points
        return inter_states, init_reference_out, inter_references

# 修改Pose-query的初始化 --- 修改时间： 2024-8-14
@TRANSFORMER.register_module()
class PETRTransformerV1(Transformer):
    """Implements the PETR transformer.

    Args:
        as_two_stage (bool): Generate query from encoder features.
            Default: False.
        num_feature_levels (int): Number of feature maps from FPN:
            Default: 4.
        two_stage_num_proposals (int): Number of proposals when set
            `as_two_stage` as True. Default: 300.
    """

    def __init__(self,
                 hm_encoder=dict(
                     type='DetrTransformerEncoder',
                     num_layers=1,
                     transformerlayers=dict(
                         type='BaseTransformerLayer',
                         attn_cfgs=dict(
                             type='MultiScaleDeformableAttention',
                             embed_dims=256,
                             num_levels=1),
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'ffn',
                                          'norm'))),
                 refine_decoder=dict(
                     type='DeformableDetrTransformerDecoder',
                     num_layers=1,
                     return_intermediate=True,
                     transformerlayers=dict(
                         type='DetrTransformerDecoderLayer',
                         attn_cfgs=[
                             dict(
                                 type='MultiheadAttention',
                                 embed_dims=256,
                                 num_heads=8,
                                 dropout=0.1),
                             dict(
                                 type='MultiScaleDeformableAttention',
                                 embed_dims=256)
                         ],
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'cross_attn',
                                          'norm', 'ffn', 'norm'))),
                 as_two_stage=True,
                 num_feature_levels=4,
                 two_stage_num_proposals=300,
                 num_keypoints=17,
                 **kwargs):
        super(PETRTransformerV1, self).__init__(**kwargs)
        self.as_two_stage = as_two_stage
        self.num_feature_levels = num_feature_levels
        self.two_stage_num_proposals = two_stage_num_proposals
        self.embed_dims = self.encoder.embed_dims
        self.num_keypoints = num_keypoints
        self.init_layers()
        self.hm_encoder = build_transformer_layer_sequence(hm_encoder)
        self.refine_decoder = build_transformer_layer_sequence(refine_decoder)
        

    def init_layers(self):
        """Initialize layers of the DeformableDetrTransformer."""
        self.level_embeds = nn.Parameter(
            torch.Tensor(self.num_feature_levels, self.embed_dims))

        if self.as_two_stage:
            self.enc_output = nn.Linear(self.embed_dims, self.embed_dims)
            self.enc_output_norm = nn.LayerNorm(self.embed_dims)
            self.refine_query_embedding = nn.Embedding(self.num_keypoints,
                                                       self.embed_dims * 2)
        else:
            self.reference_points = nn.Linear(self.embed_dims,
                                              2 * self.num_keypoints)

    def init_weights(self):
        """Initialize the transformer weights."""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        for m in self.modules():
            if isinstance(m, MultiScaleDeformableAttention):
                m.init_weights()
        for m in self.modules():
            if isinstance(m, MultiScaleDeformablePoseAttention):
                m.init_weights()
        if not self.as_two_stage:
            xavier_init(self.reference_points, distribution='uniform', bias=0.)
        normal_(self.level_embeds)
        normal_(self.refine_query_embedding.weight)

    def gen_encoder_output_proposals(self, memory, memory_padding_mask,
                                     spatial_shapes):
        """Generate proposals from encoded memory.

        Args:
            memory (Tensor): The output of encoder, has shape
                (bs, num_key, embed_dim). num_key is equal the number of points
                on feature map from all level.
            memory_padding_mask (Tensor): Padding mask for memory.
                has shape (bs, num_key).
            spatial_shapes (Tensor): The shape of all feature maps.
                has shape (num_level, 2).

        Returns:
            tuple: A tuple of feature map and bbox prediction.

                - output_memory (Tensor): The input of decoder, has shape
                    (bs, num_key, embed_dim). num_key is equal the number of
                    points on feature map from all levels.
                - output_proposals (Tensor): The normalized proposal
                    after a inverse sigmoid, has shape (bs, num_keys, 4).
        """

        N, S, C = memory.shape
        proposals = []
        _cur = 0
        for lvl, (H, W) in enumerate(spatial_shapes):
            mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H * W)].view(
                N, H, W, 1)
            valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)
            valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)

            grid_y, grid_x = torch.meshgrid(
                torch.linspace(
                    0, H - 1, H, dtype=torch.float32, device=memory.device),
                torch.linspace(
                    0, W - 1, W, dtype=torch.float32, device=memory.device))
            grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)

            scale = torch.cat([valid_W.unsqueeze(-1),
                               valid_H.unsqueeze(-1)], 1).view(N, 1, 1, 2)
            grid = (grid.unsqueeze(0).expand(N, -1, -1, -1) + 0.5) / scale
            proposal = grid.view(N, -1, 2)
            proposals.append(proposal)
            _cur += (H * W)
        output_proposals = torch.cat(proposals, 1)
        output_proposals_valid = ((output_proposals > 0.01) &
                                  (output_proposals < 0.99)).all(
                                      -1, keepdim=True)
        output_proposals = torch.log(output_proposals / (1 - output_proposals))
        output_proposals = output_proposals.masked_fill(
            memory_padding_mask.unsqueeze(-1), float('inf'))
        output_proposals = output_proposals.masked_fill(
            ~output_proposals_valid, float('inf'))

        output_memory = memory
        output_memory = output_memory.masked_fill(
            memory_padding_mask.unsqueeze(-1), float(0))
        output_memory = output_memory.masked_fill(~output_proposals_valid,
                                                  float(0))
        output_memory = self.enc_output_norm(self.enc_output(output_memory))
        return output_memory, output_proposals

    @staticmethod
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """Get the reference points used in decoder.

        Args:
            spatial_shapes (Tensor): The shape of all feature maps,
                has shape (num_level, 2).
            valid_ratios (Tensor): The radios of valid points on the
                feature map, has shape (bs, num_levels, 2).
            device (obj:`device`): The device where reference_points should be.

        Returns:
            Tensor: reference points used in decoder, has \
                shape (bs, num_keys, num_levels, 2).
        """
        reference_points_list = []
        for lvl, (H, W) in enumerate(spatial_shapes):
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(
                    0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(
                    0.5, W - 0.5, W, dtype=torch.float32, device=device))
            ref_y = ref_y.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 1] * H)
            ref_x = ref_x.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 0] * W)
            ref = torch.stack((ref_x, ref_y), -1)
            reference_points_list.append(ref)
        reference_points = torch.cat(reference_points_list, 1)
        reference_points = reference_points[:, :, None] * valid_ratios[:, None]
        return reference_points

    def get_valid_ratio(self, mask):
        """Get the valid radios of feature maps of all level."""
        _, H, W = mask.shape
        valid_H = torch.sum(~mask[:, :, 0], 1)
        valid_W = torch.sum(~mask[:, 0, :], 1)
        valid_ratio_h = valid_H.float() / H
        valid_ratio_w = valid_W.float() / W
        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)
        return valid_ratio

    def get_proposal_pos_embed(self,
                               proposals,
                               num_pos_feats=128,
                               temperature=10000):
        """Get the position embedding of proposal."""
        scale = 2 * math.pi
        dim_t = torch.arange(
            num_pos_feats, dtype=torch.float32, device=proposals.device)
        dim_t = temperature**(2 * (dim_t // 2) / num_pos_feats)
        # N, L, 4
        proposals = proposals.sigmoid() * scale
        # N, L, 4, 128
        pos = proposals[:, :, :, None] / dim_t
        # N, L, 4, 64, 2
        pos = torch.stack((pos[:, :, :, 0::2].sin(), pos[:, :, :, 1::2].cos()),
                          dim=4).flatten(2)
        return pos

    def forward(self,
                mlvl_feats,
                mlvl_masks,
                query_embed,
                mlvl_pos_embeds,
                kpt_branches=None,
                cls_branches=None,
                **kwargs):
        """Forward function for `Transformer`.

        Args:
            mlvl_feats (list(Tensor)): Input queries from different level.
                Each element has shape [bs, embed_dims, h, w].
            mlvl_masks (list(Tensor)): The key_padding_mask from different
                level used for encoder and decoder, each element has shape
                [bs, h, w].
            query_embed (Tensor): The query embedding for decoder,
                with shape [num_query, c].
            mlvl_pos_embeds (list(Tensor)): The positional encoding
                of feats from different level, has the shape
                 [bs, embed_dims, h, w].
            kpt_branches (obj:`nn.ModuleList`): Keypoint Regression heads for
                feature maps from each decoder layer. Only would be passed when
                `with_box_refine` is Ture. Default to None.
            cls_branches (obj:`nn.ModuleList`): Classification heads for
                feature maps from each decoder layer. Only would be passed when
                `as_two_stage` is Ture. Default to None.

        Returns:
            tuple[Tensor]: results of decoder containing the following tensor.

                - inter_states: Outputs from decoder. If
                    `return_intermediate_dec` is True output has shape \
                    (num_dec_layers, bs, num_query, embed_dims), else has \
                    shape (1, bs, num_query, embed_dims).
                - init_reference_out: The initial value of reference \
                    points, has shape (bs, num_queries, 4).
                - inter_references_out: The internal value of reference \
                    points in decoder, has shape \
                    (num_dec_layers, bs,num_query, embed_dims)
                - enc_outputs_class: The classification score of proposals \
                    generated from encoder's feature maps, has shape \
                    (batch, h*w, num_classes). \
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
                - enc_outputs_kpt_unact: The regression results generated from \
                    encoder's feature maps., has shape (batch, h*w, K*2).
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
        """
        assert self.as_two_stage or query_embed is not None

        feat_flatten = []
        mask_flatten = []
        lvl_pos_embed_flatten = []
        spatial_shapes = []
        for lvl, (feat, mask, pos_embed) in enumerate(
                zip(mlvl_feats, mlvl_masks, mlvl_pos_embeds)):
            bs, c, h, w = feat.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            feat = feat.flatten(2).transpose(1, 2)
            mask = mask.flatten(1)
            pos_embed = pos_embed.flatten(2).transpose(1, 2)
            lvl_pos_embed = pos_embed + self.level_embeds[lvl].view(1, 1, -1)
            lvl_pos_embed_flatten.append(lvl_pos_embed)
            feat_flatten.append(feat)
            mask_flatten.append(mask)
        feat_flatten = torch.cat(feat_flatten, 1)
        mask_flatten = torch.cat(mask_flatten, 1)
        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=feat_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        reference_points = \
            self.get_reference_points(spatial_shapes,
                                      valid_ratios,
                                      device=feat.device)

        feat_flatten = feat_flatten.permute(1, 0, 2)  # (H*W, bs, embed_dims)
        lvl_pos_embed_flatten = lvl_pos_embed_flatten.permute(
            1, 0, 2)  # (H*W, bs, embed_dims)
        memory = self.encoder(
            query=feat_flatten,
            key=None,
            value=None,
            query_pos=lvl_pos_embed_flatten,
            query_key_padding_mask=mask_flatten,
            spatial_shapes=spatial_shapes,
            reference_points=reference_points,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            **kwargs)

        memory = memory.permute(1, 0, 2)
        bs, _, c = memory.shape

        hm_proto = None
        if self.training:
            hm_memory = memory[
                :, level_start_index[0]:level_start_index[1], :]
            hm_pos_embed = lvl_pos_embed_flatten[
                level_start_index[0]:level_start_index[1], :, :]
            hm_mask = mask_flatten[
                :, level_start_index[0]:level_start_index[1]]
            hm_reference_points = reference_points[
                :, level_start_index[0]:level_start_index[1], [0], :]
            hm_memory = hm_memory.permute(1, 0, 2)
            hm_memory = self.hm_encoder(
                query=hm_memory,
                key=None,
                value=None,
                query_pose=hm_pos_embed,
                query_key_padding_mask=hm_mask,
                spatial_shapes=spatial_shapes[[0]],
                reference_points=hm_reference_points,
                level_start_index=level_start_index[0],
                valid_ratios=valid_ratios[:, [0], :],
                **kwargs)
            hm_memory = hm_memory.permute(1, 0, 2).reshape(bs,
                spatial_shapes[0, 0], spatial_shapes[0, 1], -1)
            hm_proto = (hm_memory, mlvl_masks[0])

        if self.as_two_stage:
            output_memory, output_proposals = \
                self.gen_encoder_output_proposals(
                    memory, mask_flatten, spatial_shapes)
            enc_outputs_class = cls_branches[self.decoder.num_layers](
                output_memory)
            enc_outputs_kpt_unact = \
                kpt_branches[self.decoder.num_layers](output_memory)
            enc_outputs_kpt_unact[..., 0::2] += output_proposals[..., 0:1]
            enc_outputs_kpt_unact[..., 1::2] += output_proposals[..., 1:2]

            topk = self.two_stage_num_proposals
            topk_proposals = torch.topk(
                enc_outputs_class[..., 0], topk, dim=1)[1]
            # topk_coords_unact = torch.gather(
            #     enc_outputs_coord_unact, 1,
            #     topk_proposals.unsqueeze(-1).repeat(1, 1, 4))
            # topk_coords_unact = topk_coords_unact.detach()
            topk_kpts_unact = torch.gather(
                enc_outputs_kpt_unact, 1,
                topk_proposals.unsqueeze(-1).repeat(
                    1, 1, enc_outputs_kpt_unact.size(-1)))
            topk_kpts_unact = topk_kpts_unact.detach()
            
            # 获取对应topk的encoder输出的token作为pose-query的初始化
            tgt_undetach = torch.gather(output_memory, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, self.embed_dims))
            tgt_detach = tgt_undetach.detach()

            reference_points = topk_kpts_unact.sigmoid()
            init_reference_out = reference_points
            # learnable query and query_pos
            query_pos, query_learned = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query_learned = query_learned.unsqueeze(0).expand(bs, -1, -1)
            query = query_learned + tgt_detach
        else:
            query_pos, query = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query = query.unsqueeze(0).expand(bs, -1, -1)
            reference_points = self.reference_points(query_pos).sigmoid()
            init_reference_out = reference_points

        # decoder
        query = query.permute(1, 0, 2)
        memory = memory.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        inter_states, inter_references = self.decoder(
            query=query,
            key=None,
            value=memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            kpt_branches=kpt_branches,
            **kwargs)

        inter_references_out = inter_references
        if self.as_two_stage:
            return inter_states, init_reference_out, \
                   inter_references_out, enc_outputs_class, \
                   enc_outputs_kpt_unact, hm_proto, memory
        return inter_states, init_reference_out, \
               inter_references_out, None, None, None, None, None, hm_proto

    def forward_refine(self,
                       mlvl_masks,
                       memory,
                       reference_points_pose,
                       img_inds,
                       kpt_branches=None,
                       **kwargs):
        mask_flatten = []
        spatial_shapes = []
        for lvl, mask in enumerate(mlvl_masks):
            bs, h, w = mask.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            mask = mask.flatten(1)
            mask_flatten.append(mask)
        mask_flatten = torch.cat(mask_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=mask_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        # pose refinement (17 queries corresponding to 17 keypoints)
        # learnable query and query_pos
        refine_query_embedding = self.refine_query_embedding.weight
        query_pos, query = torch.split(
            refine_query_embedding, refine_query_embedding.size(1) // 2, dim=1)
        pos_num = reference_points_pose.size(0)
        query_pos = query_pos.unsqueeze(0).expand(pos_num, -1, -1)
        query = query.unsqueeze(0).expand(pos_num, -1, -1)
        reference_points = reference_points_pose.reshape(
            pos_num,
            reference_points_pose.size(1) // 2, 2)
        query = query.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        pos_memory = memory[:, img_inds, :]
        mask_flatten = mask_flatten[img_inds, :]
        valid_ratios = valid_ratios[img_inds, ...]
        inter_states, inter_references = self.refine_decoder(
            query=query,
            key=None,
            value=pos_memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            reg_branches=kpt_branches,
            **kwargs)
        # [num_decoder, num_query, bs, embed_dim]

        init_reference_out = reference_points
        return inter_states, init_reference_out, inter_references

# 修改Pose-query的初始化 --- 修改时间： 2024-11-6
@TRANSFORMER.register_module()
class PETRTransformerV3(Transformer):
    """Implements the PETR transformer.

    Args:
        as_two_stage (bool): Generate query from encoder features.
            Default: False.
        num_feature_levels (int): Number of feature maps from FPN:
            Default: 4.
        two_stage_num_proposals (int): Number of proposals when set
            `as_two_stage` as True. Default: 300.
    """

    def __init__(self,
                 hm_encoder=dict(
                     type='DetrTransformerEncoder',
                     num_layers=1,
                     transformerlayers=dict(
                         type='BaseTransformerLayer',
                         attn_cfgs=dict(
                             type='MultiScaleDeformableAttention',
                             embed_dims=256,
                             num_levels=1),
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'ffn',
                                          'norm'))),
                 refine_decoder=dict(
                     type='DeformableDetrTransformerDecoder',
                     num_layers=1,
                     return_intermediate=True,
                     transformerlayers=dict(
                         type='DetrTransformerDecoderLayer',
                         attn_cfgs=[
                             dict(
                                 type='MultiheadAttention',
                                 embed_dims=256,
                                 num_heads=8,
                                 dropout=0.1),
                             dict(
                                 type='MultiScaleDeformableAttention',
                                 embed_dims=256)
                         ],
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'cross_attn',
                                          'norm', 'ffn', 'norm'))),
                 as_two_stage=True,
                 num_feature_levels=4,
                 two_stage_num_proposals=300,
                 num_keypoints=17,
                 **kwargs):
        super(PETRTransformerV3, self).__init__(**kwargs)
        self.as_two_stage = as_two_stage
        self.num_feature_levels = num_feature_levels
        self.two_stage_num_proposals = two_stage_num_proposals
        self.embed_dims = self.encoder.embed_dims
        self.num_keypoints = num_keypoints
        self.init_layers()
        self.hm_encoder = build_transformer_layer_sequence(hm_encoder)
        self.refine_decoder = build_transformer_layer_sequence(refine_decoder)
        

    def init_layers(self):
        """Initialize layers of the DeformableDetrTransformer."""
        self.level_embeds = nn.Parameter(
            torch.Tensor(self.num_feature_levels, self.embed_dims))

        if self.as_two_stage:
            self.enc_output = nn.Linear(self.embed_dims, self.embed_dims)
            self.enc_output_norm = nn.LayerNorm(self.embed_dims)
            self.refine_query_embedding = nn.Embedding(self.num_keypoints,
                                                       self.embed_dims * 2)
        else:
            self.reference_points = nn.Linear(self.embed_dims,
                                              2 * self.num_keypoints)

    def init_weights(self):
        """Initialize the transformer weights."""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        for m in self.modules():
            if isinstance(m, MultiScaleDeformableAttention):
                m.init_weights()
        for m in self.modules():
            if isinstance(m, MultiScaleDeformablePoseAttention):
                m.init_weights()
        if not self.as_two_stage:
            xavier_init(self.reference_points, distribution='uniform', bias=0.)
        normal_(self.level_embeds)
        normal_(self.refine_query_embedding.weight)

    def gen_encoder_output_proposals(self, memory, memory_padding_mask,
                                     spatial_shapes):
        """Generate proposals from encoded memory.

        Args:
            memory (Tensor): The output of encoder, has shape
                (bs, num_key, embed_dim). num_key is equal the number of points
                on feature map from all level.
            memory_padding_mask (Tensor): Padding mask for memory.
                has shape (bs, num_key).
            spatial_shapes (Tensor): The shape of all feature maps.
                has shape (num_level, 2).

        Returns:
            tuple: A tuple of feature map and bbox prediction.

                - output_memory (Tensor): The input of decoder, has shape
                    (bs, num_key, embed_dim). num_key is equal the number of
                    points on feature map from all levels.
                - output_proposals (Tensor): The normalized proposal
                    after a inverse sigmoid, has shape (bs, num_keys, 4).
        """

        N, S, C = memory.shape
        proposals = []
        _cur = 0
        for lvl, (H, W) in enumerate(spatial_shapes):
            mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H * W)].view(
                N, H, W, 1)
            valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)
            valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)

            grid_y, grid_x = torch.meshgrid(
                torch.linspace(
                    0, H - 1, H, dtype=torch.float32, device=memory.device),
                torch.linspace(
                    0, W - 1, W, dtype=torch.float32, device=memory.device))
            grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)

            scale = torch.cat([valid_W.unsqueeze(-1),
                               valid_H.unsqueeze(-1)], 1).view(N, 1, 1, 2)
            grid = (grid.unsqueeze(0).expand(N, -1, -1, -1) + 0.5) / scale
            proposal = grid.view(N, -1, 2)
            proposals.append(proposal)
            _cur += (H * W)
        output_proposals = torch.cat(proposals, 1)
        output_proposals_valid = ((output_proposals > 0.01) &
                                  (output_proposals < 0.99)).all(
                                      -1, keepdim=True)
        output_proposals = torch.log(output_proposals / (1 - output_proposals))
        output_proposals = output_proposals.masked_fill(
            memory_padding_mask.unsqueeze(-1), float('inf'))
        output_proposals = output_proposals.masked_fill(
            ~output_proposals_valid, float('inf'))

        output_memory = memory
        output_memory = output_memory.masked_fill(
            memory_padding_mask.unsqueeze(-1), float(0))
        output_memory = output_memory.masked_fill(~output_proposals_valid,
                                                  float(0))
        output_memory = self.enc_output_norm(self.enc_output(output_memory))
        return output_memory, output_proposals

    @staticmethod
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """Get the reference points used in decoder.

        Args:
            spatial_shapes (Tensor): The shape of all feature maps,
                has shape (num_level, 2).
            valid_ratios (Tensor): The radios of valid points on the
                feature map, has shape (bs, num_levels, 2).
            device (obj:`device`): The device where reference_points should be.

        Returns:
            Tensor: reference points used in decoder, has \
                shape (bs, num_keys, num_levels, 2).
        """
        reference_points_list = []
        for lvl, (H, W) in enumerate(spatial_shapes):
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(
                    0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(
                    0.5, W - 0.5, W, dtype=torch.float32, device=device))
            ref_y = ref_y.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 1] * H)
            ref_x = ref_x.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 0] * W)
            ref = torch.stack((ref_x, ref_y), -1)
            reference_points_list.append(ref)
        reference_points = torch.cat(reference_points_list, 1)
        reference_points = reference_points[:, :, None] * valid_ratios[:, None]
        return reference_points

    def get_valid_ratio(self, mask):
        """Get the valid radios of feature maps of all level."""
        _, H, W = mask.shape
        valid_H = torch.sum(~mask[:, :, 0], 1)
        valid_W = torch.sum(~mask[:, 0, :], 1)
        valid_ratio_h = valid_H.float() / H
        valid_ratio_w = valid_W.float() / W
        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)
        return valid_ratio

    def get_proposal_pos_embed(self,
                               proposals,
                               num_pos_feats=128,
                               temperature=10000):
        """Get the position embedding of proposal."""
        scale = 2 * math.pi
        dim_t = torch.arange(
            num_pos_feats, dtype=torch.float32, device=proposals.device)
        dim_t = temperature**(2 * (dim_t // 2) / num_pos_feats)
        # N, L, 4
        proposals = proposals.sigmoid() * scale
        # N, L, 4, 128
        pos = proposals[:, :, :, None] / dim_t
        # N, L, 4, 64, 2
        pos = torch.stack((pos[:, :, :, 0::2].sin(), pos[:, :, :, 1::2].cos()),
                          dim=4).flatten(2)
        return pos

    def forward(self,
                mlvl_feats,
                mlvl_masks,
                query_embed,
                mlvl_pos_embeds,
                kpt_branches=None,
                cls_branches=None,
                sigma_branches=None,
                **kwargs):
        """Forward function for `Transformer`.

        Args:
            mlvl_feats (list(Tensor)): Input queries from different level.
                Each element has shape [bs, embed_dims, h, w].
            mlvl_masks (list(Tensor)): The key_padding_mask from different
                level used for encoder and decoder, each element has shape
                [bs, h, w].
            query_embed (Tensor): The query embedding for decoder,
                with shape [num_query, c].
            mlvl_pos_embeds (list(Tensor)): The positional encoding
                of feats from different level, has the shape
                 [bs, embed_dims, h, w].
            kpt_branches (obj:`nn.ModuleList`): Keypoint Regression heads for
                feature maps from each decoder layer. Only would be passed when
                `with_box_refine` is Ture. Default to None.
            cls_branches (obj:`nn.ModuleList`): Classification heads for
                feature maps from each decoder layer. Only would be passed when
                `as_two_stage` is Ture. Default to None.

        Returns:
            tuple[Tensor]: results of decoder containing the following tensor.

                - inter_states: Outputs from decoder. If
                    `return_intermediate_dec` is True output has shape \
                    (num_dec_layers, bs, num_query, embed_dims), else has \
                    shape (1, bs, num_query, embed_dims).
                - init_reference_out: The initial value of reference \
                    points, has shape (bs, num_queries, 4).
                - inter_references_out: The internal value of reference \
                    points in decoder, has shape \
                    (num_dec_layers, bs,num_query, embed_dims)
                - enc_outputs_class: The classification score of proposals \
                    generated from encoder's feature maps, has shape \
                    (batch, h*w, num_classes). \
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
                - enc_outputs_kpt_unact: The regression results generated from \
                    encoder's feature maps., has shape (batch, h*w, K*2).
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
        """
        assert self.as_two_stage or query_embed is not None

        feat_flatten = []
        mask_flatten = []
        lvl_pos_embed_flatten = []
        spatial_shapes = []
        for lvl, (feat, mask, pos_embed) in enumerate(
                zip(mlvl_feats, mlvl_masks, mlvl_pos_embeds)):
            bs, c, h, w = feat.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            feat = feat.flatten(2).transpose(1, 2)
            mask = mask.flatten(1)
            pos_embed = pos_embed.flatten(2).transpose(1, 2)
            lvl_pos_embed = pos_embed + self.level_embeds[lvl].view(1, 1, -1)
            lvl_pos_embed_flatten.append(lvl_pos_embed)
            feat_flatten.append(feat)
            mask_flatten.append(mask)
        feat_flatten = torch.cat(feat_flatten, 1)
        mask_flatten = torch.cat(mask_flatten, 1)
        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=feat_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        reference_points = \
            self.get_reference_points(spatial_shapes,
                                      valid_ratios,
                                      device=feat.device)

        feat_flatten = feat_flatten.permute(1, 0, 2)  # (H*W, bs, embed_dims)
        lvl_pos_embed_flatten = lvl_pos_embed_flatten.permute(
            1, 0, 2)  # (H*W, bs, embed_dims)
        memory = self.encoder(
            query=feat_flatten,
            key=None,
            value=None,
            query_pos=lvl_pos_embed_flatten,
            query_key_padding_mask=mask_flatten,
            spatial_shapes=spatial_shapes,
            reference_points=reference_points,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            **kwargs)

        import time
        start = time.time()
        memory = memory.permute(1, 0, 2)
        bs, _, c = memory.shape

        hm_proto = None
        if self.training:
            hm_memory = memory[
                :, level_start_index[0]:level_start_index[1], :]
            hm_pos_embed = lvl_pos_embed_flatten[
                level_start_index[0]:level_start_index[1], :, :]
            hm_mask = mask_flatten[
                :, level_start_index[0]:level_start_index[1]]
            hm_reference_points = reference_points[
                :, level_start_index[0]:level_start_index[1], [0], :]
            hm_memory = hm_memory.permute(1, 0, 2)
            hm_memory = self.hm_encoder(
                query=hm_memory,
                key=None,
                value=None,
                query_pose=hm_pos_embed,
                query_key_padding_mask=hm_mask,
                spatial_shapes=spatial_shapes[[0]],
                reference_points=hm_reference_points,
                level_start_index=level_start_index[0],
                valid_ratios=valid_ratios[:, [0], :],
                **kwargs)
            hm_memory = hm_memory.permute(1, 0, 2).reshape(bs,
                spatial_shapes[0, 0], spatial_shapes[0, 1], -1)
            hm_proto = (hm_memory, mlvl_masks[0])

        if self.as_two_stage:
            output_memory, output_proposals = \
                self.gen_encoder_output_proposals(
                    memory, mask_flatten, spatial_shapes)
            enc_outputs_class = cls_branches[self.decoder.num_layers](
                output_memory)
            enc_outputs_kpt_unact = \
                kpt_branches[self.decoder.num_layers](output_memory)
                
            enc_outputs_sigma = sigma_branches[self.decoder.num_layers](
                output_memory).sigmoid()
            
            enc_outputs_kpt_unact[..., 0::2] += output_proposals[..., 0:1]
            enc_outputs_kpt_unact[..., 1::2] += output_proposals[..., 1:2]

            topk = self.two_stage_num_proposals
            topk_proposals = torch.topk(
                enc_outputs_class[..., 0], topk, dim=1)[1]
            # topk_coords_unact = torch.gather(
            #     enc_outputs_coord_unact, 1,
            #     topk_proposals.unsqueeze(-1).repeat(1, 1, 4))
            # topk_coords_unact = topk_coords_unact.detach()
            topk_kpts_unact = torch.gather(
                enc_outputs_kpt_unact, 1,
                topk_proposals.unsqueeze(-1).repeat(
                    1, 1, enc_outputs_kpt_unact.size(-1)))
            topk_kpts_unact = topk_kpts_unact.detach()
            
            # 获取对应topk的encoder输出的token作为pose-query的初始化
            tgt_undetach = torch.gather(output_memory, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, self.embed_dims))
            tgt_detach = tgt_undetach.detach()

            reference_points = topk_kpts_unact.sigmoid()
            init_reference_out = reference_points
            # learnable query and query_pos
            query_pos, query_learned = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query_learned = query_learned.unsqueeze(0).expand(bs, -1, -1)
            query = query_learned + tgt_detach
        else:
            query_pos, query = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query = query.unsqueeze(0).expand(bs, -1, -1)
            reference_points = self.reference_points(query_pos).sigmoid()
            init_reference_out = reference_points

        # decoder
        query = query.permute(1, 0, 2)
        memory = memory.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        inter_states, inter_references = self.decoder(
            query=query,
            key=None,
            value=memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            kpt_branches=kpt_branches,
            **kwargs)
        end = time.time()
        print(f'only pose-decoder time: {end - start:.6f}')

        inter_references_out = inter_references
        if self.as_two_stage:
            return inter_states, init_reference_out, \
                   inter_references_out, enc_outputs_class, \
                   enc_outputs_kpt_unact, enc_outputs_sigma, hm_proto, memory
        return inter_states, init_reference_out, \
               inter_references_out, None, None, None, None, None, hm_proto

    def forward_refine(self,
                       mlvl_masks,
                       memory,
                       reference_points_pose,
                       img_inds,
                       kpt_branches=None,
                       **kwargs):
        mask_flatten = []
        spatial_shapes = []
        for lvl, mask in enumerate(mlvl_masks):
            bs, h, w = mask.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            mask = mask.flatten(1)
            mask_flatten.append(mask)
        mask_flatten = torch.cat(mask_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=mask_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        # pose refinement (17 queries corresponding to 17 keypoints)
        # learnable query and query_pos
        refine_query_embedding = self.refine_query_embedding.weight
        query_pos, query = torch.split(
            refine_query_embedding, refine_query_embedding.size(1) // 2, dim=1)
        pos_num = reference_points_pose.size(0)
        query_pos = query_pos.unsqueeze(0).expand(pos_num, -1, -1)
        query = query.unsqueeze(0).expand(pos_num, -1, -1)
        reference_points = reference_points_pose.reshape(
            pos_num,
            reference_points_pose.size(1) // 2, 2)
        query = query.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        pos_memory = memory[:, img_inds, :]
        mask_flatten = mask_flatten[img_inds, :]
        valid_ratios = valid_ratios[img_inds, ...]
        inter_states, inter_references = self.refine_decoder(
            query=query,
            key=None,
            value=pos_memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            reg_branches=kpt_branches,
            **kwargs)
        # [num_decoder, num_query, bs, embed_dim]

        init_reference_out = reference_points
        return inter_states, init_reference_out, inter_references

@TRANSFORMER.register_module()
class PETRTransformerV4(Transformer):
    """Implements the PETR transformer.

    Args:
        as_two_stage (bool): Generate query from encoder features.
            Default: False.
        num_feature_levels (int): Number of feature maps from FPN:
            Default: 4.
        two_stage_num_proposals (int): Number of proposals when set
            `as_two_stage` as True. Default: 300.
    """

    def __init__(self,
                 hm_encoder=dict(
                     type='DetrTransformerEncoder',
                     num_layers=1,
                     transformerlayers=dict(
                         type='BaseTransformerLayer',
                         attn_cfgs=dict(
                             type='MultiScaleDeformableAttention',
                             embed_dims=256,
                             num_levels=1),
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'ffn',
                                          'norm'))),
                 refine_decoder=dict(
                     type='DeformableDetrTransformerDecoder',
                     num_layers=1,
                     return_intermediate=True,
                     transformerlayers=dict(
                         type='DetrTransformerDecoderLayer',
                         attn_cfgs=[
                             dict(
                                 type='MultiheadAttention',
                                 embed_dims=256,
                                 num_heads=8,
                                 dropout=0.1),
                             dict(
                                 type='MultiScaleDeformableAttention',
                                 embed_dims=256)
                         ],
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'cross_attn',
                                          'norm', 'ffn', 'norm'))),
                 as_two_stage=True,
                 num_feature_levels=4,
                 two_stage_num_proposals=300,
                 num_keypoints=17,
                 **kwargs):
        super(PETRTransformerV4, self).__init__(**kwargs)
        self.as_two_stage = as_two_stage
        self.num_feature_levels = num_feature_levels
        self.two_stage_num_proposals = two_stage_num_proposals
        self.embed_dims = self.encoder.embed_dims
        self.num_keypoints = num_keypoints
        self.init_layers()
        self.hm_encoder = build_transformer_layer_sequence(hm_encoder)
        self.refine_decoder = build_transformer_layer_sequence(refine_decoder)

    def init_layers(self):
        """Initialize layers of the DeformableDetrTransformer."""
        self.level_embeds = nn.Parameter(
            torch.Tensor(self.num_feature_levels, self.embed_dims))

        if self.as_two_stage:
            self.enc_output = nn.Linear(self.embed_dims, self.embed_dims)
            self.enc_output_norm = nn.LayerNorm(self.embed_dims)
            self.refine_query_embedding = nn.Embedding(self.num_keypoints,
                                                       self.embed_dims * 2)
        # else:
            self.reference_points = nn.Linear(self.embed_dims,
                                              2 * self.num_keypoints)

    def init_weights(self):
        """Initialize the transformer weights."""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        for m in self.modules():
            if isinstance(m, MultiScaleDeformableAttention):
                m.init_weights()
        for m in self.modules():
            if isinstance(m, MultiScaleDeformablePoseAttention):
                m.init_weights()
        if not self.as_two_stage:
            xavier_init(self.reference_points, distribution='uniform', bias=0.)
        normal_(self.level_embeds)
        normal_(self.refine_query_embedding.weight)

    def gen_encoder_output_proposals(self, memory, memory_padding_mask,
                                     spatial_shapes):
        """Generate proposals from encoded memory.

        Args:
            memory (Tensor): The output of encoder, has shape
                (bs, num_key, embed_dim). num_key is equal the number of points
                on feature map from all level.
            memory_padding_mask (Tensor): Padding mask for memory.
                has shape (bs, num_key).
            spatial_shapes (Tensor): The shape of all feature maps.
                has shape (num_level, 2).

        Returns:
            tuple: A tuple of feature map and bbox prediction.

                - output_memory (Tensor): The input of decoder, has shape
                    (bs, num_key, embed_dim). num_key is equal the number of
                    points on feature map from all levels.
                - output_proposals (Tensor): The normalized proposal
                    after a inverse sigmoid, has shape (bs, num_keys, 4).
        """

        N, S, C = memory.shape
        proposals = []
        _cur = 0
        for lvl, (H, W) in enumerate(spatial_shapes):
            mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H * W)].view(
                N, H, W, 1)
            valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)
            valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)

            grid_y, grid_x = torch.meshgrid(
                torch.linspace(
                    0, H - 1, H, dtype=torch.float32, device=memory.device),
                torch.linspace(
                    0, W - 1, W, dtype=torch.float32, device=memory.device))
            grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)

            scale = torch.cat([valid_W.unsqueeze(-1),
                               valid_H.unsqueeze(-1)], 1).view(N, 1, 1, 2)
            grid = (grid.unsqueeze(0).expand(N, -1, -1, -1) + 0.5) / scale
            proposal = grid.view(N, -1, 2)
            proposals.append(proposal)
            _cur += (H * W)
        output_proposals = torch.cat(proposals, 1)
        output_proposals_valid = ((output_proposals > 0.01) &
                                  (output_proposals < 0.99)).all(
                                      -1, keepdim=True)
        output_proposals = torch.log(output_proposals / (1 - output_proposals))
        output_proposals = output_proposals.masked_fill(
            memory_padding_mask.unsqueeze(-1), float('inf'))
        output_proposals = output_proposals.masked_fill(
            ~output_proposals_valid, float('inf'))

        output_memory = memory
        output_memory = output_memory.masked_fill(
            memory_padding_mask.unsqueeze(-1), float(0))
        output_memory = output_memory.masked_fill(~output_proposals_valid,
                                                  float(0))
        output_memory = self.enc_output_norm(self.enc_output(output_memory))
        return output_memory, output_proposals

    @staticmethod
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """Get the reference points used in decoder.

        Args:
            spatial_shapes (Tensor): The shape of all feature maps,
                has shape (num_level, 2).
            valid_ratios (Tensor): The radios of valid points on the
                feature map, has shape (bs, num_levels, 2).
            device (obj:`device`): The device where reference_points should be.

        Returns:
            Tensor: reference points used in decoder, has \
                shape (bs, num_keys, num_levels, 2).
        """
        reference_points_list = []
        for lvl, (H, W) in enumerate(spatial_shapes):
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(
                    0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(
                    0.5, W - 0.5, W, dtype=torch.float32, device=device))
            ref_y = ref_y.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 1] * H)
            ref_x = ref_x.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 0] * W)
            ref = torch.stack((ref_x, ref_y), -1)
            reference_points_list.append(ref)
        reference_points = torch.cat(reference_points_list, 1)
        reference_points = reference_points[:, :, None] * valid_ratios[:, None]
        return reference_points

    def get_valid_ratio(self, mask):
        """Get the valid radios of feature maps of all level."""
        _, H, W = mask.shape
        valid_H = torch.sum(~mask[:, :, 0], 1)
        valid_W = torch.sum(~mask[:, 0, :], 1)
        valid_ratio_h = valid_H.float() / H
        valid_ratio_w = valid_W.float() / W
        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)
        return valid_ratio

    def get_proposal_pos_embed(self,
                               proposals,
                               num_pos_feats=128,
                               temperature=10000):
        """Get the position embedding of proposal."""
        scale = 2 * math.pi
        dim_t = torch.arange(
            num_pos_feats, dtype=torch.float32, device=proposals.device)
        dim_t = temperature**(2 * (dim_t // 2) / num_pos_feats)
        # N, L, 4
        proposals = proposals.sigmoid() * scale
        # N, L, 4, 128
        pos = proposals[:, :, :, None] / dim_t
        # N, L, 4, 64, 2
        pos = torch.stack((pos[:, :, :, 0::2].sin(), pos[:, :, :, 1::2].cos()),
                          dim=4).flatten(2)
        return pos

    def forward(self,
                mlvl_feats,
                mlvl_masks,
                query_embed,
                mlvl_pos_embeds,
                kpt_branches=None,
                cls_branches=None,
                **kwargs):
        """Forward function for `Transformer`.

        Args:
            mlvl_feats (list(Tensor)): Input queries from different level.
                Each element has shape [bs, embed_dims, h, w].
            mlvl_masks (list(Tensor)): The key_padding_mask from different
                level used for encoder and decoder, each element has shape
                [bs, h, w].
            query_embed (Tensor): The query embedding for decoder,
                with shape [num_query, c].
            mlvl_pos_embeds (list(Tensor)): The positional encoding
                of feats from different level, has the shape
                 [bs, embed_dims, h, w].
            kpt_branches (obj:`nn.ModuleList`): Keypoint Regression heads for
                feature maps from each decoder layer. Only would be passed when
                `with_box_refine` is Ture. Default to None.
            cls_branches (obj:`nn.ModuleList`): Classification heads for
                feature maps from each decoder layer. Only would be passed when
                `as_two_stage` is Ture. Default to None.

        Returns:
            tuple[Tensor]: results of decoder containing the following tensor.

                - inter_states: Outputs from decoder. If
                    `return_intermediate_dec` is True output has shape \
                    (num_dec_layers, bs, num_query, embed_dims), else has \
                    shape (1, bs, num_query, embed_dims).
                - init_reference_out: The initial value of reference \
                    points, has shape (bs, num_queries, 4).
                - inter_references_out: The internal value of reference \
                    points in decoder, has shape \
                    (num_dec_layers, bs,num_query, embed_dims)
                - enc_outputs_class: The classification score of proposals \
                    generated from encoder's feature maps, has shape \
                    (batch, h*w, num_classes). \
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
                - enc_outputs_kpt_unact: The regression results generated from \
                    encoder's feature maps., has shape (batch, h*w, K*2).
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
        """
        assert self.as_two_stage or query_embed is not None

        feat_flatten = []
        mask_flatten = []
        lvl_pos_embed_flatten = []
        spatial_shapes = []
        for lvl, (feat, mask, pos_embed) in enumerate(
                zip(mlvl_feats, mlvl_masks, mlvl_pos_embeds)):
            bs, c, h, w = feat.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            feat = feat.flatten(2).transpose(1, 2)
            mask = mask.flatten(1)
            pos_embed = pos_embed.flatten(2).transpose(1, 2)
            lvl_pos_embed = pos_embed + self.level_embeds[lvl].view(1, 1, -1)
            lvl_pos_embed_flatten.append(lvl_pos_embed)
            feat_flatten.append(feat)
            mask_flatten.append(mask)
        feat_flatten = torch.cat(feat_flatten, 1)
        mask_flatten = torch.cat(mask_flatten, 1)
        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=feat_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        reference_points = \
            self.get_reference_points(spatial_shapes,
                                      valid_ratios,
                                      device=feat.device)

        feat_flatten = feat_flatten.permute(1, 0, 2)  # (H*W, bs, embed_dims)
        lvl_pos_embed_flatten = lvl_pos_embed_flatten.permute(
            1, 0, 2)  # (H*W, bs, embed_dims)
        memory = self.encoder(
            query=feat_flatten,
            key=None,
            value=None,
            query_pos=lvl_pos_embed_flatten,
            query_key_padding_mask=mask_flatten,
            spatial_shapes=spatial_shapes,
            reference_points=reference_points,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            **kwargs)

        memory = memory.permute(1, 0, 2)
        bs, _, c = memory.shape

        hm_proto = None
        if self.training:
            hm_memory = memory[
                :, level_start_index[0]:level_start_index[1], :]
            hm_pos_embed = lvl_pos_embed_flatten[
                level_start_index[0]:level_start_index[1], :, :]
            hm_mask = mask_flatten[
                :, level_start_index[0]:level_start_index[1]]
            hm_reference_points = reference_points[
                :, level_start_index[0]:level_start_index[1], [0], :]
            hm_memory = hm_memory.permute(1, 0, 2)
            hm_memory = self.hm_encoder(
                query=hm_memory,
                key=None,
                value=None,
                query_pose=hm_pos_embed,
                query_key_padding_mask=hm_mask,
                spatial_shapes=spatial_shapes[[0]],
                reference_points=hm_reference_points,
                level_start_index=level_start_index[0],
                valid_ratios=valid_ratios[:, [0], :],
                **kwargs)
            hm_memory = hm_memory.permute(1, 0, 2).reshape(bs,
                spatial_shapes[0, 0], spatial_shapes[0, 1], -1)
            hm_proto = (hm_memory, mlvl_masks[0])

        if self.as_two_stage:
            output_memory, output_proposals = \
                self.gen_encoder_output_proposals(
                    memory, mask_flatten, spatial_shapes)
            enc_outputs_class = cls_branches[self.decoder.num_layers](
                output_memory)
            enc_outputs_kpt_unact = \
                kpt_branches[self.decoder.num_layers](output_memory)
            enc_outputs_kpt_unact[..., 0::2] += output_proposals[..., 0:1]
            enc_outputs_kpt_unact[..., 1::2] += output_proposals[..., 1:2]

        #     topk = self.two_stage_num_proposals
        #     topk_proposals = torch.topk(
        #         enc_outputs_class[..., 0], topk, dim=1)[1]
        #     # topk_coords_unact = torch.gather(
        #     #     enc_outputs_coord_unact, 1,
        #     #     topk_proposals.unsqueeze(-1).repeat(1, 1, 4))
        #     # topk_coords_unact = topk_coords_unact.detach()
        #     topk_kpts_unact = torch.gather(
        #         enc_outputs_kpt_unact, 1,
        #         topk_proposals.unsqueeze(-1).repeat(
        #             1, 1, enc_outputs_kpt_unact.size(-1)))
        #     topk_kpts_unact = topk_kpts_unact.detach()

        #     reference_points = topk_kpts_unact.sigmoid()
        #     init_reference_out = reference_points
        #     # learnable query and query_pos
        #     query_pos, query = torch.split(query_embed, c, dim=1)
        #     query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
        #     query = query.unsqueeze(0).expand(bs, -1, -1)
        # else:
            query_pos, query = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query = query.unsqueeze(0).expand(bs, -1, -1)
            reference_points = self.reference_points(query_pos).sigmoid()
            init_reference_out = reference_points

        # decoder
        query = query.permute(1, 0, 2)
        memory = memory.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        inter_states, inter_references = self.decoder(
            query=query,
            key=None,
            value=memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            kpt_branches=kpt_branches,
            **kwargs)

        inter_references_out = inter_references
        if self.as_two_stage:
            return inter_states, init_reference_out, \
                   inter_references_out, enc_outputs_class, \
                   enc_outputs_kpt_unact, hm_proto, memory
        return inter_states, init_reference_out, \
               inter_references_out, None, None, None, None, None, hm_proto

    def forward_refine(self,
                       mlvl_masks,
                       memory,
                       reference_points_pose,
                       img_inds,
                       kpt_branches=None,
                       **kwargs):
        mask_flatten = []
        spatial_shapes = []
        for lvl, mask in enumerate(mlvl_masks):
            bs, h, w = mask.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            mask = mask.flatten(1)
            mask_flatten.append(mask)
        mask_flatten = torch.cat(mask_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=mask_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        # pose refinement (17 queries corresponding to 17 keypoints)
        # learnable query and query_pos
        refine_query_embedding = self.refine_query_embedding.weight
        query_pos, query = torch.split(
            refine_query_embedding, refine_query_embedding.size(1) // 2, dim=1)
        pos_num = reference_points_pose.size(0)
        query_pos = query_pos.unsqueeze(0).expand(pos_num, -1, -1)
        query = query.unsqueeze(0).expand(pos_num, -1, -1)
        reference_points = reference_points_pose.reshape(
            pos_num,
            reference_points_pose.size(1) // 2, 2)
        query = query.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        pos_memory = memory[:, img_inds, :]
        mask_flatten = mask_flatten[img_inds, :]
        valid_ratios = valid_ratios[img_inds, ...]
        inter_states, inter_references = self.refine_decoder(
            query=query,
            key=None,
            value=pos_memory,
            query_pos=query_pos,
            key_padding_mask=mask_flatten,
            reference_points=reference_points,
            spatial_shapes=spatial_shapes,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            reg_branches=kpt_branches,
            **kwargs)
        # [num_decoder, num_query, bs, embed_dim]

        init_reference_out = reference_points
        return inter_states, init_reference_out, inter_references

@TRANSFORMER_LAYER_SEQUENCE.register_module()
class VideoPoseTransformerDecoderV1(TransformerLayerSequence):
    """Implements the decoder in PETR transformer.

    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        coder_norm_cfg (dict): Config of last normalization layer. Default：
            `LN`.
    """

    def __init__(self,
                 *args,
                 return_intermediate=False,
                 num_keypoints=17,
                 **kwargs):

        super(VideoPoseTransformerDecoderV1, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        self.num_keypoints = num_keypoints

    def forward(self,
                query,
                *args,
                reference_points=None,
                valid_ratios=None,
                kpt_branches=None,
                **kwargs):
        """Forward function for `TransformerDecoder`.

        Args:
            query (Tensor): Input query with shape (num_query, bs, embed_dims).
            reference_points (Tensor): The reference points of offset,
                has shape (bs, num_query, K*2).
            valid_ratios (Tensor): The radios of valid points on the feature
                map, has shape (bs, num_levels, 2).
            kpt_branches: (obj:`nn.ModuleList`): Used for refining the
                regression results. Only would be passed when `with_box_refine`
                is True, otherwise would be passed a `None`.

        Returns:
            tuple (Tensor): Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims] and
                [num_layers, bs, num_query, K*2].
        """
        output = query
        intermediate = []
        intermediate_reference_points = []
        for lid, layer in enumerate(self.layers):
            if reference_points.shape[-1] == self.num_keypoints * 2:
                reference_points_input = \
                    reference_points[:, :, None] * \
                    valid_ratios.repeat(1, 1, self.num_keypoints)[:, None]
            else:
                assert reference_points.shape[-1] == 2
                reference_points_input = reference_points[:, :, None] * \
                                         valid_ratios[:, None]
            output = layer(
                output,
                *args,
                reference_points=reference_points_input,
                **kwargs)
            output = output.permute(1, 0, 2)

            if kpt_branches is not None:
                tmp = kpt_branches[lid](output)
                if reference_points.shape[-1] == self.num_keypoints * 2:
                    new_reference_points = tmp + inverse_sigmoid(
                        reference_points)
                    new_reference_points = new_reference_points.sigmoid()
                else:
                    raise NotImplementedError
                reference_points = new_reference_points.detach()
            

            output = output.permute(1, 0, 2)
            if self.return_intermediate:
                intermediate.append(output)
                intermediate_reference_points.append(reference_points)
                

        if self.return_intermediate:
            return torch.stack(intermediate), torch.stack(
                intermediate_reference_points)

        return output, reference_points


# 适用3帧
@TRANSFORMER_LAYER_SEQUENCE.register_module()
class TransformerDecoderV2(TransformerLayerSequence):
    """Implements the decoder in PETR transformer.

    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        coder_norm_cfg (dict): Config of last normalization layer. Default：
            `LN`.
    """

    def __init__(self,
                 *args,
                 return_intermediate=False,
                 num_keypoints=17,
                 **kwargs):

        super(TransformerDecoderV2, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        self.num_keypoints = num_keypoints

    def forward(self,
                query,
                *args,
                reference_points=None,
                valid_ratios=None,
                pre_kpt_branches=None,
                kpt_branches=None,
                next_kpt_branches=None,
                **kwargs):
        """Forward function for `TransformerDecoder`.

        Args:
            query (Tensor): Input query with shape (num_query, bs, embed_dims).
            reference_points (Tensor): The reference points of offset,
                has shape (bs, num_query, K*2).
            valid_ratios (Tensor): The radios of valid points on the feature
                map, has shape (bs, num_levels, 2).
            kpt_branches: (obj:`nn.ModuleList`): Used for refining the
                regression results. Only would be passed when `with_box_refine`
                is True, otherwise would be passed a `None`.

        Returns:
            tuple (Tensor): Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims] and
                [num_layers, bs, num_query, K*2].
        """
        output = query
        intermediate = []
        intermediate_reference_points = []
        for lid, layer in enumerate(self.layers):
            if reference_points.shape[-1] == self.num_keypoints * 2:
                reference_points_input = \
                    reference_points[:, :, None] * \
                    valid_ratios.repeat(1, 1, self.num_keypoints)[:, None]
            else:
                assert reference_points.shape[-1] == 2
                reference_points_input = reference_points[:, :, None] * \
                                         valid_ratios[:, None]
            output = layer(
                output,
                *args,
                reference_points=reference_points_input,
                **kwargs)
            output = output.permute(1, 0, 2)

            # 回归出辅助帧的相对偏移量
            if kpt_branches is not None and pre_kpt_branches is not None and next_kpt_branches is not None:
                pre_tmp = pre_kpt_branches[lid](output)
                tmp = kpt_branches[lid](output)
                next_tmp = next_kpt_branches[lid](output)
                tmps = torch.concat([pre_tmp, tmp, next_tmp], dim=1)
                if reference_points.shape[-1] == self.num_keypoints * 2:
                    new_reference_points = tmps + inverse_sigmoid(
                        reference_points)
                    new_reference_points = new_reference_points.sigmoid()
                else:
                    raise NotImplementedError
                # reference_points = new_reference_points.detach()
                reference_points = new_reference_points
                
            
            output = output.permute(1, 0, 2)
            if self.return_intermediate:
                intermediate.append(output)
                intermediate_reference_points.append(reference_points)
                

        if self.return_intermediate:
            return torch.stack(intermediate), torch.stack(
                intermediate_reference_points)

        return output, reference_points


# 适用5帧
@TRANSFORMER_LAYER_SEQUENCE.register_module()
class TransformerDecoderV2_1(TransformerLayerSequence):
    """Implements the decoder in PETR transformer.

    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        coder_norm_cfg (dict): Config of last normalization layer. Default：
            `LN`.
    """

    def __init__(self,
                 *args,
                 return_intermediate=False,
                 num_keypoints=17,
                 **kwargs):

        super(TransformerDecoderV2_1, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        self.num_keypoints = num_keypoints

    def forward(self,
                query,
                *args,
                reference_points=None,
                valid_ratios=None,
                pre_pre_kpt_branches=None,
                pre_kpt_branches=None,
                kpt_branches=None,
                next_kpt_branches=None,
                next_next_kpt_branches=None,
                **kwargs):
        """Forward function for `TransformerDecoder`.

        Args:
            query (Tensor): Input query with shape (num_query, bs, embed_dims).
            reference_points (Tensor): The reference points of offset,
                has shape (bs, num_query, K*2).
            valid_ratios (Tensor): The radios of valid points on the feature
                map, has shape (bs, num_levels, 2).
            kpt_branches: (obj:`nn.ModuleList`): Used for refining the
                regression results. Only would be passed when `with_box_refine`
                is True, otherwise would be passed a `None`.

        Returns:
            tuple (Tensor): Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims] and
                [num_layers, bs, num_query, K*2].
        """
        output = query
        intermediate = []
        intermediate_reference_points = []
        for lid, layer in enumerate(self.layers):
            if reference_points.shape[-1] == self.num_keypoints * 2:
                reference_points_input = \
                    reference_points[:, :, None] * \
                    valid_ratios.repeat(1, 1, self.num_keypoints)[:, None]
            else:
                assert reference_points.shape[-1] == 2
                reference_points_input = reference_points[:, :, None] * \
                                         valid_ratios[:, None]
            output = layer(
                output,
                *args,
                reference_points=reference_points_input,
                **kwargs)
            output = output.permute(1, 0, 2)

            # 回归出辅助帧的相对偏移量
            if kpt_branches is not None and pre_kpt_branches is not None and next_kpt_branches is not None:
                pre_pre_tmp = pre_pre_kpt_branches[lid](output)
                pre_tmp = pre_kpt_branches[lid](output)
                tmp = kpt_branches[lid](output)
                next_tmp = next_kpt_branches[lid](output)
                next_next_tmp = next_next_kpt_branches[lid](output)
                tmps = torch.concat([pre_pre_tmp, pre_tmp, tmp, next_tmp, next_next_tmp], dim=1)
                if reference_points.shape[-1] == self.num_keypoints * 2:
                    new_reference_points = tmps + inverse_sigmoid(
                        reference_points)
                    new_reference_points = new_reference_points.sigmoid()
                else:
                    raise NotImplementedError
                # reference_points = new_reference_points.detach()
                reference_points = new_reference_points
            
            output = output.permute(1, 0, 2)
            if self.return_intermediate:
                intermediate.append(output)
                intermediate_reference_points.append(reference_points)
                

        if self.return_intermediate:
            return torch.stack(intermediate), torch.stack(
                intermediate_reference_points)

        return output, reference_points


# 添加时间 2024-10-22 用于提取关节点特征
@TRANSFORMER_LAYER_SEQUENCE.register_module()
class VideoPoseTransformerDecoderV16(TransformerLayerSequence):
    """Implements the decoder in PETR transformer.

    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        coder_norm_cfg (dict): Config of last normalization layer. Default：
            `LN`.
    """

    def __init__(self,
                 *args,
                 return_intermediate=False,
                 num_keypoints=17,
                 **kwargs):

        super(VideoPoseTransformerDecoderV16, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        self.num_keypoints = num_keypoints

    def forward(self,
                query,
                *args,
                reference_points=None,
                valid_ratios=None,
                kpt_branches=None,
                **kwargs):
        """Forward function for `TransformerDecoder`.

        Args:
            query (Tensor): Input query with shape (num_query, bs, embed_dims).
            reference_points (Tensor): The reference points of offset,
                has shape (bs, num_query, K*2).
            valid_ratios (Tensor): The radios of valid points on the feature
                map, has shape (bs, num_levels, 2).
            kpt_branches: (obj:`nn.ModuleList`): Used for refining the
                regression results. Only would be passed when `with_box_refine`
                is True, otherwise would be passed a `None`.

        Returns:
            tuple (Tensor): Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims] and
                [num_layers, bs, num_query, K*2].
        """
        # only one layer
        # shape: bs, 300, dims --- pose query
        output = query
        for lid, layer in enumerate(self.layers):
            if reference_points.shape[-1] == self.num_keypoints * 2:
                reference_points_input = \
                    reference_points[:, :, None] * \
                    valid_ratios.repeat(1, 1, self.num_keypoints)[:, None]
            else:
                assert reference_points.shape[-1] == 2
                reference_points_input = reference_points[:, :, None] * \
                                         valid_ratios[:, None]
            output = layer(
                output,
                *args,
                reference_points=reference_points_input,
                **kwargs)

        return output


@TRANSFORMER_LAYER_SEQUENCE.register_module()
class VideoPoseTransformerDecoderV10(TransformerLayerSequence):
    """Implements the decoder in PETR transformer.

    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        coder_norm_cfg (dict): Config of last normalization layer. Default：
            `LN`.
    """

    def __init__(self,
                 *args,
                 return_intermediate=False,
                 num_keypoints=17,
                 **kwargs):

        super(VideoPoseTransformerDecoderV10, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        self.num_keypoints = num_keypoints

    def forward(self,
                query,
                *args,
                reference_points=None,
                valid_ratios=None,
                kpt_branches=None,
                **kwargs):
        """Forward function for `TransformerDecoder`.

        Args:
            query (Tensor): Input query with shape (num_query, bs, embed_dims).
            reference_points (Tensor): The reference points of offset,
                has shape (bs, num_query, K*2).
            valid_ratios (Tensor): The radios of valid points on the feature
                map, has shape (bs, num_levels, 2).
            kpt_branches: (obj:`nn.ModuleList`): Used for refining the
                regression results. Only would be passed when `with_box_refine`
                is True, otherwise would be passed a `None`.

        Returns:
            tuple (Tensor): Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims] and
                [num_layers, bs, num_query, K*2].
        """
        output = query
        intermediate = []
        intermediate_reference_points = []
        for lid, layer in enumerate(self.layers):
            if reference_points.shape[-1] == self.num_keypoints * 2:
                reference_points_input = \
                    reference_points[:, :, None] * \
                    valid_ratios.repeat(1, 1, self.num_keypoints)[:, None]
            else:
                assert reference_points.shape[-1] == 2
                reference_points_input = reference_points[:, :, None] * \
                                         valid_ratios[:, None]
            output = layer(
                output,
                *args,
                reference_points=reference_points_input,
                **kwargs)
            output = output.permute(1, 0, 2)

            if kpt_branches is not None:
                tmp = kpt_branches[lid](output)
                if reference_points.shape[-1] == self.num_keypoints * 2:
                    new_reference_points = tmp + inverse_sigmoid(
                        reference_points)
                    new_reference_points = new_reference_points.sigmoid()
                else:
                    raise NotImplementedError
                reference_points = new_reference_points.detach()

            output = output.permute(1, 0, 2)
            if self.return_intermediate:
                intermediate.append(output)
                intermediate_reference_points.append(reference_points)

        if self.return_intermediate:
            return torch.stack(intermediate), torch.stack(
                intermediate_reference_points)

        return output, reference_points


@TRANSFORMER_LAYER_SEQUENCE.register_module()
class VideoPoseTransformerDecoderV11(TransformerLayerSequence):
    """Implements the decoder in PETR transformer.

    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        coder_norm_cfg (dict): Config of last normalization layer. Default：
            `LN`.
    """

    def __init__(self,
                 *args,
                 return_intermediate=False,
                 num_keypoints=17,
                 **kwargs):

        super(VideoPoseTransformerDecoderV11, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        self.num_keypoints = num_keypoints

    def forward(self,
                query,
                *args,
                reference_points=None,
                valid_ratios=None,
                kpt_branches=None,
                **kwargs):
        """Forward function for `TransformerDecoder`.

        Args:
            query (Tensor): Input query with shape (num_query, bs, embed_dims).
            reference_points (Tensor): The reference points of offset,
                has shape (bs, num_query, K*2).
            valid_ratios (Tensor): The radios of valid points on the feature
                map, has shape (bs, num_levels, 2).
            kpt_branches: (obj:`nn.ModuleList`): Used for refining the
                regression results. Only would be passed when `with_box_refine`
                is True, otherwise would be passed a `None`.

        Returns:
            tuple (Tensor): Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims] and
                [num_layers, bs, num_query, K*2].
        """
        output = query
        intermediate = []
        intermediate_reference_points = []
        for lid, layer in enumerate(self.layers):
            if reference_points.shape[-1] == self.num_keypoints * 2:
                reference_points_input = \
                    reference_points[:, :, None] * \
                    valid_ratios.repeat(1, 1, self.num_keypoints)[:, None]
            else:
                assert reference_points.shape[-1] == 2
                reference_points_input = reference_points[:, :, None] * \
                                         valid_ratios[:, None]
            output = layer(
                output,
                *args,
                reference_points=reference_points_input,
                **kwargs)
            output = output.permute(1, 0, 2)

            if kpt_branches is not None:
                tmp = kpt_branches[lid](output)
                if reference_points.shape[-1] == self.num_keypoints * 2:
                    new_reference_points = tmp + inverse_sigmoid(
                        reference_points)
                    new_reference_points = new_reference_points.sigmoid()
                else:
                    raise NotImplementedError
                reference_points = new_reference_points.detach()

            output = output.permute(1, 0, 2)
            if self.return_intermediate:
                intermediate.append(output)
                intermediate_reference_points.append(reference_points)

        if self.return_intermediate:
            return torch.stack(intermediate), torch.stack(
                intermediate_reference_points)

        return output, reference_points

# 废弃  
@TRANSFORMER_LAYER_SEQUENCE.register_module()
class VideoPoseTransformerDecoder_N(TransformerLayerSequence):
    """Implements the decoder in PETR transformer.

    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        coder_norm_cfg (dict): Config of last normalization layer. Default：
            `LN`.
    """

    def __init__(self,
                 *args,
                 return_intermediate=False,
                 num_keypoints=17,
                 **kwargs):

        super(VideoPoseTransformerDecoder_N, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        self.num_keypoints = num_keypoints

    def forward(self,
                query,
                *args,
                reference_points=None,
                valid_ratios=None,
                kpt_branches=None,
                **kwargs):
        """Forward function for `TransformerDecoder`.

        Args:
            query (Tensor): Input query with shape (num_query, bs, embed_dims).
            reference_points (Tensor): The reference points of offset,
                has shape (bs, num_query, K*2).
            valid_ratios (Tensor): The radios of valid points on the feature
                map, has shape (bs, num_levels, 2).
            kpt_branches: (obj:`nn.ModuleList`): Used for refining the
                regression results. Only would be passed when `with_box_refine`
                is True, otherwise would be passed a `None`.

        Returns:
            tuple (Tensor): Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims] and
                [num_layers, bs, num_query, K*2].
        """
        output = query
        now_frame_pos_token = query[:, 1]
        intermediate = []
        intermediate_reference_points = []
        initial_reference_points = reference_points[0::2]
        
        for lid, layer in enumerate(self.layers):
            # cross-attn
            if lid == 0:
                output = output[:, 0::2].flatten(0, 1)[None]
            if lid < 3:
                if reference_points.shape[-1] == self.num_keypoints * 2:
                    reference_points_input = \
                        reference_points[:, :, None] * \
                        valid_ratios.repeat(1, 1, self.num_keypoints)[:, None]
                else:
                    assert reference_points.shape[-1] == 2
                    reference_points_input = reference_points[:, :, None] * \
                                            valid_ratios[:, None]
                output = layer(
                    output,
                    *args,
                    reference_points=reference_points_input,
                    **kwargs)
                output = output.permute(1, 0, 2)

                if kpt_branches is not None:
                    tmp = kpt_branches[lid](output)
                    if reference_points.shape[-1] == self.num_keypoints * 2:
                        new_reference_points = tmp + inverse_sigmoid(
                            reference_points)
                        new_reference_points = new_reference_points.sigmoid()
                    else:
                        raise NotImplementedError
                    # reference_points = new_reference_points.detach()
                    reference_points = new_reference_points
                output = output.permute(1, 0, 2)
                    
            else: # time-attn
                # 加入当前帧pose-token
                if lid == 3:
                    output = output.reshape(1, -1, 2, output.shape[-1]).squeeze(dim=0) # shape: num_pose, 2, dims
                    output = torch.stack([output[:, 0], now_frame_pos_token, output[:, 1]], dim=1).transpose(0, 1) # shape: 3, num_pose, dims
                output = layer(
                    output,
                    *args,
                    reference_points=None,
                    **kwargs)
        if kpt_branches is not None:
            tmp = kpt_branches[-1](output[1])
            new_reference_points = tmp + inverse_sigmoid(
                            initial_reference_points.squeeze())
            new_reference_points = new_reference_points.sigmoid()
            reference_points = new_reference_points
            intermediate.append(output)
            intermediate_reference_points.append(reference_points)
                
        if self.return_intermediate:
            return torch.stack(intermediate), torch.stack(
                intermediate_reference_points)
        
@TRANSFORMER_LAYER_SEQUENCE.register_module()
class VideoPoseTransformerDecoderV3(TransformerLayerSequence):
    """Implements the decoder in PETR transformer.

    Args:
        return_intermediate (bool): Whether to return intermediate outputs.
        coder_norm_cfg (dict): Config of last normalization layer. Default：
            `LN`.
    """

    def __init__(self,
                 *args,
                 return_intermediate=False,
                 num_keypoints=17,
                 **kwargs):

        super(VideoPoseTransformerDecoderV3, self).__init__(*args, **kwargs)
        self.return_intermediate = return_intermediate
        self.num_keypoints = num_keypoints

    def forward(self,
                query,
                *args,
                reference_points=None,
                valid_ratios=None,
                kpt_branches=None,
                counter=None,
                **kwargs):
        """Forward function for `TransformerDecoder`.

        Args:
            query (Tensor): Input query with shape (num_query, bs, embed_dims).
            reference_points (Tensor): The reference points of offset,
                has shape (bs, num_query, K*2).
            valid_ratios (Tensor): The radios of valid points on the feature
                map, has shape (bs, num_levels, 2).
            kpt_branches: (obj:`nn.ModuleList`): Used for refining the
                regression results. Only would be passed when `with_box_refine`
                is True, otherwise would be passed a `None`.

        Returns:
            tuple (Tensor): Results with shape [1, num_query, bs, embed_dims] when
                return_intermediate is `False`, otherwise it has shape
                [num_layers, num_query, bs, embed_dims] and
                [num_layers, bs, num_query, K*2].
        """
        output = query
        now_frame_pos_token = query[:, 1]
        intermediate = []
        intermediate_reference_points = []
        initial_reference_points = reference_points[0::2]
        
        for lid, layer in enumerate(self.layers):
            # cross-attn
            if lid == 0:
                output = output[:, 0::2].flatten(0, 1)[None]
            if lid < 3:
                if reference_points.shape[-1] == self.num_keypoints * 2:
                    reference_points_input = \
                        reference_points[:, :, None] * \
                        valid_ratios.repeat(1, 1, self.num_keypoints)[:, None]
                else:
                    assert reference_points.shape[-1] == 2
                    reference_points_input = reference_points[:, :, None] * \
                                            valid_ratios[:, None]
                output = layer(
                    output,
                    *args,
                    reference_points=reference_points_input,
                    **kwargs)
                output = output.permute(1, 0, 2)

                if kpt_branches is not None:
                    tmp = kpt_branches[lid](output)
                    if reference_points.shape[-1] == self.num_keypoints * 2:
                        new_reference_points = tmp + inverse_sigmoid(
                            reference_points)
                        new_reference_points = new_reference_points.sigmoid()
                    else:
                        raise NotImplementedError
                    # reference_points = new_reference_points.detach()
                    reference_points = new_reference_points
                output = output.permute(1, 0, 2)
                    
            else: # time-attn
                # 加入当前帧pose-token
                if lid == 3:
                    output = output.reshape(1, -1, 2, output.shape[-1]).squeeze(dim=0) # shape: num_pose, 2, dims
                    output = torch.stack([output[:, 0], now_frame_pos_token, output[:, 1]], dim=1).transpose(0, 1) # shape: 3, num_pose, dims
                output = layer(
                    output,
                    *args,
                    reference_points=None,
                    **kwargs)
        if kpt_branches is not None:
            tmp = kpt_branches[-1](output[1])
            new_reference_points = tmp + inverse_sigmoid(
                            initial_reference_points.squeeze())
            new_reference_points = new_reference_points.sigmoid()
            reference_points = new_reference_points
            intermediate.append(output)
            intermediate_reference_points.append(reference_points)
                
        if self.return_intermediate:
            return torch.stack(intermediate), torch.stack(
                intermediate_reference_points)
        
@TRANSFORMER.register_module()
class TransformerMulFrames(Transformer):
    """Implements the PETR transformer.

    Args:
        as_two_stage (bool): Generate query from encoder features.
            Default: False.
        num_feature_levels (int): Number of feature maps from FPN:
            Default: 4.
        two_stage_num_proposals (int): Number of proposals when set
            `as_two_stage` as True. Default: 300.
    """
    def __init__(self,
                 hm_encoder=dict(
                     type='DetrTransformerEncoder',
                     num_layers=1,
                     transformerlayers=dict(
                         type='BaseTransformerLayer',
                         attn_cfgs=dict(
                             type='MultiScaleDeformableAttention',
                             embed_dims=256,
                             num_levels=1),
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'ffn',
                                          'norm'))),
                 refine_decoder=dict(
                     type='DeformableDetrTransformerDecoder',
                     num_layers=1,
                     return_intermediate=True,
                     transformerlayers=dict(
                         type='DetrTransformerDecoderLayer',
                         attn_cfgs=[
                             dict(
                                 type='MultiheadAttention',
                                 embed_dims=256,
                                 num_heads=8,
                                 dropout=0.1),
                             dict(
                                 type='MultiScaleDeformableAttention',
                                 embed_dims=256)
                         ],
                         feedforward_channels=1024,
                         ffn_dropout=0.1,
                         operation_order=('self_attn', 'norm', 'cross_attn',
                                          'norm', 'ffn', 'norm'))),
                 as_two_stage=True,
                 num_feature_levels=4,
                 two_stage_num_proposals=300,
                 num_keypoints=17,
                 num_frames=3,
                 reanchor_alpha=0.0,
                 **kwargs):
        super(TransformerMulFrames, self).__init__(**kwargs)
        self.as_two_stage = as_two_stage
        self.num_feature_levels = num_feature_levels
        self.two_stage_num_proposals = two_stage_num_proposals
        self.embed_dims = self.encoder.embed_dims
        self.num_keypoints = num_keypoints
        self.num_frames = num_frames
        self.reanchor_alpha = reanchor_alpha
        # self.hm_encoder = build_transformer_layer_sequence(hm_encoder)
        self.refine_decoder = build_transformer_layer_sequence(refine_decoder)
        self.init_layers()

    def init_layers(self):
        """Initialize layers of the DeformableDetrTransformer."""
        self.level_embeds = nn.Parameter(
            torch.Tensor(self.num_feature_levels, self.embed_dims))

        if self.as_two_stage:
            self.enc_output = nn.Linear(self.embed_dims, self.embed_dims)
            self.enc_output_norm = nn.LayerNorm(self.embed_dims)
            self.refine_query_embedding = nn.Embedding(self.num_keypoints,
                                                       self.embed_dims * 2)
        else:
            self.reference_points = nn.Linear(self.embed_dims,
                                          2 * self.num_keypoints)
            

    def init_weights(self):
        """Initialize the transformer weights."""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        for m in self.modules():
            if isinstance(m, MultiScaleDeformableAttention):
                m.init_weights()
        for m in self.modules():
            if isinstance(m, MultiScaleDeformablePoseAttention):
                m.init_weights()
        if not self.as_two_stage:
            xavier_init(self.reference_points, distribution='uniform', bias=0.)
        normal_(self.level_embeds)
        normal_(self.refine_query_embedding.weight)

    def gen_encoder_output_proposals(self, memory, memory_padding_mask,
                                     spatial_shapes):
        """Generate proposals from encoded memory.

        Args:
            memory (Tensor): The output of encoder, has shape
                (bs, num_key, embed_dim). num_key is equal the number of points
                on feature map from all level.
            memory_padding_mask (Tensor): Padding mask for memory.
                has shape (bs, num_key).
            spatial_shapes (Tensor): The shape of all feature maps.
                has shape (num_level, 2).

        Returns:
            tuple: A tuple of feature map and bbox prediction.

                - output_memory (Tensor): The input of decoder, has shape
                    (bs, num_key, embed_dim). num_key is equal the number of
                    points on feature map from all levels.
                - output_proposals (Tensor): The normalized proposal
                    after a inverse sigmoid, has shape (bs, num_keys, 4).
        """

        N, S, C = memory.shape
        proposals = []
        _cur = 0
        for lvl, (H, W) in enumerate(spatial_shapes):
            mask_flatten_ = memory_padding_mask[:, _cur:(_cur + H * W)].view(
                N, H, W, 1)
            valid_H = torch.sum(~mask_flatten_[:, :, 0, 0], 1)
            valid_W = torch.sum(~mask_flatten_[:, 0, :, 0], 1)

            grid_y, grid_x = torch.meshgrid(
                torch.linspace(
                    0, H - 1, H, dtype=torch.float32, device=memory.device),
                torch.linspace(
                    0, W - 1, W, dtype=torch.float32, device=memory.device))
            grid = torch.cat([grid_x.unsqueeze(-1), grid_y.unsqueeze(-1)], -1)

            scale = torch.cat([valid_W.unsqueeze(-1),
                               valid_H.unsqueeze(-1)], 1).view(N, 1, 1, 2)
            grid = (grid.unsqueeze(0).expand(N, -1, -1, -1) + 0.5) / scale
            proposal = grid.view(N, -1, 2)
            proposals.append(proposal)
            _cur += (H * W)
        output_proposals = torch.cat(proposals, 1)
        output_proposals_valid = ((output_proposals > 0.01) &
                                  (output_proposals < 0.99)).all(
                                      -1, keepdim=True)
        output_proposals = torch.log(output_proposals / (1 - output_proposals))
        output_proposals = output_proposals.masked_fill(
            memory_padding_mask.unsqueeze(-1), float('inf'))
        output_proposals = output_proposals.masked_fill(
            ~output_proposals_valid, float('inf'))

        output_memory = memory
        output_memory = output_memory.masked_fill(
            memory_padding_mask.unsqueeze(-1), float(0))
        output_memory = output_memory.masked_fill(~output_proposals_valid,
                                                  float(0))
        output_memory = self.enc_output_norm(self.enc_output(output_memory))
        return output_memory, output_proposals

    @staticmethod
    def get_reference_points(spatial_shapes, valid_ratios, device):
        """Get the reference points used in decoder.

        Args:
            spatial_shapes (Tensor): The shape of all feature maps,
                has shape (num_level, 2).
            valid_ratios (Tensor): The radios of valid points on the
                feature map, has shape (bs, num_levels, 2).
            device (obj:`device`): The device where reference_points should be.

        Returns:
            Tensor: reference points used in decoder, has \
                shape (bs, num_keys, num_levels, 2).
        """
        reference_points_list = []
        for lvl, (H, W) in enumerate(spatial_shapes):
            ref_y, ref_x = torch.meshgrid(
                torch.linspace(
                    0.5, H - 0.5, H, dtype=torch.float32, device=device),
                torch.linspace(
                    0.5, W - 0.5, W, dtype=torch.float32, device=device))
            ref_y = ref_y.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 1] * H)
            ref_x = ref_x.reshape(-1)[None] / (
                valid_ratios[:, None, lvl, 0] * W)
            ref = torch.stack((ref_x, ref_y), -1)
            reference_points_list.append(ref)
        reference_points = torch.cat(reference_points_list, 1)
        reference_points = reference_points[:, :, None] * valid_ratios[:, None]
        return reference_points

    def get_valid_ratio(self, mask):
        """Get the valid radios of feature maps of all level."""
        _, H, W = mask.shape
        valid_H = torch.sum(~mask[:, :, 0], 1)
        valid_W = torch.sum(~mask[:, 0, :], 1)
        valid_ratio_h = valid_H.float() / H
        valid_ratio_w = valid_W.float() / W
        valid_ratio = torch.stack([valid_ratio_w, valid_ratio_h], -1)
        return valid_ratio

    def get_proposal_pos_embed(self,
                               proposals,
                               num_pos_feats=128,
                               temperature=10000):
        """Get the position embedding of proposal."""
        scale = 2 * math.pi
        dim_t = torch.arange(
            num_pos_feats, dtype=torch.float32, device=proposals.device)
        dim_t = temperature**(2 * (dim_t // 2) / num_pos_feats)
        # N, L, 4
        proposals = proposals.sigmoid() * scale
        # N, L, 4, 128
        pos = proposals[:, :, :, None] / dim_t
        # N, L, 4, 64, 2
        pos = torch.stack((pos[:, :, :, 0::2].sin(), pos[:, :, :, 1::2].cos()),
                          dim=4).flatten(2)
        return pos

    def forward(self,
                mlvl_feats,
                mlvl_masks,
                query_embed,
                mlvl_pos_embeds,
                pre_pre_kpt_branches=None,
                pre_kpt_branches=None,
                kpt_branches=None,
                next_kpt_branches=None,
                next_next_kpt_branches=None,
                cls_branches=None,
                sigma_branches=None,
                track_queries=None,
                track_reference_points=None,
                track_query_pos=None,
                **kwargs):
        """Forward function for `Transformer`.

        Args:
            mlvl_feats (list(Tensor)): Input queries from different level.
                Each element has shape [bs, embed_dims, h, w].
            mlvl_masks (list(Tensor)): The key_padding_mask from different
                level used for encoder and decoder, each element has shape
                [bs, h, w].
            query_embed (Tensor): The query embedding for decoder,
                with shape [num_query, c].
            mlvl_pos_embeds (list(Tensor)): The positional encoding
                of feats from different level, has the shape
                 [bs, embed_dims, h, w].
            kpt_branches (obj:`nn.ModuleList`): Keypoint Regression heads for
                feature maps from each decoder layer. Only would be passed when
                `with_box_refine` is Ture. Default to None.
            cls_branches (obj:`nn.ModuleList`): Classification heads for
                feature maps from each decoder layer. Only would be passed when
                `as_two_stage` is Ture. Default to None.

        Returns:
            tuple[Tensor]: results of decoder containing the following tensor.

                - inter_states: Outputs from decoder. If
                    `return_intermediate_dec` is True output has shape \
                    (num_dec_layers, bs, num_query, embed_dims), else has \
                    shape (1, bs, num_query, embed_dims).
                - init_reference_out: The initial value of reference \
                    points, has shape (bs, num_queries, 4).
                - inter_references_out: The internal value of reference \
                    points in decoder, has shape \
                    (num_dec_layers, bs,num_query, embed_dims)
                - enc_outputs_class: The classification score of proposals \
                    generated from encoder's feature maps, has shape \
                    (batch, h*w, num_classes). \
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
                - enc_outputs_kpt_unact: The regression results generated from \
                    encoder's feature maps., has shape (batch, h*w, K*2).
                    Only would be returned when `as_two_stage` is True, \
                    otherwise None.
        """
        assert self.as_two_stage or query_embed is not None
        # import time
        # start = time.time()
        feat_flatten = []
        mask_flatten = []
        lvl_pos_embed_flatten = []
        spatial_shapes = []
        for lvl, (feat, mask, pos_embed) in enumerate(
                zip(mlvl_feats, mlvl_masks, mlvl_pos_embeds)):
            bs, c, h, w = feat.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            feat = feat.flatten(2).transpose(1, 2)
            mask = mask.flatten(1)
            pos_embed = pos_embed.flatten(2).transpose(1, 2)
            lvl_pos_embed = pos_embed + self.level_embeds[lvl].view(1, 1, -1)
            lvl_pos_embed_flatten.append(lvl_pos_embed)
            feat_flatten.append(feat)
            mask_flatten.append(mask)
        feat_flatten = torch.cat(feat_flatten, 1)
        mask_flatten = torch.cat(mask_flatten, 1)
        lvl_pos_embed_flatten = torch.cat(lvl_pos_embed_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=feat_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        reference_points = \
            self.get_reference_points(spatial_shapes,
                                      valid_ratios,
                                      device=feat.device)

        feat_flatten = feat_flatten.permute(1, 0, 2)  # (H*W, bs, embed_dims)
        lvl_pos_embed_flatten = lvl_pos_embed_flatten.permute(
            1, 0, 2)  # (H*W, bs, embed_dims)
        # shape: num_tokens, bs, c
        memory = self.encoder(
            query=feat_flatten,
            key=None,
            value=None,
            query_pos=lvl_pos_embed_flatten,
            query_key_padding_mask=mask_flatten,
            spatial_shapes=spatial_shapes,
            reference_points=reference_points,
            level_start_index=level_start_index,
            valid_ratios=valid_ratios,
            **kwargs)
        # end = time.time()
        # print(f'only encoder time: {end - start:.6f}')


        memory = memory.permute(1, 0, 2)
        bs, _, c = memory.shape
        
        now_frame_memory = memory[self.num_frames//2::self.num_frames]
        now_frame_lvl_pos_embed_flatten = lvl_pos_embed_flatten[:, self.num_frames//2::self.num_frames] # token_num, bs//3, dim
        now_frame_mask_flatten = mask_flatten[self.num_frames//2::self.num_frames] # shape: bs//3, token_num, dim
        now_frame_reference_points = reference_points[self.num_frames//2::self.num_frames] # shape: bs//3, token_num, dim
        now_frame_valid_ratios = valid_ratios[self.num_frames//2::self.num_frames] # shape: bs//3, token_num, dim
        now_frame_mlvl_masks =[mlvl_mask[self.num_frames//2::self.num_frames] for mlvl_mask in mlvl_masks]

        # hm_proto = None
        # if self.training:
        #     hm_memory = now_frame_memory[
        #         :, level_start_index[0]:level_start_index[1], :]
        #     hm_pos_embed = now_frame_lvl_pos_embed_flatten[
        #         level_start_index[0]:level_start_index[1], :, :]
        #     hm_mask = now_frame_mask_flatten[
        #         :, level_start_index[0]:level_start_index[1]]
        #     hm_reference_points = now_frame_reference_points[
        #         :, level_start_index[0]:level_start_index[1], [0], :]
        #     hm_memory = hm_memory.permute(1, 0, 2)
        #     hm_memory = self.hm_encoder(
        #         query=hm_memory,
        #         key=None,
        #         value=None,
        #         query_pose=hm_pos_embed,
        #         query_key_padding_mask=hm_mask,
        #         spatial_shapes=spatial_shapes[[0]],
        #         reference_points=hm_reference_points,
        #         level_start_index=level_start_index[0],
        #         valid_ratios=now_frame_valid_ratios[:, [0], :],
        #         **kwargs)
        #     hm_memory = hm_memory.permute(1, 0, 2).reshape(bs//3,
        #         spatial_shapes[0, 0], spatial_shapes[0, 1], -1)
        #     hm_proto = (hm_memory, now_frame_mlvl_masks[0])

        if self.as_two_stage:
            output_memory, output_proposals = \
                self.gen_encoder_output_proposals(
                    now_frame_memory, now_frame_mask_flatten, spatial_shapes)
            enc_outputs_class = cls_branches[self.decoder.num_layers](
                output_memory)
            enc_outputs_kpt_unact = \
                kpt_branches[self.decoder.num_layers](output_memory)
            enc_outputs_kpt_unact[..., 0::2] += output_proposals[..., 0:1]
            enc_outputs_kpt_unact[..., 1::2] += output_proposals[..., 1:2]
            enc_outputs_sigma_unact = \
                sigma_branches[self.decoder.num_layers](output_memory)

            # Determine number of track vs detect queries
            n_track = track_queries.shape[1] if track_queries is not None else 0
            n_detect = self.two_stage_num_proposals - n_track

            topk = n_detect
            topk_proposals = torch.topk(
                enc_outputs_class[..., 0], topk, dim=1)[1]
            topk_kpts_unact = torch.gather(
                enc_outputs_kpt_unact, 1,
                topk_proposals.unsqueeze(-1).repeat(
                    1, 1, enc_outputs_kpt_unact.size(-1)))
            topk_kpts_unact = topk_kpts_unact.detach()
            tgt_undetach = torch.gather(output_memory, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, self.embed_dims))
            tgt_detach = tgt_undetach.detach()
            detect_refs = topk_kpts_unact.sigmoid()

            # learnable query and query_pos (for detect queries only)
            query_pos_embed, query_embed_val = torch.split(query_embed, c, dim=1)
            # Only take n_detect learnable queries  
            detect_query_pos = query_pos_embed[:n_detect].unsqueeze(0).expand(bs // self.num_frames, -1, -1)
            detect_query = query_embed_val[:n_detect].unsqueeze(0).expand(bs // self.num_frames, -1, -1)
            detect_query = tgt_detach + detect_query
            detect_refs_3frames = detect_refs.repeat(1, self.num_frames, 1)

            # ── Pose-Aware Re-Anchoring ──────────────────────────────────
            # Instead of letting track queries keep stale reference points
            # from frame t-1, match each track query to the nearest encoder
            # proposal in the *current* frame and blend the references.
            # Identity is preserved through the query content (hidden state);
            # only the spatial anchor is refreshed.
            if n_track > 0 and track_reference_points is not None and self.reanchor_alpha > 0:
                with torch.no_grad():
                    all_enc_kpts = enc_outputs_kpt_unact.sigmoid()       # [bs//3, N_enc, K*2]
                    enc_scores = enc_outputs_class.sigmoid().squeeze(-1)  # [bs//3, N_enc]

                    # Pre-filter to top-M proposals to bound memory
                    M = min(300, all_enc_kpts.shape[1])
                    _, top_m_inds = enc_scores.topk(M, dim=1)            # [bs//3, M]
                    top_m_kpts = torch.gather(
                        all_enc_kpts, 1,
                        top_m_inds.unsqueeze(-1).expand(-1, -1, all_enc_kpts.size(-1)))  # [bs//3, M, K*2]

                    # Mean-squared-error across keypoints → [bs//3, n_track, M]
                    dists = ((track_reference_points.unsqueeze(2) -
                              top_m_kpts.unsqueeze(1)) ** 2).mean(-1)
                    nearest_idx = dists.argmin(dim=2)                    # [bs//3, n_track]

                    matched_refs = torch.gather(
                        top_m_kpts, 1,
                        nearest_idx.unsqueeze(-1).expand(-1, -1, top_m_kpts.size(-1)))  # [bs//3, n_track, K*2]

                # Blend: alpha × current-frame proposal + (1-alpha) × stale prediction
                a = self.reanchor_alpha
                track_reference_points = a * matched_refs + (1.0 - a) * track_reference_points
            # ─────────────────────────────────────────────────────────────

            if n_track > 0:
                # Concatenate: track queries first, detect queries second
                query = torch.cat([track_queries, detect_query], dim=1)  # [bs//3, n_track+n_detect, 256]
                query_pos = torch.cat([track_query_pos, detect_query_pos], dim=1)
                # Combine single-frame refs first, THEN repeat for 3 frames.
                # This ensures correct layout: [(track+detect)_f0, (track+detect)_f1, (track+detect)_f2]
                all_single_frame_refs = torch.cat([track_reference_points, detect_refs], dim=1)  # [bs//3, n_track+n_detect, K*2]
                reference_points = all_single_frame_refs.repeat(1, self.num_frames, 1)  # [bs//3, 3*(n_track+n_detect), K*2]
            else:
                query = detect_query
                query_pos = detect_query_pos
                reference_points = detect_refs_3frames

            init_reference_out = reference_points
        else:
            query_pos, query = torch.split(query_embed, c, dim=1)
            query_pos = query_pos.unsqueeze(0).expand(bs, -1, -1)
            query = query.unsqueeze(0).expand(bs, -1, -1)
            reference_points = self.reference_points(query_pos).sigmoid()
            init_reference_out = reference_points

        query = query.permute(1, 0, 2)
        memory = memory.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        if self.num_frames == 3:
            inter_states, inter_references = self.decoder(
                query=query,
                key=None,
                value=memory,
                query_pos=query_pos,
                key_padding_mask=mask_flatten,
                reference_points=reference_points,
                spatial_shapes=spatial_shapes,
                level_start_index=level_start_index,
                valid_ratios=now_frame_valid_ratios,
                pre_kpt_branches=pre_kpt_branches,
                kpt_branches=kpt_branches,
                next_kpt_branches=next_kpt_branches,
                **kwargs)
        else:
            inter_states, inter_references = self.decoder(
                query=query,
                key=None,
                value=memory,
                query_pos=query_pos,
                key_padding_mask=mask_flatten,
                reference_points=reference_points,
                spatial_shapes=spatial_shapes,
                level_start_index=level_start_index,
                valid_ratios=now_frame_valid_ratios,
                pre_pre_kpt_branches=pre_pre_kpt_branches,
                pre_kpt_branches=pre_kpt_branches,
                kpt_branches=kpt_branches,
                next_kpt_branches=next_kpt_branches,
                next_next_kpt_branches=next_next_kpt_branches,
                **kwargs)
            
        # end = time.time()
        # print(f'only pose-decoder time: {end - start:.6f}')
    
        inter_references_out = inter_references
        if self.as_two_stage:
            return inter_states, init_reference_out, \
                   inter_references_out, enc_outputs_class, \
                   enc_outputs_kpt_unact, enc_outputs_sigma_unact, None, memory, n_track
        return inter_states, init_reference_out, \
               inter_references_out, None, None, None, None, None, None, 0
    
    def forward_refine(self,
                       mlvl_masks,
                       memory,
                       reference_points_pose,
                       img_inds,
                       pre_pre_kpt_branches=None,
                       pre_kpt_branches=None,
                       kpt_branches=None,
                       next_kpt_branches=None,
                       next_next_kpt_branches=None,
                       **kwargs):     
        mask_flatten = []
        spatial_shapes = []
        for lvl, mask in enumerate(mlvl_masks):
            bs, h, w = mask.shape
            spatial_shape = (h, w)
            spatial_shapes.append(spatial_shape)
            mask = mask.flatten(1)
            mask_flatten.append(mask)
        mask_flatten = torch.cat(mask_flatten, 1)
        spatial_shapes = torch.as_tensor(
            spatial_shapes, dtype=torch.long, device=mask_flatten.device)
        level_start_index = torch.cat((spatial_shapes.new_zeros(
            (1, )), spatial_shapes.prod(1).cumsum(0)[:-1]))
        valid_ratios = torch.stack(
            [self.get_valid_ratio(m) for m in mlvl_masks], 1)

        # pose refinement (17 queries corresponding to 17 keypoints)
        # learnable query and query_pos
        refine_query_embedding = self.refine_query_embedding.weight
        query_pos, query = torch.split(
            refine_query_embedding, refine_query_embedding.size(1) // 2, dim=1)

        pos_num = reference_points_pose.size(0) // self.num_frames
        query_pos = query_pos.unsqueeze(0).expand(pos_num, -1, -1) # num_gts, num_keypoints, embed_dims 
        query = query.unsqueeze(0).expand(pos_num, -1, -1)
        reference_points = reference_points_pose.reshape(-1, reference_points_pose.size(1) // 2, 2)
        query = query.permute(1, 0, 2)
        query_pos = query_pos.permute(1, 0, 2)
        # num_tokens, num_gts, num_frames, embed_dims
        pos_memory = memory[:, img_inds, :, :]
        mask_flatten = mask_flatten.reshape(-1, self.num_frames, mask_flatten.size(-1))[img_inds, :]
        valid_ratios = valid_ratios.reshape(-1, self.num_frames, valid_ratios.size(-2), valid_ratios.size(-1))[img_inds, ...]
        if self.num_frames == 3:
            inter_states, inter_references = self.refine_decoder(
                query=query,
                key=None,
                value=pos_memory,
                query_pos=query_pos,
                key_padding_mask=mask_flatten,
                reference_points=reference_points,
                spatial_shapes=spatial_shapes,
                level_start_index=level_start_index,
                valid_ratios=valid_ratios.flatten(0, 1),
                pre_reg_branches=pre_kpt_branches,
                reg_branches=kpt_branches,
                next_reg_branches=next_kpt_branches,
                **kwargs)
        else:
            inter_states, inter_references = self.refine_decoder(
                query=query,
                key=None,
                value=pos_memory,
                query_pos=query_pos,
                key_padding_mask=mask_flatten,
                reference_points=reference_points,
                spatial_shapes=spatial_shapes,
                level_start_index=level_start_index,
                valid_ratios=valid_ratios.flatten(0, 1),
                pre_pre_reg_branches=pre_pre_kpt_branches,
                pre_reg_branches=pre_kpt_branches,
                reg_branches=kpt_branches,
                next_reg_branches=next_kpt_branches,
                next_next_reg_branches=next_next_kpt_branches,
                **kwargs)
        # [num_decoder, num_query, bs, embed_dim]
        
        init_reference_out = reference_points
        return inter_states, init_reference_out, inter_references
